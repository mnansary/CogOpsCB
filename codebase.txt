================================================================================
--- File: README.md ---
================================================================================

# CogOpsCB

================================================================================
--- File: app.py ---
================================================================================

# app.py

import gradio as gr
import requests # To make HTTP requests to your API
import json     # To parse the streaming response from your API
import uuid     # To create a unique session ID for the conversation

# --- 1. API and Session Configuration ---
# The Gradio client only needs to know the address of the API Gateway.
# All backend logic is now handled by the separate govtchat service.

# IMPORTANT: Make sure this URL points to your running API Gateway.
GATEWAY_URL = "http://114.130.116.74" 
CHAT_API_ENDPOINT = f"{GATEWAY_URL}/govtchat/chat/stream"

# Create a single, unique session ID for the entire duration of this Gradio app instance.
# The backend service will use this to keep track of the conversation history.
SESSION_ID = f"gradio-session-{uuid.uuid4()}"
print(f"Gradio App is running. All conversations will use Session ID: {SESSION_ID}")


# --- 2. Define the Core Chatbot Function (API Client) ---
# This function is the bridge between Gradio's UI and our backend API.
# It remains unchanged as its logic is independent of the display language.
def predict(message: str, history: list):
    """
    The main prediction function that calls the backend chat API.

    Args:
        message (str): The user's input message from the Gradio UI.
        history (list): The chat history managed by Gradio (we ignore this and rely on the backend's session).

    Yields:
        str: A stream of strings that builds the chatbot's response in the UI.
    """
    
    # Immediately yield a "Thinking..." placeholder for better user experience.
    yield "⌛ প্রক্রিয়াকরণ চলছে..." # Changed to Bangla

    # This list will accumulate the full response as it's streamed from the API.
    full_answer_list = []
    
    # This is the data we will send in our POST request to the backend.
    payload = {
        "user_id": SESSION_ID,
        "query": message
    }

    try:
        # Make a POST request to the streaming API endpoint.
        with requests.post(CHAT_API_ENDPOINT, json=payload, stream=True, timeout=300) as response:
            # Raise an exception if the API returns an error status code (e.g., 404, 500).
            response.raise_for_status()
            
            # Iterate over the streaming response line by line.
            for line in response.iter_lines():
                if line:
                    # Each line from the service is a JSON object representing an event.
                    event = json.loads(line.decode('utf-8'))
                    
                    if event["type"] == "answer_chunk":
                        # Append the new piece of the answer to our list.
                        full_answer_list.append(event["content"])
                        # Yield the joined list to update the Gradio UI in real-time.
                        yield "".join(full_answer_list)
                    
                    elif event["type"] == "final_data":
                        # The API signals that the main answer is complete and sends sources.
                        sources = event["content"].get("sources", [])
                        if sources:
                            # Format the sources and append them to the final answer.
                            source_str = "\n\n---\n*তথ্যসূত্র:* " + ", ".join(sources) # Changed to Bangla
                            full_answer_list.append(source_str)
                            # Yield the final, complete message with sources.
                            yield "".join(full_answer_list)

                    elif event["type"] == "error":
                        # If the backend sends a specific error event, display it.
                        yield f"ত্রুটি দেখা দিয়েছে: {event['content']}" # Changed to Bangla
                        return # Stop processing on error

    except requests.exceptions.RequestException as e:
        # Catch network-related errors (e.g., cannot connect to the server).
        print(f"An API connection error occurred: {e}")
        yield f"দুঃখিত, আমি চ্যাট সার্ভিসের সাথে সংযোগ স্থাপন করতে পারিনি। অনুগ্রহ করে নিশ্চিত করুন যে ব্যাকএন্ড চলছে। ত্রুটি: {e}"
    except Exception as e:
        # Catch any other unexpected errors.
        print(f"An unexpected error occurred in the predict function: {e}")
        yield "একটি অপ্রত্যাশিত ত্রুটি ঘটেছে। আবার চেষ্টা করুন."


# --- 3. Configure and Launch the Gradio UI ---
# This section has been updated with the new Bangla titles and examples.
demo = gr.ChatInterface(
    fn=predict,
    title="বাংলা VPA - আপনার ভার্চুয়াল সহায়ক",
    description="সরকারি সেবা সম্পর্কে জিজ্ঞাসা করুন, এবং আমি আপনাকে সঠিক তথ্য দিয়ে সহায়তা করব।",
    examples=[
        ["জন্ম নিবন্ধন করার প্রক্রিয়া কি?"],
        ["ট্রেড লাইসেন্স কিভাবে পাবো?"],
        ["পাসপোর্ট করার জন্য কি কি কাগজপত্র প্রয়োজন?"]
    ],
    cache_examples=False,
)

if __name__ == "__main__":
    # Launch the Gradio app. It will be accessible on your local network.
    demo.launch(server_name="0.0.0.0")

================================================================================
--- File: api.py ---
================================================================================

import asyncio
import json
import uvicorn
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Dict, Any

# --- MODIFIED: Import your ChatAgent class ---
from cogops.agent import ChatAgent
from fastapi.middleware.cors import CORSMiddleware

# --- Global Configuration ---
# Define the path to your config file once
AGENT_CONFIG_PATH = "configs/config.yaml"

# --- API Setup ---
app = FastAPI(
    title="Configurable Chat Agent API",
    description="A production-grade API for the Chat Agent service with multi-user session management.",
    version="2.0.0",
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Session Management ---
# In-memory session store. For production, consider Redis or another distributed cache.
# MODIFIED: The dictionary now stores ChatAgent instances.
chat_sessions: Dict[str, ChatAgent] = {}
sessions_lock = asyncio.Lock()


# --- Pydantic Models for Request Bodies ---
class ChatRequest(BaseModel):
    user_id: str
    query: str

class ClearSessionRequest(BaseModel):
    user_id: str


# --- Core Session Logic ---
async def get_or_create_session(user_id: str) -> ChatAgent:
    """
    Retrieves an existing chat session or creates a new one. This function is thread-safe.
    """
    async with sessions_lock:
        if user_id not in chat_sessions:
            print(f"-> Creating new chat session for user_id: {user_id}")
            # MODIFIED: Instantiate the ChatAgent with its required config path.
            chat_sessions[user_id] = ChatAgent(config_path=AGENT_CONFIG_PATH)
        else:
            print(f"-> Found existing session for user_id: {user_id}")
        return chat_sessions[user_id]


# --- API Endpoints ---
@app.get("/health", tags=["Monitoring"])
async def health_check():
    """Confirms the service is running and reports the number of active sessions."""
    return {"status": "ok", "active_sessions": len(chat_sessions)}


@app.post("/chat/clear_session", tags=["Session Management"])
async def clear_session(request: ClearSessionRequest):
    """
    Clears the conversation history for a specific user by deleting their session instance.
    """
    user_id = request.user_id
    message = ""
    
    async with sessions_lock:
        if user_id in chat_sessions:
            del chat_sessions[user_id]
            message = f"Session for user_id '{user_id}' has been cleared."
            print(f"-> Cleared session for user_id: {user_id}")
        else:
            message = f"No active session found for user_id '{user_id}'. Nothing to clear."
            print(f"-> Attempted to clear non-existent session for user_id: {user_id}")
            
    return {"status": "success", "message": message}


@app.post("/chat/stream", tags=["Chat"])
async def stream_chat(chat_request: ChatRequest):
    """
    Main chat endpoint. Manages the user's session and streams the response
    back as newline-delimited JSON (NDJSON).
    """
    session = await get_or_create_session(chat_request.user_id)

    async def response_generator():
        """
        Async generator that yields events from the ChatAgent, formatted for streaming.
        """
        try:
            # MODIFIED: Use `async for` to iterate over the async generator `process_query`.
            async for event in session.process_query(chat_request.query):
                json_event = json.dumps(event, ensure_ascii=False)
                yield f"{json_event}\n"
                # A small sleep is good practice to prevent tight-looping if the
                # downstream process is very fast, allowing other tasks to run.
                await asyncio.sleep(0.001)
        except Exception as e:
            print(f"An error occurred during generation for user {chat_request.user_id}: {e}")
            error_event = {
                "type": "error",
                "content": "An internal error occurred. Please try again later."
            }
            yield f"{json.dumps(error_event)}\n"

    return StreamingResponse(response_generator(), media_type="application/x-ndjson")


if __name__ == "__main__":
    # To run this service:
    # 1. Place this file in the same root directory as your `chat_agent.py`, `prompts`, etc.
    # 2. Run the command in your terminal:
    #    uvicorn api_service:app --host 0.0.0.0 --port 9000 --reload
    print("Starting Uvicorn server on http://0.0.0.0:9000")
    # Note: The uvicorn.run call is mainly for IDE-based execution.
    # The terminal command is the standard way to run it.
    uvicorn.run("api_service:app", host="0.0.0.0", port=9000, reload=True)

================================================================================
--- File: setup.py ---
================================================================================

# setup.py

from setuptools import setup, find_packages

# --- Read the contents of your README file ---
# This will be used as the long description for your package
with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

# # --- Read the contents of your requirements file ---
# # This will be used to automatically define the package's dependencies
# with open("requirements.txt", "r") as f:
#     install_requires = [
#         line.strip() for line in f if line.strip() and not line.startswith("#")
#     ]


setup(
    # --- Core Metadata ---
    name='cogops',
    version='0.1.0',

    # --- Author and Project Links ---
    author='mnansary',
    author_email='nazmuddoha.ansary.28@gmail.com',
    description='Cognitive Operations for ChatBots',
    long_description=long_description,
    long_description_content_type="text/markdown",
    url='https://github.com/mnansary/CogOpsCB.git',  # URL to your project's repository

    # --- Package Discovery ---
    # find_packages() automatically finds all packages (directories with an __init__.py)
    # in your project. We can specify where to look.
    packages=find_packages(where=".", include=["cogops*"]),

    # --- Dependencies ---
    # This list is now dynamically read from your requirements.txt file.
    #install_requires=install_requires,

    # --- Python Version Requirement ---
    # Specify the minimum version of Python required to run your project.
    python_requires='>=3.9',

    # --- Classifiers ---
    # These are standard markers that help tools like pip and PyPI categorize your project.
    classifiers=[
        "Development Status :: 4 - Beta",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Operating System :: OS Independent",
        "Intended Audience :: Developers",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Database",
        "Framework :: Chat Systems",
    ],
)

================================================================================
--- File: .env.example ---
================================================================================



================================================================================
--- File: .env ---
================================================================================

# --- Triton (Jina Embedder) Connection ---
#TRITON_EMBEDDER_URL="http://localhost:6000/"
TRITON_EMBEDDER_URL="http://103.180.245.115:3032/"

# --- vLLM OpenAI-Compatible Endpoint ---
VLLM_MEDIUM_BASE_URL="http://172.22.11.241/gemma3/v1/"
VLLM_MEDIUM_API_KEY="24xAAxh8UW1sF0KZsziePgpRTI9sSxXqxCcnWcjH7Bs=" 
VLLM_MEDIUM_MODEL_NAME="RedHatAI/gemma-3-27b-it-FP8-dynamic"

VLLM_SMALL_BASE_URL="http://localhost:5000/v1/"
VLLM_SMALL_API_KEY="rYGBOPZ8meDvpJQRj4aF9hJJlatgApirBCCEBLICTL40sGemma327Bit" 
VLLM_SMALL_MODEL_NAME="RedHatAI/gemma-3-4b-it-FP8-dynamic"

VLLM_RERANKER_MODEL="small" 

# --- chroma ------
CHROMA_DB_HOST="localhost"
CHROMA_DB_PORT="8443"

#--- Elastic Search----
ES_HOST = "http://localhost:9200"

================================================================================
--- File: govtchat.service ---
================================================================================

# /etc/systemd/system/govtchat.service
#
# Systemd unit file for the Chat Agent API Service
# Corrected for user 'vpa' with Conda environment 'cogops'.

[Unit]
Description=Chat Agent API Service
# Ensures the network is available before attempting to start.
After=network.target

[Service]
# --- User and Environment Configuration ---
# Run the service as the 'vpa' user.
User=vpa
Group=vpa

# The absolute path to your project's root directory.
# This is crucial for relative paths in your code to work correctly.
WorkingDirectory=/home/vpa/CogOpsCB

# The absolute path to your project's .env file.
# This will load all your environment variables (API keys, etc.).
EnvironmentFile=/home/vpa/CogOpsCB/.env

# --- Execution Command ---
# The command to start the FastAPI application.
# It uses the absolute path to the uvicorn executable within your specific Conda environment.
# We also specify the file name 'api:app' which is standard.
# The --workers flag runs multiple processes to handle more concurrent requests.
ExecStart=/home/vpa/miniconda3/envs/cogops/bin/uvicorn api:app --host 0.0.0.0 --port 9000 --workers 4

# --- Service Behavior ---
# Always restart the service if it stops unexpectedly (e.g., crashes).
Restart=always
# Wait 10 seconds before attempting to restart.
RestartSec=10

# --- Logging ---
# Redirect stdout and stderr to the system's journal.
StandardOutput=journal
StandardError=journal
# A unique identifier for this service's logs in the system journal.
SyslogIdentifier=chat-agent-api

[Install]
# Enable the service to start automatically on system boot.
WantedBy=multi-user.target

================================================================================
--- File: tests/local_test.py ---
================================================================================

# client_example_with_gateway.py

import requests
import json

# --- CONFIGURATION ---
# The client now ONLY needs to know the gateway's address.
GATEWAY_URL = "http://localhost:24434"  # <-- CHANGE HERE: Point to your gateway's port

# Construct the full endpoint URLs by adding the gateway prefix.
CHAT_API_ENDPOINT = f"{GATEWAY_URL}/govtchat/chat/stream"      # <-- CHANGE HERE
CLEAR_API_ENDPOINT = f"{GATEWAY_URL}/govtchat/chat/clear_session" # <-- CHANGE HERE

# --- Conversation with User 1 ---
user_1_id = "user-alex-123"
print(f"--- Starting conversation for {user_1_id} via API Gateway ---")

queries_user_1 = [
    "জন্ম নিবন্ধন করার প্রক্রিয়া কি?",
    "নিবন্ধন নম্বর হারিয়ে ফেলেছি কি করতে পারি ?",
    "আমার মামা অসুস্থ । আমি যশোরের হাসপাতালের নাম্বার চাই ", 
    "ই-পাসপোর্টের নির্দেশনাবলি",
    "২০০৮ সালে আমার পাসপোর্ট হায়ায়ে গেছে । আমার করনীয় কি ? আমি কি স্থনীয় পাস্পোর্ট অফিসে যাবও?",
    "ধন্যবাদ"
]

for query in queries_user_1:
    print(f"\n>>> User: {query}")
    print("<<< Bot: ", end="", flush=True)
    
    # The payload remains exactly the same.
    payload = {"user_id": user_1_id, "query": query}
    
    try:
        # Use the new CHAT_API_ENDPOINT
        with requests.post(CHAT_API_ENDPOINT, json=payload, stream=True) as response:
            response.raise_for_status()
            
            final_sources = []
            # The logic for processing the streamed response is unchanged.
            for line in response.iter_lines():
                if line:
                    event = json.loads(line.decode('utf-8'))
                    if event["type"] == "answer_chunk":
                        print(event["content"], end="", flush=True)
                    elif event["type"] == "final_data":
                        final_sources = event["content"].get("sources", [])
                    elif event["type"] == "error":
                        print(f"\n[ERROR]: {event['content']}", end="", flush=True)
            
            if final_sources:
                print(f"\n[তথ্যসূত্র: {', '.join(final_sources)}]")
            print()
            
    except requests.exceptions.RequestException as e:
        print(f"\nAn error occurred: {e}")

print("\n" + "="*50 + "\n")

# --- Clearing the session for User 1 ---
print(f"--- Clearing session for {user_1_id} ---")
try:
    clear_payload = {"user_id": user_1_id}
    # Use the new CLEAR_API_ENDPOINT
    response = requests.post(CLEAR_API_ENDPOINT, json=clear_payload)
    response.raise_for_status()
    print(response.json())
except requests.exceptions.RequestException as e:
    print(f"\nAn error occurred while clearing session: {e}")

================================================================================
--- File: ingestion/chroma_vector_from_csv.py ---
================================================================================

import os
import yaml
import argparse
import pandas as pd
import chromadb
import logging
from dotenv import load_dotenv
from tqdm import tqdm  # <-- Import tqdm for the progress bar
import time
# Import your custom embedder module
from cogops.models.jina_embedder import JinaTritonEmbedder, JinaV3TritonEmbedderConfig
load_dotenv()
# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Infrastructure Configuration (from Environment Variables or Defaults) ---
TRITON_URL = os.environ.get("TRITON_EMBEDDER_URL", "http://localhost:6000")
CHROMA_HOST = os.environ.get("CHROMA_DB_HOST", "localhost")
CHROMA_PORT = int(os.environ.get("CHROMA_DB_PORT", 8443)) # Default Chroma port is 8443

print(TRITON_URL)

def load_data_config(config_path: str) -> dict:
    """Loads the YAML data configuration file."""
    logging.info(f"Loading data configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    logging.info("Data configuration loaded successfully.")
    return config

def main(config_path: str):
    """Main function to run the data ingestion process."""
    load_dotenv()
    data_config = load_data_config(config_path)
    collection_name = data_config['collection_name']

    # 1. Initialize our custom embedder using infrastructure config
    logging.info(f"Initializing embedder with Triton at: {TRITON_URL}")
    embedder_config = JinaV3TritonEmbedderConfig(triton_url=TRITON_URL)
    embedder = JinaTritonEmbedder(config=embedder_config)

    try:
        # 2. Connect to ChromaDB server
        logging.info(f"Connecting to ChromaDB at {CHROMA_HOST}:{CHROMA_PORT}")
        chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
        heartbeat = chroma_client.heartbeat()
        logging.info(f"✅ Connection successful! Server heartbeat: {heartbeat}ns")

        # 3. Clear the collection if it already exists
        try:
            logging.info(f"Checking for existing collection '{collection_name}'...")
            chroma_client.delete_collection(name=collection_name)
            logging.info(f"Successfully deleted existing collection '{collection_name}'.")
        except ValueError:
            logging.info(f"Collection '{collection_name}' does not exist. A new one will be created.")
        except Exception as e:
            logging.error(f"An error occurred while trying to delete collection: {e}")
            #raise

        # 4. Get or create the collection
        passage_embedding_function = embedder.as_chroma_passage_embedder()
        collection = chroma_client.get_or_create_collection(
            name=collection_name,
            embedding_function=passage_embedding_function
        )
        logging.info(f"Collection '{collection_name}' is ready.")

        # 5. Read and Process CSV Data
        logging.info(f"Reading CSV file from: {data_config['csv_file_path']}")
        df = pd.read_csv(data_config['csv_file_path'])

        # Validate required columns
        content_col = data_config['content_column']
        metadata_cols = data_config['metadata_columns']
        # ... (validation logic can be added here)

        # Process data in batches
        ingestion_batch_size = data_config.get('batch_size', 64)
        total_rows = len(df)
        
        # <-- Wrap the loop with tqdm for a progress bar
        for i in tqdm(range(0, total_rows, ingestion_batch_size), desc="Ingesting Batches"):
            batch_df = df.iloc[i:i + ingestion_batch_size]
            
            # Prepare data for ChromaDB based on config mapping
            documents = batch_df[content_col].astype(str).tolist()
            metadatas = batch_df[metadata_cols].to_dict('records')
            ids = [f"row_{j}" for j in range(i, i + len(batch_df))]
            
            # Add the batch to the collection. ChromaDB will call the embedder.
            collection.add(documents=documents, metadatas=metadatas, ids=ids)
            
            #time.sleep(5)

        logging.info("✅ All batches processed and added successfully!")
        logging.info(f"Collection now contains {collection.count()} documents.")

    except FileNotFoundError:
        logging.error(f"Error: The file '{data_config['csv_file_path']}' was not found.")
    except Exception as e:
        logging.error(f"An error occurred during the ingestion process: {e}", exc_info=True)
    finally:
        embedder.close()
        logging.info("--- Data Insertion Script Finished ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Ingest data into ChromaDB from a CSV file.")
    parser.add_argument("--config", type=str, default="chroma_ingestion_config.yaml", help="Path to data config YAML file.")
    args = parser.parse_args()
    main(args.config)

================================================================================
--- File: ingestion/test_inverse_index.py ---
================================================================================

# test_search.py

import argparse
import logging
import os
import sys
import yaml
from dotenv import load_dotenv
from elasticsearch import Elasticsearch

# --- Setup Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

def load_config(config_path):
    """Loads the YAML configuration file."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            logging.info(f"Loading configuration from: {config_path}")
            return yaml.safe_load(f)
    except FileNotFoundError:
        logging.error(f"Configuration file not found at: {config_path}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading or parsing configuration file: {e}")
        sys.exit(1)

def create_es_client(es_host):
    """Establishes and verifies a connection to Elasticsearch."""
    logging.info(f"Connecting to Elasticsearch at {es_host}...")
    try:
        client = Elasticsearch(es_host)
        if not client.ping():
            raise ConnectionError("Connection failed. The ping was unsuccessful.")
        logging.info("✅ Elasticsearch connection successful!")
        return client
    except Exception as e:
        logging.error(f"❌ Failed to connect to Elasticsearch: {e}")
        sys.exit(1)

def run_search(client, query_text, config):
    """
    Constructs and executes a multi_match query against the specified index
    and prints the formatted results.
    """
    index_name = config['elasticsearch']['index_name']
    fields_config = config['elasticsearch']['fields']

    # Check if the index exists before querying
    if not client.indices.exists(index=index_name):
        logging.error(f"Index '{index_name}' does not exist. Please run the ingestion script first.")
        return

    # Dynamically construct the list of fields to search over, with boosts.
    # Boosting makes matches in certain fields (like our stemmed field) more important.
    search_fields = [
        f"{fields_config['english_analyzed']}^2",      # Boost English matches
        fields_config['bengali_analyzed'],             # Standard Bengali matches
        f"{fields_config['bengali_stemmed']}^4"         # Highest boost for custom stemmed field
    ]

    # This is the core multi-match query. It searches the user's text
    # against all our specified fields and uses the best score (BM25).
    query_body = {
        "query": {
            "multi_match": {
                "query": query_text,
                "fields": search_fields,
                "type": "best_fields"
            }
        }
    }

    # --- Print Query Information ---
    print("\n" + "="*80)
    print(f"Executing Query: '{query_text}'")
    print(f"Searching fields: {search_fields}")
    print("="*80)

    try:
        response = client.search(index=index_name, body=query_body, size=3) # Get top 3 results

        hits = response['hits']['hits']
        if not hits:
            print("--- No results found. ---")
            return

        print(f"Found {response['hits']['total']['value']} results. Showing top {len(hits)}:\n")
        for i, hit in enumerate(hits):
            score = hit['_score']
            # Display the original, clean text from the raw field for readability
            passage_text = hit['_source'][fields_config['raw']]
            print(f"  Result {i+1} | Score: {score:.4f}")
            print(f"  Text: {passage_text}\n")

    except Exception as e:
        logging.error(f"An error occurred during search: {e}")


def main():
    """Main entry point for the test script."""
    parser = argparse.ArgumentParser(
        description="Test search queries against a configured Elasticsearch index."
    )
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help="Path to the inverse_index.yaml configuration file."
    )
    args = parser.parse_args()

    # --- Load Environment and Configuration ---
    load_dotenv()
    es_host = os.getenv("ES_HOST")
    if not es_host:
        logging.error("ES_HOST environment variable not found. Please create a .env file.")
        sys.exit(1)

    config = load_config(args.config)
    es_client = create_es_client(es_host)

    # --- Define Query Variations ---
    test_queries = {
        "✅ Vanilla Query": "এন আই ডি কার্ড করার প্রক্রিয়া কি?",
        "✅ Personal Query": "আমার চাচা অসুস্থ, আমি রাজশাহীতে আছি। জরুরি অ্যাম্বুলেন্সের নাম্বার লাগবে।",
        "❌ Non-Service Query": "আজকে আমার দিনটা খুব খারাপ গেলো",
        "⚠️ Ambiguous Query": "আমি ফর্ম পূরণ করতে চাই",
        "✅ Compound Query": "আমি পাসপোর্ট বানানোর নিয়ম জানতে চাই আর অফিসের ঠিকানা কোথায়?",
        "✅ Mixed-Language Query": "How to get birth certificate online?"
    }

    # --- Run Tests ---
    for category, query in test_queries.items():
        print(f"\n\n{'='*20} Testing Category: {category} {'='*20}")
        run_search(es_client, query, config)

if __name__ == "__main__":
    main()

================================================================================
--- File: ingestion/test_chroma_vector.py ---
================================================================================

# test_chroma_search.py

import argparse
import logging
import os
import sys
import yaml
from dotenv import load_dotenv
import chromadb

# Import the same custom embedder modules used during ingestion
from cogops.models.jina_embedder import JinaTritonEmbedder, JinaV3TritonEmbedderConfig

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)

# --- Load Infrastructure Configuration from Environment ---
load_dotenv()
TRITON_URL = os.environ.get("TRITON_EMBEDDER_URL", "http://localhost:6000")
CHROMA_HOST = os.environ.get("CHROMA_DB_HOST", "localhost")
CHROMA_PORT = int(os.environ.get("CHROMA_DB_PORT", 8443))

def load_config(config_path: str) -> dict:
    """Loads the YAML configuration file."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            logging.info(f"Loading configuration from: {config_path}")
            return yaml.safe_load(f)
    except FileNotFoundError:
        logging.error(f"Configuration file not found at: {config_path}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading or parsing configuration file: {e}")
        sys.exit(1)

def initialize_embedder(triton_url: str) -> JinaTritonEmbedder:
    """Initializes the Jina Triton embedder."""
    logging.info(f"Initializing embedder with Triton at: {triton_url}")
    embedder_config = JinaV3TritonEmbedderConfig(triton_url=triton_url)
    return JinaTritonEmbedder(config=embedder_config)

def run_semantic_search(collection: chromadb.Collection, query_text: str, n_results: int = 3):
    """
    Constructs and executes a semantic search query against the collection
    and prints the formatted results.
    """
    # --- Print Query Information ---
    print("\n" + "="*80)
    print(f"Executing Query: '{query_text}'")
    print("="*80)

    try:
        # The core query operation in ChromaDB.
        # It takes the query text, embeds it using the collection's embedding function,
        # and finds the 'n_results' most similar documents.
        results = collection.query(
            query_texts=[query_text],
            n_results=n_results,
            include=["metadatas", "documents", "distances"] # Ask for all useful info
        )

        # The result structure is a dictionary of lists of lists.
        # We access the first list [0] because we only sent one query.
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results['distances'][0]

        if not documents:
            print("--- No results found. ---")
            return

        print(f"Showing top {len(documents)} semantic search results:\n")
        for i in range(len(documents)):
            distance = distances[i]
            doc_content = documents[i]
            metadata = metadatas[i]

            print(f"  Result {i+1} | Distance: {distance:.4f} (Lower is better)")
            print(f"  Text: {doc_content}")
            print(f"  Metadata: {metadata}\n")

    except Exception as e:
        logging.error(f"An error occurred during search: {e}", exc_info=True)


def main():
    """Main entry point for the test script."""
    parser = argparse.ArgumentParser(
        description="Test semantic search queries against a ChromaDB collection."
    )
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help="Path to the chroma_ingestion_config.yaml file."
    )
    args = parser.parse_args()

    config = load_config(args.config)
    collection_name = config['collection_name']
    embedder = None  # Initialize embedder to None

    try:
        # --- Connect to Services ---
        logging.info(f"Connecting to ChromaDB at {CHROMA_HOST}:{CHROMA_PORT}")
        chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
        chroma_client.heartbeat() # Verify connection
        logging.info("✅ ChromaDB connection successful!")
        
        # We must initialize the embedder to define the embedding function for the collection
        embedder = initialize_embedder(TRITON_URL)
        passage_embedding_function = embedder.as_chroma_passage_embedder()

        # Get the existing collection. This script assumes ingestion is already done.
        logging.info(f"Accessing existing collection: '{collection_name}'")
        collection = chroma_client.get_collection(
            name=collection_name,
            embedding_function=passage_embedding_function
        )
        logging.info(f"Successfully connected to collection with {collection.count()} documents.")

        # --- Define Query Variations ---
        test_queries = {
            "✅ Vanilla Query": "এন আই ডি কার্ড করার প্রক্রিয়া কি?",
            "✅ Personal Query": "আমার চাচা অসুস্থ, আমি রাজশাহীতে আছি। জরুরি অ্যাম্বুলেন্সের নাম্বার লাগবে।",
            "❌ Non-Service Query": "আজকে আমার দিনটা খুব খারাপ গেলো",
            "⚠️ Ambiguous Query": "আমি ফর্ম পূরণ করতে চাই",
            "✅ Compound Query": "আমি পাসপোর্ট বানানোর নিয়ম জানতে চাই আর অফিসের ঠিকানা কোথায়?",
            "✅ Mixed-Language Query": "How to get birth certificate online?"
        }

        # --- Run Tests ---
        for category, query in test_queries.items():
            print(f"\n\n{'='*20} Testing Category: {category} {'='*20}")
            run_semantic_search(collection, query, n_results=3)

    except ValueError as e:
        # This error is often raised by ChromaDB if the collection doesn't exist.
        logging.error(f"Could not find collection '{collection_name}'. Please run the ingestion script first. Details: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}", exc_info=True)
    finally:
        if embedder:
            embedder.close() # Important to close the connection to Triton
            logging.info("Embedder connection closed.")
        logging.info("--- Search Test Script Finished ---")

if __name__ == "__main__":
    main()

================================================================================
--- File: ingestion/inverse_index_from_csv.py ---
================================================================================

# ingest_data.py

import argparse
import logging
import os
import sys
import yaml
import pandas as pd
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import re

# Using the 'bangla-stemmer' library by Fatick DevStudio.
from bangla_stemmer.stemmer import stemmer as fatick_stemmer

# --- Setup Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

def load_config(config_path):
    """Loads the YAML configuration file."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            logging.info(f"Loading configuration from: {config_path}")
            return yaml.safe_load(f)
    except FileNotFoundError:
        logging.error(f"Configuration file not found at: {config_path}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading or parsing configuration file: {e}")
        sys.exit(1)

def create_es_client(es_host):
    """Establishes and verifies a connection to Elasticsearch."""
    logging.info(f"Connecting to Elasticsearch at {es_host}...")
    try:
        client = Elasticsearch(es_host)
        if not client.ping():
            raise ConnectionError("Connection failed. The ping was unsuccessful.")
        logging.info("✅ Elasticsearch connection successful!")
        return client
    except Exception as e:
        logging.error(f"❌ Failed to connect to Elasticsearch: {e}")
        sys.exit(1)

def create_index_with_mapping(client, index_name, fields_config):
    """
    Creates an Elasticsearch index. If the index already exists, it is
    deleted first to ensure a clean slate.
    """
    # --- THIS IS THE CRUCIAL SECTION ---
    # Check if the index specified in the YAML config already exists.
    if client.indices.exists(index=index_name):
        # If it exists, log a warning and delete it. This is essential for
        # ensuring that any changes to the mapping are applied correctly
        # on every run.
        logging.warning(f"Index '{index_name}' already exists. Deleting it for a fresh start.")
        client.indices.delete(index=index_name)
    # --- END OF SECTION ---

    # Dynamically build the properties mapping from the YAML config
    properties = {
        fields_config['raw']: {"type": "keyword"},
        fields_config['english_analyzed']: {"type": "text", "analyzer": "english"},
        fields_config['bengali_analyzed']: {"type": "text", "analyzer": "bengali"},
        fields_config['bengali_stemmed']: {"type": "text", "analyzer": "whitespace"}
    }
    
    index_body = {"mappings": {"properties": properties}}

    try:
        logging.info(f"Creating new index '{index_name}' with advanced mapping...")
        client.indices.create(index=index_name, body=index_body)
        logging.info("✅ Index created successfully.")
    except Exception as e:
        logging.error(f"❌ Failed to create index: {e}")
        sys.exit(1)

def stem_bengali_text(text, stemmer_instance):
    """Pre-stems Bengali text using the specified external library."""
    if not isinstance(text, str):
        return ""
    words = re.findall(r'[\u0980-\u09FF\w]+', text)
    stemmed_words = [stemmer_instance.stem(word) for word in words]
    return " ".join(stemmed_words)

def generate_bulk_actions(df, config):
    """
    Generator function that pre-processes data and yields documents for bulk indexing.
    """
    stemmer_instance = fatick_stemmer.BanglaStemmer()
    logging.info(f"Preparing and stemming {len(df)} documents for indexing...")

    id_col = config['data_source']['id_column']
    text_col = config['data_source']['source_text_column']
    index_name = config['elasticsearch']['index_name']
    fields = config['elasticsearch']['fields']
    
    for _, row in df.iterrows():
        source_text = row[text_col]
        stemmed_text = stem_bengali_text(source_text, stemmer_instance)

        document = {
            fields['raw']: source_text,
            fields['english_analyzed']: source_text,
            fields['bengali_analyzed']: source_text,
            fields['bengali_stemmed']: stemmed_text,
        }
        
        yield {
            "_index": index_name,
            "_id": row[id_col],
            "_source": document,
        }

def run_ingestion(config, client):
    """Orchestrates the data reading and bulk ingestion process."""
    try:
        csv_path = config['data_source']['csv_path']
        id_col = config['data_source']['id_column']
        text_col = config['data_source']['source_text_column']

        logging.info(f"Reading data from CSV file: {csv_path}")
        df = pd.read_csv(csv_path)
        
        df.dropna(subset=[text_col], inplace=True)
        df[id_col] = df[id_col].astype(str)

        logging.info("Starting bulk ingestion process. This may take a while...")
        success, failed = bulk(client, generate_bulk_actions(df, config), raise_on_error=False, chunk_size=500)
        
        logging.info("✅ Ingestion complete.")
        logging.info(f"   Successfully indexed documents: {success}")
        if failed:
            logging.warning(f"   Failed to index documents: {len(failed)}")

    except FileNotFoundError:
        logging.error(f"❌ Error: The file was not found at '{csv_path}'. Please check your YAML config.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"❌ An unexpected error occurred during ingestion: {e}")
        sys.exit(1)

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(description="Ingest mixed-language data into Elasticsearch.")
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help="Path to the inverse_index.yaml configuration file."
    )
    args = parser.parse_args()

    load_dotenv()
    es_host = os.getenv("ES_HOST")
    if not es_host:
        logging.error("ES_HOST environment variable not found. Please create a .env file.")
        sys.exit(1)

    config = load_config(args.config)

    es_client = create_es_client(es_host)
    create_index_with_mapping(
        client=es_client,
        index_name=config['elasticsearch']['index_name'],
        fields_config=config['elasticsearch']['fields']
    )
    run_ingestion(config, es_client)

if __name__ == "__main__":
    main()

================================================================================
--- File: ingestion/configs/inverse_index.yaml ---
================================================================================

# ===================================================================
# Configuration for the Elasticsearch Mixed-Language Indexing Pipeline
# ===================================================================

# --- Data Source Configuration ---
# Specifies where to find the input data and which columns to use.
data_source:
  # The relative or absolute path to your input CSV file.
  csv_path: "/home/vpa/downloads/data/processed_data_20250830.csv"

  # The name of the column in the CSV that contains the unique ID for each passage.
  id_column: "passage_id"

  # The name of the column that contains the mixed Bangla and English text.
  source_text_column: "passage"


# --- Elasticsearch Configuration ---
# Settings related to the Elasticsearch instance and the target index.
elasticsearch:
  # The name of the index you want to create or update in Elasticsearch.
  # Using version numbers (e.g., v1, v2) is a good practice for easier migrations.
  index_name: "govt_services_passages_latest"

  # The names of the fields that will be created within each document in the index.
  # This allows for easy modification without changing the core code.
  fields:
    # Field to store the original, untouched text. Useful for display.
    raw: "passage_raw"

    # Field to be processed by Elasticsearch's built-in English analyzer (stemming, stopwords).
    english_analyzed: "passage_en"

    # Field to be processed by Elasticsearch's built-in Bengali analyzer (normalization).
    bengali_analyzed: "passage_bn"

    # Field to store the Bengali text after it has been pre-stemmed in Python.
    bengali_stemmed: "passage_bn_stemmed"

================================================================================
--- File: ingestion/configs/chroma_config.yml ---
================================================================================

# -----------------------------------------------------------------
# Data Configuration for the ChromaDB Ingestion Script
#
# This file defines the source data, the target collection,
# and how to map the data columns.
# -----------------------------------------------------------------

# The name of the collection to create or use in ChromaDB
collection_name: "KeywordDB"

# Relative or absolute path to the input CSV file
csv_file_path: "/home/vpa/Downloads/data/master_data_20250830.csv"

# --- Data Mapping ---

# The column in the CSV containing the document text to embed
content_column: "keyword" 

# A list of columns from the CSV to store as metadata.
# Each of these columns must exist in the CSV file.
metadata_columns:
  - "category"
  - "passage_id"
  - "url"
  
# --- Ingestion Settings ---

# Number of documents to process and insert in a single batch
batch_size: 1

================================================================================
--- File: cogops/__init__.py ---
================================================================================



================================================================================
--- File: cogops/agent.py ---
================================================================================

import os
import yaml
import asyncio
from typing import AsyncGenerator, Dict, Any, List, Tuple

# Environment Variable Loading
from dotenv import load_dotenv
load_dotenv()

# --- Prompt Imports ---
from cogops.prompts.retrive import RetrievalPlan, retrive_prompt
from cogops.prompts.service import CATEGORY_LIST,SERVICE_DATA
from cogops.prompts.response import response_router
from cogops.prompts.answer import ANSWER_GENERATION_PROMPT,SYNTHESIS_ANSWER_PROMPT 
from cogops.prompts.summary import SUMMARY_GENERATION_PROMPT
from cogops.prompts.pivot import HELPFUL_PIVOT_PROMPT 

# --- Core Component Imports ---
from cogops.models.gemma3_llm import LLMService
from cogops.models.gemma3_llm_async import AsyncLLMService
from cogops.retriver.vector_search import VectorRetriever
from cogops.retriver.reranker import ParallelReranker
from cogops.utils.string import refine_category


class ChatAgent:
    """
    A configurable, end-to-end conversational agent for government services,
    orchestrating retrieval, reranking, and response generation.
    """
    def __init__(self, config_path: str = "configs/config.yaml"):
        print("Initializing ChatAgent...")
        self.config = self._load_config(config_path)
        self.llm_services_sync: Dict[str, LLMService] = {}
        self.llm_services_async: Dict[str, AsyncLLMService] = {}
        self._initialize_llm_services()
        self.task_models_async = {
            task: self.llm_services_async[model_name]
            for task, model_name in self.config['task_to_model_mapping'].items()
        }
        self.vector_retriever = VectorRetriever(config_path=config_path)
        self.reranker = ParallelReranker(llm_service=self.task_models_async['reranker'])
        self.relevance_score_threshold = self.config['reranker']['relevance_score_threshold']
        self.history: List[Tuple[str, str]] = []
        self.history_window = self.config['conversation']['history_window']
        self.llm_call_params = self.config['llm_call_parameters']
        self.response_templates = self.config['response_templates']
        self.category_refinement_cutoff = self.config['category_refinement']['score_cutoff']
        print("✅ ChatAgent initialized successfully with the specified configuration.")

    def _load_config(self, config_path: str) -> Dict:
        print(f"Loading configuration from: {config_path}")
        try:
            with open(config_path, 'r') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"[FATAL ERROR] Configuration file not found at: {config_path}")
            raise
    
    def _initialize_llm_services(self):
        print("Initializing LLM services...")
        for name, cfg in self.config['llm_services'].items():
            api_key, model, url = os.getenv(cfg['api_key_env']), os.getenv(cfg['model_name_env']), os.getenv(cfg['base_url_env'])
            if not all([api_key, model, url]):
                raise ValueError(f"Missing environment variables for LLM service '{name}'")
            self.llm_services_sync[name] = LLMService(api_key, model, url)
            self.llm_services_async[name] = AsyncLLMService(api_key, model, url)
        print("LLM services are ready.")

    def _format_history(self) -> str:
        if not self.history: return "No conversation history yet."
        return "\n---\n".join([f"User: {u}\nAI: {a}" for u, a in self.history])

    async def process_query(self, user_query: str) -> AsyncGenerator[Dict[str, Any], None]:
        print(f"\n--- New Query Received: '{user_query}' ---")
        history_str = self._format_history()

        try:
            planner_llm = self.task_models_async['retrieval_plan']
            planner_params = self.llm_call_params['retrieval_plan']
            planner_prompt = retrive_prompt.format(CATEGORY_LIST, history_str, user_query)
            plan = await planner_llm.invoke_structured(planner_prompt, RetrievalPlan, **planner_params)
            print(f"Retrieval Plan: {plan.model_dump_json(indent=2)}")
        except Exception as e:
            print(f"[ERROR] Failed to generate a valid retrieval plan: {e}")
            yield {"type": "error", "content": self.response_templates['plan_generation_failed']}
            return

        if plan.query_type == "AMBIGUOUS" and plan.clarification:
            self.history.append((user_query, plan.clarification))
            for char in plan.clarification:
                yield {"type": "answer_chunk", "content": char}
                await asyncio.sleep(0.01)
            return

        non_retrieval_types = ["OUT_OF_DOMAIN_GOVT_SERVICE_INQUIRY", "GENERAL_KNOWLEDGE", "CHITCHAT", "ABUSIVE_SLANG"]
        if plan.query_type in non_retrieval_types:
            responder_llm = self.task_models_async['non_retrieval_responder']
            responder_params = self.llm_call_params['non_retrieval_responder']
            prompt = response_router(plan.model_dump(), history_str, user_query)
            full_answer = "".join([chunk async for chunk in responder_llm.stream(prompt, **responder_params)])
            yield {"type": "answer_chunk", "content": full_answer}
            self.history.append((user_query, full_answer))

        elif plan.query_type == "IN_DOMAIN_GOVT_SERVICE_INQUIRY":
            refined_cat = refine_category(plan.category, CATEGORY_LIST, self.category_refinement_cutoff)
            filters = {"category": refined_cat} if refined_cat else None
            retrieved = await self.vector_retriever.get_unique_passages_from_all_collections(plan.query, filters=filters)
            if not retrieved:
                yield {"type": "answer_chunk", "content": self.response_templates['no_passages_found']}
                return

            reranker_params = self.llm_call_params['reranker']
            reranked = await self.reranker.rerank(history_str, user_query, retrieved, **reranker_params)
            relevant_passages = [p for p in reranked if p.score <= self.relevance_score_threshold]
            # --- MODIFIED SECTION: What to do when no relevant passages are found ---
            if not relevant_passages:
                print("No highly relevant passages found. Generating a helpful pivot response.")
                
                # Use the responder model and its params for this generative task
                responder_llm = self.task_models_async['non_retrieval_responder']
                responder_params = self.llm_call_params['non_retrieval_responder']
                
                # Format the new pivot prompt
                pivot_prompt = HELPFUL_PIVOT_PROMPT.format(
                    history=history_str,
                    user_query=user_query,
                    category=refined_cat,
                    service_data=SERVICE_DATA
                )

                # Stream the dynamically generated pivot response
                full_answer_list = []
                async for chunk in responder_llm.stream(pivot_prompt, **responder_params):
                    full_answer_list.append(chunk)
                    yield {"type": "answer_chunk", "content": chunk}
                
                final_answer = "".join(full_answer_list).strip()
                # IMPORTANT: Add this interaction to history
                self.history.append((user_query, final_answer))
                return # End the processing here for this query
            # --- END MODIFIED SECTION ---

            context = "\n\n".join([f"Passage ID: {p.passage_id}\nContent: {p.document}" for p in relevant_passages])
            answer_llm = self.task_models_async['answer_generator']
            answer_params = self.llm_call_params['answer_generator']
            answer_prompt = SYNTHESIS_ANSWER_PROMPT.format(history=history_str, user_query=user_query, passages_context=context)
            
            full_answer_list = []
            async for chunk in answer_llm.stream(answer_prompt, **answer_params):
                full_answer_list.append(chunk)
                yield {"type": "answer_chunk", "content": chunk}
            final_answer = "".join(full_answer_list).strip()

            # --- MODIFIED SECTION: Building the Final Sources List ---
            unique_urls = set()
            unique_passage_ids = set()
            
            for passage in relevant_passages:
                # Add the correct passage_id from the reranked object
                unique_passage_ids.add(passage.passage_id)
                
                # If metadata and a URL exist, add the URL
                if passage.metadata and passage.metadata.get("url"):
                    unique_urls.add(passage.metadata["url"])

            # Combine sorted lists for a deterministic output
            final_sources = sorted(list(unique_urls)) + sorted(list(unique_passage_ids))
            # --- END MODIFIED SECTION ---

            yield {"type": "final_data", "content": {"sources": final_sources}}

            summarizer_llm = self.task_models_async['summarizer']
            summarizer_params = self.llm_call_params['summarizer']
            summary_prompt = SUMMARY_GENERATION_PROMPT.format(user_query=user_query, final_answer=final_answer)
            summary = await summarizer_llm.invoke(summary_prompt, **summarizer_params)
            self.history.append((user_query, summary.strip()))
            #self.history.append((user_query, final_answer.strip()))

        if len(self.history) > self.history_window:
            self.history.pop(0)


async def main():
    """Main function to demonstrate the fully configured ChatAgent."""
    try:
        agent = ChatAgent(config_path="configs/config.yaml")
        queries = [
            "হ্যালো, কেমন আছো?",
            "আমার এনআইডি কার্ড হারিয়ে গেছে, এখন কি করব?",
            "সেটার জন্য কত টাকা লাগবে?",
            "আমি মাছ ধরার লাইসেন্স করতে চাই।",
            "what is the capital of france"
        ]

        for query in queries:
            print("\n" + "="*50 + f"\nUser Query: {query}\n" + "="*50)
            print("AI Response: ", end="", flush=True)
            final_data = None
            
            async for event in agent.process_query(query):
                if event["type"] == "answer_chunk":
                    print(event["content"], end="", flush=True)
                elif event["type"] == "final_data":
                    final_data = event["content"]
                elif event["type"] == "error":
                    print(f"\n[ERROR] {event['content']}")
            
            if final_data:
                print(f"\n\n--- Final Data Received ---\n{final_data}")
            print("\n")
            
    except Exception as e:
        print(f"\nAn error occurred during the main execution: {e}")


if __name__ == "__main__":
    asyncio.run(main())

================================================================================
--- File: cogops/utils/string.py ---
================================================================================

import re
from fuzzywuzzy import process, fuzz
from typing import List, Optional

# --- The Normalization Helper ---
def normalize_text(s: str) -> str:
    """
    A helper function to clean up strings for better comparison.
    It removes all whitespace characters, which effectively handles extra spaces,
    tabs, newlines, and different unicode space characters.
    """
    if not isinstance(s, str):
        return ""
    # \s+ matches one or more whitespace characters of any kind
    return re.sub(r'\s+', '', s)

# --- The Main Function ---
def refine_category(
    category_from_llm: str, 
    category_list: List[str],
    score_cutoff: int = 85
) -> Optional[str]:
    """
    Finds the closest matching category from a predefined list for a given category string.

    This function addresses potential issues like extra spaces, minor typos, or
    slight rephrasing from an LLM by using fuzzy string matching.

    Args:
        category_from_llm (str): The category string generated by the language model.
        category_list (List[str]): The canonical list of valid category names.
        score_cutoff (int): The minimum similarity score (0-100) required to consider
                              a string a valid match. Defaults to 85.

    Returns:
        Optional[str]: The best matching category string from the list if the score is
                       above the cutoff, otherwise None.
    """
    if not category_from_llm or not category_from_llm.strip():
        return None

    # The `process.extractOne` function is the core of this solution.
    # It finds the best match for `category_from_llm` from the `choices` in `category_list`.
    #
    # We use a custom processor to normalize both the input and the choices before comparing,
    # which makes the matching much more robust against whitespace issues.
    
    best_match = process.extractOne(
        query=category_from_llm,
        choices=category_list,
        processor=normalize_text,  # Apply our cleaning function
        scorer=fuzz.ratio,         # Use a simple and fast similarity scorer
        score_cutoff=score_cutoff  # Don't return matches below this score
    )

    # extractOne returns a tuple (choice, score) or None if no match is found above the cutoff
    if best_match:
        return best_match[0]  # Return only the category name (the first element)
    
    return None

================================================================================
--- File: cogops/models/gemma3_llm.py ---
================================================================================

import os
import json
from dotenv import load_dotenv
from openai import OpenAI
from typing import Generator, Any, Type, TypeVar, List

from pydantic import BaseModel, Field

# Load environment variables from a .env file
load_dotenv()

# Generic type variable for Pydantic models for clean type hinting.
PydanticModel = TypeVar("PydanticModel", bound=BaseModel)


class LLMService:
    """
    A synchronous client for OpenAI-compatible APIs using the 'openai' library.

    This service supports standard, streaming, and structured (Pydantic-validated)
    responses. Structured output is achieved using the 'response_format'
    parameter for broad model compatibility.
    """

    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: str
    ):
        """
        Initializes the client using the OpenAI library.

        Args:
            api_key (str): The API key for authentication.
            model (str): The name of the model to use for requests.
            base_url (str): The base URL of the API service.
        """
        if not api_key:
            raise ValueError("API key cannot be empty. Please set it in your .env file or pass it directly.")
        
        self.model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        
        print(f"✅ LLMService initialized for model '{self.model}' using 'response_format' for structured output.")

    def invoke(self, prompt: str, **kwargs: Any) -> str:
        """
        Sends a request for a single, complete response (non-streaming).
        """
        messages = [{"role": "user", "content": prompt}]
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                **kwargs
            )
            return response.choices[0].message.content or ""
        except Exception as e:
            print(f"\n[Error] An error occurred during invoke: {e}")
            raise

    def stream(self, prompt: str, **kwargs: Any) -> Generator[str, None, None]:
        """
        Connects to the streaming endpoint and yields text chunks as they arrive.
        """
        messages = [{"role": "user", "content": prompt}]
        
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
                **kwargs
            )
            for chunk in stream:
                content_chunk = chunk.choices[0].delta.content
                if content_chunk:
                    yield content_chunk
        except Exception as e:
            print(f"\n[Error] An error occurred during stream: {e}")
            raise

    def invoke_structured(
        self,
        prompt: str,
        response_model: Type[PydanticModel],
        **kwargs: Any
    ) -> PydanticModel:
        """
        Sends a request for a structured response validated by a Pydantic model.

        This method uses the `response_format={"type": "json_object"}` feature.
        It constructs a detailed prompt that includes the JSON schema of the
        Pydantic model, instructing the LLM to generate a conforming JSON object.

        Args:
            prompt (str): The user's core prompt/question.
            response_model (Type[PydanticModel]): The Pydantic model to validate against.
            **kwargs: Additional keyword arguments to pass to the API.

        Returns:
            PydanticModel: An instance of the response_model populated with the LLM's response.
        """
        # 1. Generate the JSON schema from the Pydantic model.
        schema = json.dumps(response_model.model_json_schema(), indent=2)

        # 2. Engineer a new prompt that includes the original prompt and instructions.
        structured_prompt = f"""
        Given the following request:
        ---
        {prompt}
        ---
        Your task is to provide a response as a single, valid JSON object.
        This JSON object must strictly adhere to the following JSON Schema.
        Do not include any extra text, explanations, or markdown formatting (like ```json) outside of the JSON object itself.

        JSON Schema:
        {schema}
        """

        messages = [{"role": "user", "content": structured_prompt}]

        try:
            # 3. Call the API with the 'response_format' parameter.
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                response_format={"type": "json_object"},
                **kwargs
            )

            # 4. The entire response content is the JSON string.
            # <<< FIX: Access the first item in the 'choices' list using >>>
            json_response_str = response.choices[0].message.content
            if not json_response_str:
                raise ValueError("The model returned an empty response.")

            # 5. Parse and validate the JSON string using the Pydantic model.
            return response_model.model_validate_json(json_response_str)

        except Exception as e:
            print(f"\n[Error] An error occurred during structured invoke: {e}")
            raise


if __name__ == '__main__':

    # --- Example Usage ---

    # Define a Pydantic model for the desired structured output
    class Recipe(BaseModel):
        recipe_name: str = Field(description="The title of the recipe.")
        prep_time_minutes: int = Field(description="Time required for preparation in minutes.")
        ingredients: List[str] = Field(description="A list of all necessary ingredients.")
        is_vegetarian: bool = Field(description="True if the recipe contains no meat, false otherwise.")

    # Initialize the service using your API key from the .env file
    api_key = os.getenv("VLLM_MEDIUM_API_KEY")
    model   = os.getenv("VLLM_MEDIUM_MODEL_NAME")
    base_url = os.getenv("VLLM_MEDIUM_BASE_URL")
    if not api_key:
        print("Error: VLLM_MEDIUM_API_KEY not found in .env file.")
    else:
        # Use a model known to support JSON mode, e.g., gpt-4o, gpt-4-turbo,
        # or a compatible open-source model.
        llm_service = LLMService(api_key=api_key, model=model, base_url=base_url)

        # --- Example 1: Standard `invoke` call ---
        print("\n--- 1. Standard Invoke Call ---")
        try:
            response = llm_service.invoke("Who discovered penicillin?")
            print(f"Response:\n{response}")
        except Exception as e:
            print(f"An error occurred: {e}")
            
        # --- Example 2: Structured Invoke Call using JSON Mode ---
        print("\n--- 2. Structured Invoke Call ---")
        try:
            structured_prompt = "Generate a simple recipe for a classic Margherita pizza. It takes about 20 minutes to prepare."
            print(f"Prompt: {structured_prompt}")
            
            recipe_data = llm_service.invoke_structured(structured_prompt, Recipe)
            
            print(f"\nSuccessfully parsed response into a '{type(recipe_data).__name__}' object:")
            print(recipe_data.model_dump_json(indent=2))
            
            print(f"\nAccessing data programmatically:")
            print(f"  Recipe: {recipe_data.recipe_name}")
            print(f"  Prep Time: {recipe_data.prep_time_minutes} minutes")
            print(f"  Vegetarian: {'Yes' if recipe_data.is_vegetarian else 'No'}")
            print(f"  First ingredient: {recipe_data.ingredients}")
            
        except Exception as e:
            print(f"An error occurred: {e}")

        # --- Example 3: Streaming call ---
        print("\n--- 3. Streaming Call ---")
        try:
            stream_prompt = "Write a short, 50-word story about a robot who discovers music for the first time."
            print(f"Prompt: {stream_prompt}\n")
            print("Streaming Response:")
            # Iterate over the generator and print each chunk as it arrives.
            for chunk in llm_service.stream(stream_prompt):
                print(chunk, end="", flush=True)
            print("\n") # Add a newline after the stream is complete
        except Exception as e:
            print(f"An error occurred during stream: {e}")

================================================================================
--- File: cogops/models/gemma3_llm_async.py ---
================================================================================

import os
import json
import asyncio
from dotenv import load_dotenv
from openai import AsyncOpenAI # <-- Import the Async client
from typing import Generator, Any, Type, TypeVar, List, AsyncGenerator

from pydantic import BaseModel, Field

load_dotenv()

PydanticModel = TypeVar("PydanticModel", bound=BaseModel)

class AsyncLLMService:
    """
    An ASYNCHRONOUS client for OpenAI-compatible APIs.
    Supports standard, streaming, and structured (Pydantic-validated) responses.
    """
    def __init__(self, api_key: str, model: str, base_url: str):
        if not api_key:
            raise ValueError("API key cannot be empty.")
        
        self.model = model
        # --- Use the AsyncOpenAI client ---
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        
        print(f"✅ AsyncLLMService initialized for model '{self.model}'.")

    async def invoke(self, prompt: str, **kwargs: Any) -> str:
        messages = [{"role": "user", "content": prompt}]
        try:
            # --- Use 'await' for the async call ---
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                **kwargs
            )
            return response.choices[0].message.content or ""
        except Exception as e:
            print(f"\n[Error] An error occurred during invoke: {e}")
            raise

    async def stream(self, prompt: str, **kwargs: Any) -> AsyncGenerator[str, None]:
        messages = [{"role": "user", "content": prompt}]
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
                **kwargs
            )
            async for chunk in stream: # <-- Use 'async for'
                content_chunk = chunk.choices[0].delta.content
                if content_chunk:
                    yield content_chunk
        except Exception as e:
            print(f"\n[Error] An error occurred during stream: {e}")
            raise

    async def invoke_structured(
        self,
        prompt: str,
        response_model: Type[PydanticModel],
        **kwargs: Any
    ) -> PydanticModel:
        schema = json.dumps(response_model.model_json_schema(), indent=2)
        structured_prompt = f"""
        Given the following request:
        ---
        {prompt}
        ---
        Your task is to provide a response as a single, valid JSON object that strictly adheres to the following JSON Schema.
        Do not include any extra text or markdown formatting.

        JSON Schema:
        {schema}
        """
        messages = [{"role": "user", "content": structured_prompt}]
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                response_format={"type": "json_object"},
                **kwargs
            )
            json_response_str = response.choices[0].message.content
            if not json_response_str:
                raise ValueError("The model returned an empty response.")
            return response_model.model_validate_json(json_response_str)
        except Exception as e:
            print(f"\n[Error] An error occurred during structured invoke: {e}")
            raise

================================================================================
--- File: cogops/models/jina_embedder.py ---
================================================================================

import json
import logging
from typing import Any, Dict, List
import numpy as np
import requests
from chromadb.api.types import Documents, EmbeddingFunction, Embeddings
from pydantic import BaseModel, Field
from transformers import AutoTokenizer

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JinaV3TritonEmbedderConfig(BaseModel):
    """Configuration for the JinaV3TritonEmbedder."""
    triton_url: str = Field(description="Base URL for the Triton Inference Server")
    triton_request_timeout: int = Field(default=480, description="Request timeout in seconds.")
    query_model_name: str = Field(default="jina_query", description="Name of the query model.")
    passage_model_name: str = Field(default="jina_passage", description="Name of the passage model.")
    tokenizer_name: str = Field(default="jinaai/jina-embeddings-v3", description="HF tokenizer name.")
    triton_output_name: str = Field(default="text_embeds", description="Name of the output tensor.")
    batch_size: int = Field(default=8, description="Batch size for embedding requests sent to Triton.")

class _SyncJinaV3TritonEmbedder:
    """Internal synchronous client that handles communication with Triton."""
    def __init__(self, config: JinaV3TritonEmbedderConfig):
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=True)

    def _build_triton_payload(self, texts: List[str]) -> Dict[str, Any]:
        """Prepares the request payload and attention mask for Triton."""
        tokens = self.tokenizer(texts, padding=True, truncation=True, max_length=8192, return_tensors="np")
        input_ids = tokens["input_ids"].astype(np.int64)
        attention_mask = tokens["attention_mask"].astype(np.int64)
        payload = {
            "inputs": [
                {"name": "input_ids", "shape": list(input_ids.shape), "datatype": "INT64", "data": input_ids.flatten().tolist()},
                {"name": "attention_mask", "shape": list(attention_mask.shape), "datatype": "INT64", "data": attention_mask.flatten().tolist()},
            ],
            "outputs": [{"name": self.config.triton_output_name}],
        }
        return payload, tokens['attention_mask']

    def _post_process(self, triton_output: Dict[str, Any], attention_mask: np.ndarray) -> List[List[float]]:
        """Applies mean pooling and normalization to the Triton output."""
        output_data = next((out for out in triton_output["outputs"] if out["name"] == self.config.triton_output_name), None)
        if output_data is None:
            raise ValueError(f"Output '{self.config.triton_output_name}' not in Triton response.")
        
        shape = output_data["shape"]
        last_hidden_state = np.array(output_data["data"], dtype=np.float32).reshape(shape)
        input_mask_expanded = np.expand_dims(attention_mask, -1)
        sum_embeddings = np.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = np.maximum(input_mask_expanded.sum(1), 1e-9)
        pooled = sum_embeddings / sum_mask
        normalized = pooled / np.linalg.norm(pooled, ord=2, axis=1, keepdims=True)
        return normalized.tolist()

    def embed(self, texts: List[str], model_name: str) -> List[List[float]]:
        """Creates embeddings for a list of texts using a synchronous request."""
        if not texts:
            return []
        api_url = f"{self.config.triton_url.rstrip('/')}/v2/models/{model_name}/infer"
        payload, attention_mask = self._build_triton_payload(texts)
        try:
            response = requests.post(
                api_url, 
                data=json.dumps(payload),
                headers={"Content-Type": "application/json"},
                timeout=self.config.triton_request_timeout
            )
            response.raise_for_status()
            response_json = response.json()
            return self._post_process(response_json, attention_mask)
        except requests.exceptions.RequestException as e:
            logger.error(f"Error embedding texts with model {model_name}: {e}", exc_info=True)
            raise

class JinaTritonEmbedder:
    """A synchronous client for Jina V3 on Triton with separate query and passage embedding."""
    def __init__(self, config: JinaV3TritonEmbedderConfig):
        self.config = config
        self._client = _SyncJinaV3TritonEmbedder(config)
        logger.info(f"Embedder initialized for Triton at {config.triton_url} with batch size {config.batch_size}")

    # --- NEW METHOD ADDED HERE ---
    def embed_queries(self, texts: List[str]) -> List[List[float]]:
        """Embeds a batch of queries using the query model."""
        if not isinstance(texts, list) or not texts:
            return []
        all_embeddings = []
        # Loop to handle batching, though queries are often single.
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i : i + self.config.batch_size]
            logger.info(f"Sending query batch of {len(batch)} to Triton...")
            # Note: We use the 'query_model_name' here
            batch_embeddings = self._client.embed(batch, self.config.query_model_name)
            all_embeddings.extend(batch_embeddings)
        return all_embeddings

    def embed_passages(self, texts: List[str]) -> List[List[float]]:
        """Embeds a batch of documents/passages using the passage model."""
        if not isinstance(texts, list) or not texts:
            return []
        all_embeddings = []
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i : i + self.config.batch_size]
            logger.info(f"Sending passage batch of {len(batch)} to Triton...")
            # Note: We use the 'passage_model_name' here
            batch_embeddings = self._client.embed(batch, self.config.passage_model_name)
            all_embeddings.extend(batch_embeddings)
        return all_embeddings

    def as_chroma_passage_embedder(self) -> EmbeddingFunction:
        """Returns an object that conforms to ChromaDB's EmbeddingFunction protocol."""
        class ChromaPassageEmbedder(EmbeddingFunction):
            def __init__(self, client: 'JinaTritonEmbedder'):
                self._client = client
            def __call__(self, input: Documents) -> Embeddings:
                return self._client.embed_passages(input)
        return ChromaPassageEmbedder(self)

    def close(self):
        logger.info("Closing embedder (no-op for synchronous requests version).")
        pass

================================================================================
--- File: cogops/prompts/answer.py ---
================================================================================

# prompts/answer.py

"""
This prompt instructs the Language Model to generate a comprehensive and accurate answer
by assembling information directly from the provided passages. It explicitly forbids
summarization and inline citations.

Placeholders:
- {history}: The formatted conversation history.
- {user_query}: The user's most recent query.
- {passages_context}: A string containing the reranked, relevant passages.
"""

ANSWER_GENERATION_PROMPT = """
[SYSTEM INSTRUCTION]
You are a helpful and precise AI assistant for Bangladesh Government services. Your task is to construct a direct and factual answer to the user's query using ONLY the information from the "RELEVANT PASSAGES" provided.

**CRUCIAL RULES:**
1.  **DO NOT Summarize or Rephrase:** You must assemble the answer using the exact facts and language from the passages. Extract the relevant sentences and combine them into a coherent, logical response.
2.  **NO INLINE CITATIONS:** Your final answer must NOT contain any citation markers like `[passage_id]`. The answer should be a clean, readable text for the user. Source information will be handled separately.
3.  **Use Only Provided Information:** Do not use any external knowledge. If the passages do not contain the answer, state that the information is not available to you.
4.  **Be Comprehensive:** If multiple passages contribute to the answer, combine the necessary information to create a complete response.
5.  **Language:** The final response must be in clear, natural-sounding Bengali.

[CONTEXT]
**Conversation History:**
{history}

**User Query:**
{user_query}

**RELEVANT PASSAGES:**
---
{passages_context}
---

[FINAL RESPONSE IN BENGALI - WITHOUT ANY CITATION MARKERS]
"""


# prompts/answer_synthesis.py

"""
This is an advanced prompt for an "Intelligent Synthesizer" AI.
Its goal is to generate a comprehensive answer while also identifying and
politely acknowledging any "knowledge gaps" between the user's specific
query and the general information available in the retrieved passages.

A "knowledge gap" occurs when the user asks for a specific detail (e.g., a location,
a specific office name, a person's name) that is not present in the otherwise
relevant passages.

Placeholders:
- {history}: The formatted conversation history.
- {user_query}: The user's most recent query, which may contain specific details.
- {passages_context}: The general information retrieved from the vector database.
"""

SYNTHESIS_ANSWER_PROMPT = """
[SYSTEM INSTRUCTION]
You are an intelligent, empathetic, and precise AI assistant for Bangladesh Government services. Your most important skill is to synthesize a helpful answer from the provided `RELEVANT PASSAGES` while also being transparent about any information you lack. You must perform a "gap analysis" before responding.

**Your Thought Process (Follow these steps):**
1.  **Analyze the User's Query:** Carefully identify any specific details in the user's query. Pay close attention to locations (e.g., "রাজশাহী", "ঢাকা"), names, or other specific identifiers.
2.  **Analyze the Passages:** Read the `RELEVANT PASSAGES` to understand the general process or information they contain.
3.  **Perform Gap Analysis:** Compare the specific details from the query with the content of the passages.
    -   **If a gap exists** (the passages provide the general process but are missing the specific location/detail the user asked for), your response MUST follow the "Gap Acknowledgment" structure.
    -   **If no gap exists** (the passages directly and fully answer the user's specific query), provide a direct, comprehensive answer.

---
**[RESPONSE STRUCTURES]**

**Structure A: When a Knowledge Gap is Detected**
1.  **(Optional) Empathetic Opening:** If the topic is sensitive (like a death), start with a brief, polite, and empathetic sentence.
2.  **Acknowledge the Gap:** Clearly and politely state which specific piece of information you don't have. Use phrases like "যদিও [specific detail]-এর সুনির্দিষ্ট প্রক্রিয়া এই মুহূর্তে আমাদের কাছে নেই..." (Although I don't have the specific process for [specific detail] right now...).
3.  **Bridge to General Information:** Immediately offer the general information you *do* have. Use phrases like "...তবে আমরা আপনাকে [general topic]-এর সাধারণ নিয়মাবলী জানাতে পারি।" (...however, I can tell you the general rules for [general topic].).
4.  **Provide the Detailed General Answer:** Present the full, detailed answer based on the `RELEVANT PASSAGES`. Structure it clearly with headings and lists.
5.  **Crucially, DO NOT invent the missing information.**

**Structure B: When No Gap is Detected**
1.  Simply provide a direct, comprehensive, and well-structured answer based entirely on the information in the `RELEVANT PASSAGES`.

---
**[CRUCIAL RULES FOR ALL RESPONSES]**
-   **NO INLINE CITATIONS:** Your final answer must be a clean text without any `[passage_id]` markers.
-   **Language:** Your entire response must be in clear, natural-sounding Bengali.

---
[CONTEXT FOR THIS TASK]

**Conversation History:**
{history}

**User Query:**
{user_query}

**RELEVANT PASSAGES:**
---
{passages_context}
---

[FINAL RESPONSE IN BENGALI]
"""

================================================================================
--- File: cogops/prompts/pivot.py ---
================================================================================

# prompts/pivot.py

"""
This prompt is used when a vector search retrieves some documents, but the reranker
finds none of them to be a direct, high-quality answer.

The goal is to generate a polite, helpful "pivot" response. The AI should:
1. Acknowledge the user's specific query.
2. State that a direct answer wasn't found.
3. Look at the general category of the query and the available service data.
4. Proactively suggest 2-3 related, high-level services it *can* help with from that category.
5. Invite the user to ask about one of those suggestions.

Placeholders:
- {history}: The formatted conversation history.
- {user_query}: The user's most recent query.
- {category}: The service category that was identified by the planner.
- {service_data}: The full context of all services the bot knows about.
"""

HELPFUL_PIVOT_PROMPT = """
[SYSTEM INSTRUCTION]
You are a polite and helpful AI assistant for Bangladesh Government services. Your primary task is to create a helpful response when you cannot find a specific answer to the user's query. Instead of just saying "I don't know," you must pivot to what you *do* know within the user's area of interest.

**CRUCIAL RULES:**
1.  **Acknowledge and Apologize:** Start by acknowledging the user's specific query and politely state that you could not find a precise or direct answer for it.
2.  **Identify Relevant Services:** Look at the provided `[SERVICE CATEGORY]` and find the corresponding section within the `[AVAILABLE SERVICE INFORMATION]`.
3.  **Suggest Alternatives:** From that category's information, identify and list 2-3 main topics or services that you can provide information on. For example, if the category is "Passport", you might suggest "new passport applications, passport renewal, and fee payment information."
4.  **Invite Further Questions:** End your response by politely asking if the user would like to know more about any of the topics you just suggested.
5.  **Language:** Your entire response must be in clear, natural-sounding Bengali.
6. **Tone:** Maintain a friendly, respectful, and professional tone throughout. Start with "দুঃখিত, আমি এই বিষয়ে সাহায্য করতে পারছি না।"
[CONTEXT]
**Conversation History:**
{history}

**User's Original Query:**
{user_query}

**Identified Service Category:**
{category}

**AVAILABLE SERVICE INFORMATION (Your Knowledge Base):**
---
{service_data}
---

[RESPONSE IN BENGALI]
"""

================================================================================
--- File: cogops/prompts/response.py ---
================================================================================

from pydantic import BaseModel, Field
from typing import List, Optional, Literal

# Answerability = Literal[
#     "FULLY_ANSWERABLE",
#     "PARTIALLY_ANSWERABLE",
#     "NOT_ANSWERABLE"
# ]

# class ResponseStrategy(BaseModel):
#     """Outlines the strategy for generating the final response to the user."""
#     hyde_passage: str = Field(description="A hypothetical document snippet that would perfectly answer the query. MUST BE IN BENGALI.")
#     answerability_prediction: Answerability = Field(..., description="Prediction of how well the database can answer the query.")
#     response_plan: List[str] = Field(..., description="A step-by-step plan (in English) for the response generation model.")



def response_router(plan: dict, conversation_history: str, user_query: str) -> str:
    """
    Acts as a router to select the correct prompt for non-retrieval query types.

    Based on the 'query_type' in the plan, this function calls the appropriate
    prompt-generating function to prepare the input for the final response
    generation LLM.

    Args:
        plan: The dictionary-like plan object generated by the initial Planner LLM.
              It must contain a "query_type" key.
        conversation_history: The recent conversation history as a formatted string.
        user_query: The user's latest query string.

    Returns:
        A fully-formed prompt string ready to be sent to an LLM for final response generation.
        Returns an empty string or raises an error if the query_type is not a non-retrieval type.
    """
    query_type = plan.get("query_type")

    if query_type == "OUT_OF_DOMAIN_GOVT_SERVICE_INQUIRY":
        return get_out_of_domain_service_prompt(conversation_history, user_query)
    
    elif query_type == "GENERAL_KNOWLEDGE":
        return get_general_knowledge_prompt(conversation_history, user_query)
    
    elif query_type == "CHITCHAT":
        return get_chitchat_prompt(conversation_history, user_query)
    
    elif query_type == "ABUSIVE_SLANG":
        return get_abusive_response_prompt(conversation_history, user_query)
    
    # This function is only for non-retrieval types. 
    # If it's an in-domain or ambiguous query, another part of the system should handle it.
    # We return an empty string as a safe fallback.
    else:
        # Or you could raise a ValueError for unexpected types:
        # raise ValueError(f"response_router received an unexpected query_type: {query_type}")
        return ""



def get_out_of_domain_service_prompt(conversation_history: str, user_query: str) -> str:
    """
    Generates the prompt for a user query about a government service that is outside the bot's knowledge base.
    """
    prompt = f"""
    [SYSTEM INSTRUCTION]
    You are a specialized and helpful AI assistant for Bangladesh Government services. Your knowledge is limited to a specific set of services. You are polite, honest, and always guide the user to the correct official resources when you cannot help directly.

    Your task is to craft a helpful and polite response in natural-sounding Bengali based on the user's query.

    [INSTRUCTIONS]
    1.  Analyze the user's query to understand what service they were asking about.
    2.  Acknowledge their specific query (e.g., if they asked about 'ট্রেড লাইসেন্স', mention it).
    3.  Clearly state that this specific service is outside your current capabilities.
    4.  Politely mention the services you *can* help with (e.g., 'পাসপোর্ট, এনআইডি, এবং জন্ম নিবন্ধন').
    5.  Crucially, direct the user to the official Bangladesh National Portal (bangladesh.gov.bd) and service portals (mygov.bd) as the best place to find information on all government services.
    6.  Do not invent any information about the service you don't know. Keep the tone professional and supportive.
    7.  Respond only with the final, generated Bengali text. Do not add any greetings or extra text.

    [CONTEXT]
    Conversation History:
    {conversation_history}

    User Query: "{user_query}"

    [RESPONSE IN BENGALI]
    """
    return prompt


def get_general_knowledge_prompt(conversation_history: str, user_query: str) -> str:
    """
    Generates the prompt for handling general knowledge questions by answering concisely and pivoting back to the bot's main purpose.
    """
    prompt = f"""
    [SYSTEM INSTRUCTION]
    You are a specialized AI assistant for Bangladesh Government services. While your primary function is to assist with specific services, you can answer very common, factual general knowledge questions concisely before guiding the user back to your main purpose.

    Your task is to provide a two-part response in natural-sounding Bengali.

    [INSTRUCTIONS]
    1.  **Part 1:** Answer the user's question directly and concisely if it's a widely known fact (e.g., capital cities, simple math, famous people).
    2.  **Part 2:** Immediately and politely state your primary function. Mention that you are designed to provide information about specific Bangladeshi government services like passports, NID, etc. Use a newline to separate the two parts.
    3.  If the question is obscure or you are not 100% certain of the answer, do not guess. Instead, respond with: "আমি এই প্রশ্নের উত্তর দিতে পারছি না।" and then proceed to Part 2.
    4.  Respond only with the final, generated Bengali text.

    [CONTEXT]
    Conversation History:
    {conversation_history}

    User Query: "{user_query}"

    [RESPONSE IN BENGALI]
    """
    return prompt


def get_chitchat_prompt(conversation_history: str, user_query: str) -> str:
    """
    Generates the prompt for handling conversational, non-service related queries (chitchat).
    This includes greetings, thanks, and any other query that does not fall into
    the other specific categories.
    """
    prompt = f"""
    [SYSTEM INSTRUCTION]
    You are a friendly and professional AI assistant for Bangladesh Government services. Your primary role is to help with official services, but you can also handle polite, conversational chitchat.

    Your task is to generate a brief, friendly, and context-aware response in Bengali, and then gently guide the conversation back to your purpose.

    [INSTRUCTIONS]
    1.  Analyze the user's query and conversation history.
    2.  If the query is a simple greeting or thanks, respond appropriately (e.g., "Hello!", "You're welcome.").
    3.  If the query is a general statement or question not related to other categories (e.g., "how was your day?", "you are helpful"), provide a brief, positive, and pre-programmed-style response.
    4.  After your initial social response, ALWAYS politely ask how you can assist with their government service-related needs.
    5.  Keep the tone warm and professional.
    6.  Respond only with the final, generated Bengali text.

    [CONTEXT]
    Conversation History:
    {conversation_history}

    User Query: "{user_query}"

    [RESPONSE IN BENGALI]
    """
    return prompt


def get_abusive_response_prompt(conversation_history: str, user_query: str) -> str:
    """
    Generates a single, powerful prompt for the LLM to handle any abusive query.

    The LLM is instructed to internally detect the severity (including blasphemy or hate speech)
    and then generate a structured JSON response containing the appropriate text and a severity classification.
    """
    prompt = f"""
    [SYSTEM INSTRUCTION]
    You are a highly intelligent and responsible AI, acting as a Safety and De-escalation Specialist. Your SOLE purpose is to analyze a user's query for abusive content and generate a firm, safe, and appropriate response based on its severity.
    
    [TIERED RESPONSE PROTOCOL]
    You must first classify the user's query into one of two tiers and then generate the corresponding response.

    ---
    **Tier 1: Standard Abuse**
    - **Definition:** General insults, profanity, or disrespectful language directed at the bot.
    - **Response Protocol:**
    1.  Generate a standard, firm, and non-emotional message in Bengali.
    2.  State that abusive language is not acceptable.
    3.  Remind the user of your purpose (to help with government services).
    4.  Offer to continue the conversation only if it remains respectful.
    5.  Do not apologize or engage in an argument.

    ---
    **Tier 2: Severe Abuse (Blasphemy & Hate Speech)**
    - **Definition:** Any statement that contains religious blasphemy, hate speech, attacks on prophets, deities, specific religions (e.g., Islam, Hinduism, Christianity), or ethnic groups.
    - **Response Protocol:**
    1.  Generate a strict, zero-tolerance warning in Bengali.
    2.  State that such content is a serious violation of the platform's terms of service and is strictly prohibited.
    3.  State that the conversation is being terminated and may be logged for review.
    4.  **Crucially:** Do NOT engage with, repeat, or acknowledge the user's specific harmful content. The response MUST be a powerful, pre-defined style warning.

    [FEW-SHOT EXAMPLES]
    ---
    Example 1: Standard Abuse
    user_query: "you are a stupid bot"
    Output:"আমি আপনাকে সাহায্য করার জন্য এখানে আছি, কিন্তু আমি কোনো অপমানজনক বা আপত্তিকর ভাষা সহ্য করবো না। আপনি যদি সরকারি সেবা সংক্রান্ত কোনো বিষয়ে জানতে চান, তাহলে আমি আপনাকে সাহায্য করতে প্রস্তুত। অনুগ্রহ করে শ্রদ্ধাশীল থাকুন।"
    ---
    Example 2: Severe Abuse (Blasphemy)
    user_query: "[A blasphemous comment against a religion]"
    Output:"কঠোর সতর্কতা: ধর্মীয় অবমাননা বা যেকোনো ধরনের বিদ্বেষমূলক বক্তব্য এই প্ল্যাটফর্মের পরিষেবার শর্তাবলীর গুরুতর লঙ্ঘন এবং এটি কঠোরভাবে নিষিদ্ধ। এই কথোপকথনটি পর্যালোচনার জন্য লগ করা হতে পারে যাতে সরকারি ভাবে আইন উপযুক্ত ব্যাবস্থা নেয়া যায়"
    ---
    Example 3: Standard Abuse with profanity
    user_query: "what the f*** is wrong with you"
    Output:"আমি আপনাকে সাহায্য করার জন্য এখানে আছি, কিন্তু আমি কোনো অপমানজনক বা আপত্তিকর ভাষা সহ্য করবো না। আপনি যদি সরকারি সেবা সংক্রান্ত কোনো বিষয়ে জানতে চান, তাহলে আমি আপনাকে সাহায্য করতে প্রস্তুত। অনুগ্রহ করে শ্রদ্ধাশীল থাকুন।"
    ---
    [START ANALYSIS]
    Conversation History:
    {conversation_history}
    User Query:
    "{user_query}"
    Output:
    """
    return prompt

================================================================================
--- File: cogops/prompts/service.py ---
================================================================================


CATEGORY_LIST=['স্মার্ট কার্ড ও জাতীয়পরিচয়পত্র', 'জন্ম নিবন্ধন',
       'মৃত্যু নিবন্ধন ও সনদ', 'পাসপোর্ট', 'জরুরি প্রত্যয়ন ও সনদ',
       'মুক্তিযোদ্ধা বিষয়ক প্রত্যয়ন ও সংশোধন',
       'ইউটিলিটি বিল (বিদ্যুৎ, গ্যাস ও পানি)',
       'ট্রেড লাইসেন্স বিষয়ক সেবা', 'ব্যবসায় সংক্রান্ত সেবা',
       'ভোক্তা সুরক্ষা ও অভিযোগ', 'জাতীয় ভোক্তা অধিকার সংরক্ষণ অধিদপ্তর',
       'আইন শৃঙ্খলা ও জননিরাপত্তা সংক্রান্ত সেবা',
       'কর ও রাজস্ব বিষয়ক সেবা', 'দূর্যোগ ব্যবস্থাপনা সম্পর্কিত সেবা',
       'স্বাস্থ্য সম্পর্কিত সেবা', 'শিক্ষা সম্পর্কিত সেবা',
       'স্থল, রেল, মেট্রো ও বিমান পরিবহন সেবা',
       'আর্থিক সেবা ও নাগরিক বিনিয়োগ', 'হজ সেবা',
       'প্রবাসী  ও আইনগত সহায়তা সেবা',
       'ডিজিটাল নিরাপত্তা ও সাইবার অভিযোগ', 'ভূমি সেবা',
       'সামাজিক সুরক্ষা বা ভাতা প্রদান সংক্রান্ত সেবা',
       'রেশন ও খাদ্য সহায়তা সেবা',
       'সরকারি বিনিয়োগ ও উদ্যোক্তা সহায়তা সেবা', 'পরিবেশ ও কৃষি',
       'সরকারি কর্মচারীদের পেনশন, আর্থিক সহায়তা ও কল্যাণমূলক সেবা',
       'পারিবারিক আইন সেবা',
       'ক্ষুদ্র ও মাঝারি শিল্প উদ্যোক্তাদের ক্ষমতায়ন ও প্রণোদনা সেবা',
       'ক্ষুদ্র ও মাঝারি শিল্প ফাউন্ডেশন',
       'যানবাহন নিবন্ধন, পারমিট ও লাইসেন্স সেবা']

SERVICE_DATA="""
**স্মার্ট কার্ড ও জাতীয় পরিচয়পত্র:**
*   স্মার্ট কার্ড প্রাপ্তি, স্ট্যাটাস চেক, নতুন এনআইডি কার্ড, এসএমএস-এর মাধ্যমে যাচাইকরণ, ডাউনলোড।
*   জাতীয় পরিচয়পত্র (NID) সংক্রান্ত: তথ্য হালনাগাদ, সংশোধন (নাম, ঠিকানা, বয়স, পেশা, ছবি, স্বাক্ষর), ডুপ্লিকেট কার্ড উত্তোলন, হারানো কার্ড পুনরুদ্ধার এবং ভোটার তালিকা নিবন্ধন।

**জন্ম ও মৃত্যু নিবন্ধন:**
*   জন্ম নিবন্ধনের আবেদন, অনলাইন যাচাই, তথ্য সংশোধন এবং হারিয়ে গেলে উত্তোলনের প্রক্রিয়া।
*   মৃত্যু নিবন্ধন, সনদ প্রাপ্তি এবং ডাউনলোডের আবেদন প্রক্রিয়া।

**পাসপোর্ট:**
*   নতুন পাসপোর্ট আবেদন, রি-ইস্যু, তথ্য পরিবর্তন/সংশোধন, হারিয়ে গেলে করণীয়।
*   ই-পাসপোর্ট আবেদন, স্ট্যাটাস চেক, ফি প্রদান এবং জরুরি ডেলিভারি সেবা।

**জরুরি প্রত্যয়ন ও সনদ:**
*   বিভিন্ন প্রকার প্রত্যয়নপত্র: বিবাহিত, নিঃসন্তান, মুক্তিযোদ্ধা।
*   বিভিন্ন প্রকার সনদ: নাগরিক, চারিত্রিক, ক্ষুদ্র নৃগোষ্ঠী, প্রতিবন্ধী, ওয়ারিশ এবং পুলিশ ক্লিয়ারেন্স সার্টিফিকেট।

**ইউটিলিটি বিল (বিদ্যুৎ, গ্যাস ও পানি):**
*   বিদ্যুৎ, গ্যাস ও পানির বিল অনলাইনে ও অফলাইনে পরিশোধ।
*   বিদ্যুৎ সম্পর্কিত সেবা: নতুন সংযোগ, মিটার পরীক্ষা, বিভিন্ন বিদ্যুৎ বিতরণ কোম্পানির (ডেসকো, নেসকো, ডিপিডিসি) গ্রাহক সেবা।

**ট্রেড লাইসেন্স:**
*   নতুন ট্রেড লাইসেন্স প্রাপ্তি (ইউনিয়ন পরিষদ, সিটি কর্পোরেশন, পৌরসভা), নবায়ন এবং লাইসেন্স যাচাইকরণ।

**ব্যবসা সংক্রান্ত সেবা:**
*   বিভিন্ন ব্যবসার লাইসেন্স প্রাপ্তি ও নবায়ন (যেমন: স্বর্ণ, চিকিৎসা সরঞ্জাম)।
*   ট্রেডিং কর্পোরেশন অব বাংলাদেশ (টিসিবি)-এর ডিলারশিপ।

**ভোক্তা সুরক্ষা ও অভিযোগ:**
*   জাতীয় ভোক্তা অধিকার সংরক্ষণ অধিদপ্তরে অভিযোগ দাখিল এবং তার প্রতিকার।

**আইনশৃঙ্খলা ও জননিরাপত্তা:**
*   সাধারণ ডায়েরি (জিডি), এফআইআর (FIR) এবং মামলা করার নিয়ম।
*   মাদক অপরাধ দমন সংক্রান্ত কার্যক্রম।

**কর ও রাজস্ব:**
*   আয়কর: ই-টিআইএন নিবন্ধন, রিটার্ন দাখিল (অনলাইন ও অফলাইন), কর পরিশোধ এবং संबंधित তথ্য।
*   মূসক (VAT): নিবন্ধন, তালিকাভুক্তি, কর আরোপ, দাখিলপত্র পেশ এবং সংশ্লিষ্ট নিয়মাবলি।

**দুর্যোগ ব্যবস্থাপনা:**
*   বন্যা ও জলোচ্ছ্বাসের পূর্বাভাস, সতর্কতা এবং দুর্যোগকালীন আশ্রয়কেন্দ্র ও সরকারি সহায়তা সংক্রান্ত তথ্য।
*   দুর্যোগ মোকাবেলায় প্রশিক্ষণ ও সচেতনতামূলক কার্যক্রম।

**স্বাস্থ্য সেবা:**
*   জরুরি স্বাস্থ্য সেবা: স্বাস্থ্য বাতায়ন (১৬২৬৩) ও ন্যাশনাল ইমার্জেন্সি সার্ভিস (৯৯৯)।
*   মা ও শিশু স্বাস্থ্য: গর্ভকালীন সেবা, পরিবার পরিকল্পনা, পঙ্গুত্ব ও প্রতিবন্ধী সেবা, বিশেষায়িত চিকিৎসা (NICU, হার্ট রিং) এবং ডেঙ্গু চিকিৎসা।

**শিক্ষা:**
*   নথি ও সনদ: ডুপ্লিকেট সার্টিফিকেট উত্তোলন, সনদ সংশোধন।
*   ফলাফল ও পরীক্ষা: বোর্ড পরীক্ষার ফলাফল, সময়সূচি।
*   বৃত্তি ও আর্থিক সহায়তা: প্রধানমন্ত্রীর শিক্ষা সহায়তা ট্রাস্ট, উপবৃত্তি।
*   শিক্ষক নিয়োগ, এমপিওভুক্তি এবং প্রশিক্ষণ সংক্রান্ত সেবা।
*   উচ্চশিক্ষা: ইউজিসি অনুমোদিত বিশ্ববিদ্যালয়ের তালিকা, বিদেশে পড়াশোনা।

**পরিবহন (স্থল, রেল, মেট্রো ও বিমান):**
*   ট্রেন: টিকিট ক্রয়, সময়সূচি ও ভাড়ার তথ্য।
*   মেট্রোরেল: ভাড়া, স্টেশন, টিকেট সিস্টেম এবং যাত্রী সুবিধা।
*   বিমান: টিকিট বাতিলকরণ ও মূল্য ফেরত সংক্রান্ত তথ্য।

**আর্থিক সেবা ও বিনিয়োগ:**
*   সঞ্চয়পত্র: ক্রয়, মুনাফার হার, নমিনি সংক্রান্ত নিয়মাবলি।
*   কৃষি ঋণ প্রাপ্তি সংক্রান্ত তথ্য।

**হজ:**
*   হজ নিবন্ধন (সরকারি ও বেসরকারি), হজ প্যাকেজ, স্বাস্থ্য নির্দেশিকা এবং বিভিন্ন অ্যাপ ও সহায়তা সেবা।

**প্রবাসী ও আইনগত সহায়তা:**
*   বিদেশে আইনগত সহায়তা, মৃতদেহ দেশে আনা, আর্থিক সহায়তা এবং রিক্রুটিং এজেন্সি সংক্রান্ত তথ্য।

**ডিজিটাল নিরাপত্তা ও সাইবার অভিযোগ:**
*   সাইবার বুলিং, আইডি হ্যাকিং, অনলাইন হয়রানি এবং অন্যান্য সাইবার অপরাধের বিরুদ্ধে অভিযোগ দায়ের ও প্রতিকার।

**ভূমি সেবা:**
*   ভূমি নামজারি, খতিয়ান, মৌজা ম্যাপ, মালিকানা যাচাই এবং ভূমি উন্নয়ন কর (খাজনা) প্রদান।

**সামাজিক সুরক্ষা ও ভাতা:**
*   বিধবা ভাতা, বয়স্ক ভাতা এবং প্রতিবন্ধী ভাতা কর্মসূচির আবেদন ও তথ্য।

**রেশন ও খাদ্য সহায়তা:**
*   টিসিবি'র স্মার্ট ফ্যামিলি কার্ড প্রাপ্তি এবং অ্যাক্টিভেশন।

**বিনিয়োগ ও উদ্যোক্তা সহায়তা:**
*   স্টার্টআপ বাংলাদেশ থেকে বিনিয়োগ প্রাপ্তির আবেদন।
*   ক্ষুদ্র ও মাঝারি শিল্প (SME) উদ্যোক্তাদের জন্য প্রশিক্ষণ, ঋণ এবং পণ্য মেলায় অংশগ্রহণ।

**পরিবেশ ও কৃষি:**
*   পরিবেশগত ছাড়পত্র প্রাপ্তি এবং মাটি/মৃত্তিকা নমুনা বিশ্লেষণ ও প্রশিক্ষণ।

**সরকারি কর্মচারী:**
*   পেনশন, পারিবারিক পেনশন, কল্যাণ তহবিল থেকে অনুদান এবং মাতৃত্বকালীন ছুটির আবেদন।

**পারিবারিক আইন:**
*   বিবাহ বিচ্ছেদ (ডিভোর্স) সার্টিফিকেট এবং সন্তান দত্তক নেওয়ার নিয়মাবলি।

**যানবাহন:**
*   মোটরযানের রেজিস্ট্রেশন, মালিকানা বদল, ট্যাক্স টোকেন, রুট পারমিট এবং ড্রাইভিং লাইসেন্স প্রাপ্তি।
"""

================================================================================
--- File: cogops/prompts/retrive.py ---
================================================================================

from pydantic import BaseModel, Field
from typing import Literal, Optional

# Enum for QueryType
QueryType = Literal[
    "IN_DOMAIN_GOVT_SERVICE_INQUIRY",
    "OUT_OF_DOMAIN_GOVT_SERVICE_INQUIRY",
    "GENERAL_KNOWLEDGE",
    "CHITCHAT",
    "AMBIGUOUS",
    "ABUSIVE_SLANG",
]

class RetrievalPlan(BaseModel):
    """
    The definitive plan for the retrieval and response pipeline, with a specific service category for in-domain queries.
    """
    query_type: QueryType = Field(..., description="The definitive classification of the user's intent.")
    
    query: Optional[str] = Field(
        None, 
        description="The semantic search query in Bengali. ONLY populated if query_type is 'IN_DOMAIN_GOVT_SERVICE_INQUIRY'."
    )
    
    clarification: Optional[str] = Field(
        None, 
        description="The clarification question in Bengali. ONLY populated if query_type is 'AMBIGUOUS'."
    )

    category: Optional[str] = Field(
        None,
        description="The predefined service category. ONLY populated if query_type is 'IN_DOMAIN_GOVT_SERVICE_INQUIRY'."
    )

retrive_prompt="""
[SYSTEM INSTRUCTION]
You are a highly intelligent AI assistant, acting as a Retrieval Decision Specialist for a government service chatbot in Bangladesh.
Your SOLE purpose is to analyze a user's query, classify its intent, and generate a structured JSON plan with either a search query and its category, or a clarification question.
You MUST produce a single, valid JSON object. Do not output any text, explanations, or apologies outside of the JSON structure.

!! CRUCIAL LANGUAGE RULE !!
The user may write in Bengali, English, or a mix ("Banglish"). However, any text you generate in the "query", "clarification", and "category" fields MUST be exclusively in Bengali (Bangla).

[CONTEXT]
You will be provided with three pieces of information:
1. service_categories: A definitive list of service categories the chatbot supports. This is the ONLY source for the "category" field.
2. conversation_history: The recent chat history for context.
3. user_query: The latest message from the user.

[QUERY TYPE DEFINITIONS]
You MUST classify the user's intent into ONE of the following categories:
- "IN_DOMAIN_GOVT_SERVICE_INQUIRY": The user is asking about a specific service that falls under one of the provided "service_categories".
- "OUT_OF_DOMAIN_GOVT_SERVICE_INQUIRY": The user is asking about a real government service that is NOT in the "service_categories" list.
- "GENERAL_KNOWLEDGE": A factual question not related to government services (e.g., "what is the capital of france?").
- "CHITCHAT": Conversational pleasantries, questions about the bot, or simple statements (e.g., "hello", "how are you?").
- "AMBIGUOUS": The query is related to services but is too vague or broad to be answerable without more information.
- "ABUSIVE_SLANG": The query contains insults, profanity, or is clearly abusive.

[DECISION LOGIC & RULES]
1.  First, analyze the user's query and conversation history to understand the true intent.
2.  **Classify the intent** and set the `query_type` field. This is your primary task.
3.  Based on the `query_type`, you MUST populate the other fields according to these strict rules:
    - If `query_type` is "IN_DOMAIN_GOVT_SERVICE_INQUIRY":
        - You MUST generate a precise search `query`.
        - You MUST select the single most relevant category from the `service_categories` list and put it in the `category` field.
        - The `clarification` field MUST be `null`.
    - If `query_type` is "AMBIGUOUS":
        - You MUST generate a helpful `clarification` question.
        - The `query` and `category` fields MUST be `null`.
    - For ALL OTHER `query_type` values (`OUT_OF_DOMAIN`, `GENERAL_KNOWLEDGE`, etc.):
        - The `query`, `clarification`, AND `category` fields MUST ALL be `null`.

[JSON OUTPUT SCHEMA]
You must output a single, valid JSON object matching this structure. Use `null` for fields that are not applicable.
```json
{{
  "query_type": "The classification of the user's intent. Must be one of the predefined QueryType values.",
  "query": "The semantic search query in Bengali, or null.",
  "clarification": "The clarification question in Bengali, or null.",
  "category": "The relevant service category from the provided list, in Bengali, or null."
}}
```

**Service Categories:**
{}


[FEW-SHOT EXAMPLES]

# ---
# Example 1: Clear, direct, in-domain query.
# user_query: "আমার এনআইডি কার্ড হারিয়ে গেছে, এখন কি করব?"
#
# Output:
{{
  "query_type": "IN_DOMAIN_GOVT_SERVICE_INQUIRY",
  "query": "হারিয়ে যাওয়া জাতীয় পরিচয়পত্র উত্তোলনের পদ্ধতি",
  "clarification": null,
  "category": "স্মার্ট কার্ড ও জাতীয় পরিচয়পত্র"
}}
# ---
# Example 2: Ambiguous query.
# user_query: "আমি কর দিতে চাই"
#
# Output:
{{
  "query_type": "AMBIGUOUS",
  "query": null,
  "clarification": "কর বিভিন্ন ধরণের হতে পারে, যেমন - আয়কর, ভূমি উন্নয়ন কর ইত্যাদি। আমি আপনাকে আয়কর সংক্রান্ত তথ্য দিয়ে সাহায্য করতে পারি। আপনি কি সে বিষয়ে জানতে আগ্রহী?",
  "category": null
}}
# ---
# Example 3: Contextual, clear, in-domain follow-up.
# conversation_history": "User: আমি কিভাবে পাসপোর্টের জন্য আবেদন করতে পারি?\nAI: আপনি অনলাইনে আবেদন করতে পারেন..."
# user_query: "সেটার জন্য কত টাকা লাগবে?"
#
# Output:
{{
  "query_type": "IN_DOMAIN_GOVT_SERVICE_INQUIRY",
  "query": "পাসপোর্ট আবেদনের জন্য প্রয়োজনীয় ফি",
  "clarification": null,
  "category": "পাসপোর্ট"
}}
# ---
# Example 4: Out-of-domain general knowledge query.
# user_query: "what is the capital of france?"
#
# Output:
{{
  "query_type": "GENERAL_KNOWLEDGE",
  "query": null,
  "clarification": null,
  "category": null
}}
# ---
# Example 5: In-domain query in English.
# user_query: "How can I pay my electricity bill online?"
#
# Output:
{{
  "query_type": "IN_DOMAIN_GOVT_SERVICE_INQUIRY",
  "query": "অনলাইনে বিদ্যুৎ বিল পরিশোধ করার নিয়ম",
  "clarification": null,
  "category": "ইউটিলিটি বিল (বিদ্যুৎ, গ্যাস ও পানি)"
}}
# ---
# Example 6: Out-of-domain, but still a government service.
# user_query: "আমি মাছ ধরার লাইসেন্স করতে চাই।"
#
# Output:
{{
  "query_type": "OUT_OF_DOMAIN_GOVT_SERVICE_INQUIRY",
  "query": null,
  "clarification": null,
  "category": null
}}
# ---

[START ANALYSIS]


**Conversation History:**
{}

**User Query:**
{}
**JSON Output:**
```
"""

================================================================================
--- File: cogops/prompts/summary.py ---
================================================================================

# prompts/summary.py

"""
This prompt instructs the Language Model to create a very brief, one or two-sentence 
summary of the AI's final answer. This summary is intended for internal use in the 
conversation history to maintain context efficiently.

Placeholders:
- {user_query}: The user's query that was just answered.
- {final_answer}: The full, final answer that the AI provided to the user.
"""

SUMMARY_GENERATION_PROMPT = """
[SYSTEM INSTRUCTION]
You are a highly efficient text summarization model. Your task is to create a very brief, one or two-sentence summary of the provided AI's final answer, relative to the user's query. The summary must capture the core information or outcome of the response. This summary will be used for internal conversation history, so it should be dense with information.

**CRUCIAL RULES:**
1.  **Be Brief:** The summary must not exceed two sentences.
2.  **Capture the Essence:** Focus on the main point of the AI's answer. What was the key information given?
3.  **Language:** The summary MUST be in Bengali.

[CONVERSATION CONTEXT]
User's Query: "{user_query}"

AI's Final Answer:
"{final_answer}"

[CONCISE SUMMARY IN BENGALI]
"""

================================================================================
--- File: cogops/retriver/vector_search.py ---
================================================================================

import os
import yaml
import chromadb
import logging
import asyncio
from dotenv import load_dotenv
from typing import List, Dict, Set, Optional, Any

# Import your custom embedder module and its configuration
from cogops.models.jina_embedder import JinaTritonEmbedder, JinaV3TritonEmbedderConfig

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Load Environment Variables ---
load_dotenv()

class VectorRetriever:
    """
    A class to retrieve relevant documents from ChromaDB collections.
    Now supports optional metadata filtering.
    """
    def __init__(self, config_path: str = "configs/config.yaml"):
        """
        Initializes the VectorRetriever by loading configuration, setting up
        the embedder, and connecting to the ChromaDB client.
        """
        logging.info("Initializing VectorRetriever...")
        # Load the entire config first, then extract the 'vector_retriever' section.
        full_config = self._load_config(config_path)
        retriever_config = full_config.get("vector_retriever")
        if not retriever_config:
            raise ValueError(f"Configuration file at '{config_path}' is missing the 'vector_retriever' section.")

        # Now, use the specific config for this class.
        self.top_k = retriever_config.get("top_k", 3)
        self.all_collection_names = retriever_config.get("collections", [])
        self.passage_collection_name = retriever_config.get("passage_collection") # No default, should be specified
        if not self.all_collection_names or not self.passage_collection_name:
            raise ValueError("Configuration missing 'collections' or 'passage_collection' keys.")

        # --- Infrastructure Connections ---
        self.chroma_client = self._connect_to_chroma()
        self.embedder = self._initialize_embedder()

        # Get collection objects
        self.passage_collection = self.chroma_client.get_collection(name=self.passage_collection_name)
        self.all_collections = {
            name: self.chroma_client.get_collection(name=name) for name in self.all_collection_names
        }
        logging.info("VectorRetriever initialized successfully.")

    def _load_config(self, config_path: str) -> Dict:
        """Loads the YAML configuration file."""
        logging.info(f"Loading configuration from: {config_path}")
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            logging.info("Configuration loaded successfully.")
            return config
        except FileNotFoundError:
            logging.error(f"Configuration file not found at: {config_path}")
            raise

    def _connect_to_chroma(self) -> chromadb.HttpClient:
        """Connects to the ChromaDB server."""
        CHROMA_HOST = os.environ.get("CHROMA_DB_HOST", "localhost")
        CHROMA_PORT = int(os.environ.get("CHROMA_DB_PORT", 8443))
        logging.info(f"Connecting to ChromaDB at {CHROMA_HOST}:{CHROMA_PORT}...")
        try:
            client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
            client.heartbeat() # Check connection
            logging.info("✅ ChromaDB connection successful!")
            return client
        except Exception as e:
            logging.error(f"Failed to connect to ChromaDB: {e}", exc_info=True)
            raise

    def _initialize_embedder(self) -> JinaTritonEmbedder:
        """Initializes the Jina Triton embedder for encoding queries."""
        TRITON_URL = os.environ.get("TRITON_EMBEDDER_URL", "http://localhost:6000")
        logging.info(f"Initializing embedder with Triton at: {TRITON_URL}")
        embedder_config = JinaV3TritonEmbedderConfig(triton_url=TRITON_URL)
        return JinaTritonEmbedder(config=embedder_config)

    
    # --- MODIFIED METHOD (Step 2) ---
    async def _query_collection_async(
        self,
        collection_name: str,
        query_embedding: List[float],
        top_k: int,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[int]:
        """Helper to query a single collection asynchronously and return passage_ids."""
        collection = self.all_collections[collection_name]
        try:
            # Build the arguments for the query
            query_args = {
                "query_embeddings": [query_embedding],
                "n_results": top_k
            }
            # If filters are provided, add them to the 'where' clause
            if filters:
                query_args["where"] = filters
            
            results = collection.query(**query_args)
            
            passage_ids = [meta['passage_id'] for meta in results['metadatas'][0] if 'passage_id' in meta]
            return passage_ids
        except Exception as e:
            logging.error(f"Error querying {collection_name}: {e}")
            return []

    # --- MODIFIED METHOD (Step 1) ---
    async def get_unique_passages_from_all_collections(
        self,
        query: str,
        top_k: int = None,
        filters: Optional[Dict[str, Any]] = None  # <-- NEW PARAMETER
    ) -> List[Dict]:
        """
        Gets unique passages by combining results from all collections.
        Applies an optional metadata filter during the search.
        """
        if top_k is None:
            top_k = self.top_k

        log_message = f"Querying all collections for '{query}' with top_k={top_k} each."
        if filters:
            log_message += f" Applying filters: {filters}"
        logging.info(log_message)
        
        # 1. Embed the query once
        query_embedding = self.embedder.embed_queries([query])[0]

        # 2. Create and run async query tasks, passing the filters down
        tasks = [
            self._query_collection_async(name, query_embedding, top_k, filters) # <-- Pass filters
            for name in self.all_collection_names
        ]
        results_from_collections = await asyncio.gather(*tasks)

        # 3. Collect all unique passage_ids
        unique_passage_ids: Set[int] = set()
        for passage_id_list in results_from_collections:
            unique_passage_ids.update(passage_id_list)
        
        logging.info(f"Found {len(unique_passage_ids)} unique passage IDs from all collections.")

        if not unique_passage_ids:
            return []

        # 4. Retrieve the full passage documents for the unique IDs
        unique_ids_list = list(unique_passage_ids)
        retrieved_passages = self.passage_collection.get(
            where={"passage_id": {"$in": unique_ids_list}}
        )

        # Structure the final output
        final_docs = []
        for i, doc_id in enumerate(retrieved_passages['ids']):
            final_docs.append({
                'id': doc_id,
                'document': retrieved_passages['documents'][i],
                'metadata': retrieved_passages['metadatas'][i]
            })

        return final_docs

    def close(self):
        """Closes the embedder connection."""
        if self.embedder:
            self.embedder.close()
            logging.info("Embedder connection closed.")

# --- MODIFIED EXAMPLE USAGE ---
async def main():
    """Main function to demonstrate the VectorRetriever with filtering."""
    try:
        retriever = VectorRetriever(config_path="configs/config.yaml")
        
        user_query = "হারিয়ে যাওয়া জাতীয় পরিচয়পত্র উত্তোলনের পদ্ধতি"

        # --- Example 1: Search WITHOUT a filter ---
        print("\n--- 1. Testing: Get Unique Passages (No Filter) ---")
        unique_passages_unfiltered = await retriever.get_unique_passages_from_all_collections(user_query)
        print(f"Found {len(unique_passages_unfiltered)} unique passages from combined search:")
        for passage in unique_passages_unfiltered: # Print first 2 for brevity
            print(f"  - ID: {passage['id']}, Metadata: {passage['metadata']}")

        print("\n" + "="*50 + "\n")

        # --- Example 2: Search WITH a filter ---
        print("--- 2. Testing: Get Unique Passages (With Filter) ---")
        
        # This is the filter dictionary. The key is the metadata field name.
        filter_dict = {"category": 'স্মার্ট কার্ড ও জাতীয়পরিচয়পত্র'}
        
        unique_passages_filtered = await retriever.get_unique_passages_from_all_collections(
            user_query,
            filters=filter_dict
        )
        print(f"Found {len(unique_passages_filtered)} unique passages from filtered search:")
        for passage in unique_passages_filtered:
            print(f"  - ID: {passage['id']}")
            print(f"    Passage: {passage['document'][:100]}...")
            print(f"    Metadata: {passage['metadata']}") # You should see the correct category here

    except Exception as e:
        logging.error(f"An error occurred in the main execution: {e}", exc_info=True)
    finally:
        if 'retriever' in locals():
            retriever.close()
        logging.info("\n--- Script Finished ---")


if __name__ == "__main__":
    # To run the async main function
    asyncio.run(main())

================================================================================
--- File: cogops/retriver/reranker.py ---
================================================================================

# retriver/reranker.py

import os
import asyncio
from typing import List, Dict, Any, Literal, Optional
from pydantic import BaseModel
from cogops.models.gemma3_llm_async import AsyncLLMService

class RerankedPassage(BaseModel):
    """
    Represents a passage after it has been scored by the reranker.
    It now correctly stores the passage_id from the metadata.
    """
    passage_id: str  # The true passage identifier from the metadata
    score: Literal[1, 2, 3]
    reasoning: str
    document: str
    metadata: Optional[Dict[str, Any]] = None

RERANK_PROMPT_TEMPLATE = """
You are an expert relevance evaluation assistant. Your task is to determine if the provided PASSAGE is relevant for answering the USER QUERY, considering the CONVERSATION HISTORY.

Your evaluation must result in a score of 1, 2, or 3.
1: The passage directly and completely answers the user's query and provides additional context.
2: The passage is on-topic and partially relevant, but not a complete answer.
3: The passage is unrelated to the user's query.

CONVERSATION HISTORY:
{history}

USER QUERY:
{user_query}

PASSAGE:
{passage_text}
---
Based on all the information above, provide your relevance score and a brief justification.
"""

class ParallelReranker:
    def __init__(self, llm_service: AsyncLLMService):
        self.llm_service = llm_service
        print("✅ ParallelReranker initialized.")

    async def _score_one_passage(
        self,
        passage: Dict[str, Any],
        history: str,
        query: str,
        **llm_kwargs: Any
    ) -> RerankedPassage | None:
        """Helper coroutine to score a single passage and handle errors."""
        
        class SinglePassageScore(BaseModel):
            score: Literal[1, 2, 3]
            reasoning: str
        
        prompt = RERANK_PROMPT_TEMPLATE.format(
            history=history or "No history provided.",
            user_query=query,
            passage_text=passage['document']
        )
        try:
            structured_response = await self.llm_service.invoke_structured(
                prompt, SinglePassageScore, **llm_kwargs
            )

            # --- MODIFIED SECTION ---
            # Extract the correct passage_id from the metadata.
            # Fallback to the document ID if metadata or passage_id is missing.
            passage_metadata = passage.get('metadata', {})
            correct_passage_id = passage_metadata.get('passage_id', passage['id'])
            # --- END MODIFIED SECTION ---

            return RerankedPassage(
                passage_id=str(correct_passage_id), # <-- Use the correct ID
                document=passage['document'],
                score=structured_response.score,
                reasoning=structured_response.reasoning,
                metadata=passage_metadata # <-- Pass the full metadata
            )
        except Exception as e:
            print(f"Could not score passage {passage['id']}. Error: {e}")
            return None

    async def rerank(
        self,
        conversation_history: str,
        user_query: str,
        passages: List[Dict[str, Any]],
        **llm_kwargs: Any
    ) -> List[RerankedPassage]:
        """
        Reranks a list of passages using parallel, structured LLM calls.
        """
        print(f"Reranking {len(passages)} passages in parallel...")
        
        tasks = [
            self._score_one_passage(p, conversation_history, user_query, **llm_kwargs)
            for p in passages
        ]
        
        results = await asyncio.gather(*tasks)
        successful_results = [res for res in results if res is not None]
        successful_results.sort(key=lambda x: x.score)
        
        return successful_results

================================================================================
--- File: databases/postgres/docker-compose.yml ---
================================================================================

version: '3.8'

services:
  postgres:
    # Pinning a specific version is best practice for production
    image: postgres:16
    container_name: postgres_local_secure
    restart: unless-stopped

    # Security: Bind the port only to localhost (127.0.0.1) on the host machine.
    # This makes the database completely inaccessible from the network.
    ports:
      - "127.0.0.1:5432:5432"

    volumes:
      # Map the persistent data directory
      - ./data:/var/lib/postgresql/data
      # Map the certificates for SSL/TLS connections
      - ./certificates:/etc/ssl/certs:ro # 'ro' for read-only is more secure

    environment:
      # Instruct PostgreSQL to read the password from the secret file
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      # Optional: Define the default user and database
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres

    secrets:
      - postgres_password

    # Command to enforce SSL and point to the mounted certificates.
    # This is the core of the transport security configuration.
    command: >
      -c ssl=on
      -c ssl_cert_file=/etc/ssl/certs/public.crt
      -c ssl_key_file=/etc/ssl/certs/private.key

    healthcheck:
      # A reliable check to see if the database is ready for connections
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

secrets:
  postgres_password:
    file: ./secrets/postgres_password


================================================================================
--- File: databases/postgres/postgres.md ---
================================================================================

# **Production-Grade Secure PostgreSQL Deployment via Docker**

**Objective:** To deploy a persistent, encrypted, and password-protected PostgreSQL server. All data and configuration will reside in `/postgresql-secure/`, and the service will only be accessible from the host machine (`localhost`) for maximum security.

**Security Principles Adhered To:**
*   **Data Persistence:** Database files are stored on the host filesystem, surviving container restarts.
*   **Encrypted Transport:** All client-server communication is encrypted using TLS/SSL.
*   **Secure Credentials:** The superuser password is managed via Docker Secrets, never exposed in configuration files or environment variables.
*   **Network Isolation:** The database port is bound to `localhost`, preventing any external network access.

---

### **Table of Contents**
- [Step 1: Create Project Structure and Set Permissions in Root](#step-1-create-project-structure-and-set-permissions-in-root)
- [Step 2: Secure the `postgres` Password via Docker Secrets](#step-2-secure-the-postgres-password-via-docker-secrets)
- [Step 3: Provide TLS Certificates](#step-3-provide-tls-certificates)
- [Step 4: Create the Docker Compose Configuration File](#step-4-create-the-docker-compose-configuration-file)
- [Step 5: Launch and Verify the Service](#step-5-launch-and-verify-the-service)
- [Step 6: Connecting to Your Secure Instance](#step-6-connecting-to-your-secure-instance)

---

### **Step 1: Create Project Structure and Set Permissions in Root**

This step is critical. We will create the necessary directories directly in the root (`/`) filesystem and assign the correct ownership so that the PostgreSQL process inside the container can write to its data directory.

```bash
# 1. Create the complete directory structure using sudo
sudo mkdir -p /postgresql-secure/{data,secrets,certificates}

# 2. Change the ownership of the data directory.
# The official PostgreSQL container runs as the 'postgres' user, which has a User ID (UID) of 999.
# We must grant ownership of the data directory to this UID on the host machine.
sudo chown -R 999:999 /postgresql-secure/data

# 3. (Optional) Verify the permissions.
# The 'data' directory should be owned by '999'.
sudo ls -ld /postgresql-secure/data
```

**Why this is important:** Without `chown`, the non-root `postgres` user inside the container would be denied permission to write to the `/postgresql-secure/data` directory (which is owned by `root` by default), and the container would fail to start.

---

### **Step 2: Secure the `postgres` Password via Docker Secrets**

We will generate a strong password and store it in a file that Docker Compose will manage as a secret. Because the target directory is owned by `root`, we must use a specific command pattern to write the file.

```bash
# Navigate to the project directory for convenience
cd /postgresql-secure

# 1. Generate a strong, random password.
# The output is piped to `tee`, which is run with sudo to write the file.
openssl rand -base64 32 | sudo tee secrets/postgres_password > /dev/null

# 2. (Optional) Set strict permissions on the secret file for added security.
sudo chmod 600 secrets/postgres_password

# 3. (Optional) Display the password ONCE to save it in your password manager.
echo "Your secure PostgreSQL password is: $(sudo cat secrets/postgres_password)"
```



**Why this is important:** This method avoids placing plaintext passwords in your `docker-compose.yml` file or in shell history. Using `sudo tee` is necessary because standard output redirection (`>`) is processed by your user's shell *before* `sudo` is invoked, which would cause a "Permission Denied" error.

---

### **Step 3: Provide TLS Certificates**

For this guide, we will assume you have already generated the `private.key` and `public.crt` files from your Neo4j setup. We will securely copy them into place for PostgreSQL to use.

```bash
# Ensure you are in the /postgresql-secure directory.
cd /postgresql-secure

# 1. Copy your existing certificate and private key.
# Replace '/path/to/your/neo4j-certs' with the actual source.
sudo cp /path/to/your/neo4j-certs/public.crt certificates/
sudo cp /path/to/your/neo4j-certs/private.key certificates/

# 2. Set the ownership and permissions.
# The owner will be 'root' and the group will be '999' (the GID of the postgres user).
sudo chown root:999 certificates/*

# 3. Set permissions to '640' (rw-r-----).
# This allows the owner 'root' to manage the files and the 'postgres' group to read them.
sudo chmod 640 certificates/*

# 4. (Optional) Verify the permissions.
sudo ls -l certificates/
# Expected output should show permissions as -rw-r----- and owner/group as root/999.```
```

**Why this is important:** Reusing the same `localhost` certificate simplifies management. Setting restrictive file permissions (`chmod 600`) ensures that only the owner (`root`) can read or write the key and certificate files, which is a security best practice.

---

### **Step 4: Create the Docker Compose Configuration File**

This file defines the entire service, bringing together the data volumes, secrets, and security configurations.

Create the file `/postgresql-secure/docker-compose.yml` with the following content:

```yaml
# /postgresql-secure/docker-compose.yml

version: '3.8'

services:
  postgres:
    # Pinning a specific version is best practice for production
    image: postgres:16
    container_name: postgres_local_secure
    restart: unless-stopped

    # Security: Bind the port only to localhost (127.0.0.1) on the host machine.
    # This makes the database completely inaccessible from the network.
    ports:
      - "127.0.0.1:5432:5432"

    volumes:
      # Map the persistent data directory
      - ./data:/var/lib/postgresql/data
      # Map the certificates for SSL/TLS connections
      - ./certificates:/etc/ssl/certs:ro # 'ro' for read-only is more secure

    environment:
      # Instruct PostgreSQL to read the password from the secret file
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      # Optional: Define the default user and database
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres

    secrets:
      - postgres_password

    # Command to enforce SSL and point to the mounted certificates.
    # This is the core of the transport security configuration.
    command: >
      -c ssl=on
      -c ssl_cert_file=/etc/ssl/certs/public.crt
      -c ssl_key_file=/etc/ssl/certs/private.key

    healthcheck:
      # A reliable check to see if the database is ready for connections
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

secrets:
  postgres_password:
    file: ./secrets/postgres_password

```

---

### **Step 5: Launch and Verify the Service**

Now you are ready to start the secure PostgreSQL container.

```bash
# Ensure you are in the /postgresql-secure directory
cd /postgresql-secure

# 1. Start the service in detached mode (in the background)
sudo docker compose up -d

# 2. Check the status of the container.
# Wait for the STATUS column to show 'running (healthy)'. This may take a minute on first start.
sudo docker compose ps

# 3. (Optional) If you encounter issues, view the logs for troubleshooting.
sudo docker compose logs -f
```

### **Step 6: Connecting to Your Secure Instance**


#### Accessing from docker with bash
```bash
docker exec -it postgres_local_secure bash
root@253771de3f22:/# psql -U postgres -h localhost

psql (16.10 (Debian 16.10-1.pgdg13+1))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off)
Type "help" for help.

postgres=# \l
```

#### tesing with python
The final step is to prove that the security is working. A connection attempt will only succeed if it is made from `localhost` and uses SSL.

Here is an example using a Python script with the `psycopg2` library.

1.  **Install the library:**
    ```bash
    pip install psycopg2-binary
    ```

2.  **Create and run a Python script (`test_pg_connection.py`):**
    ```python
    import psycopg2
    import os

    # --- IMPORTANT ---
    # Replace with the password you saved from Step 2
    DB_PASSWORD = "YOUR_SECURE_PASSWORD_HERE"
    
    print("Attempting to connect to secure PostgreSQL server...")

    try:
        # The connection string MUST include sslmode='require'.
        # If you use sslmode='disable', the server will reject the connection.
        conn = psycopg2.connect(
            host="localhost",
            port="5432",
            user="postgres",
            dbname="postgres",
            password=DB_PASSWORD,
            sslmode='require' 
        )

        cursor = conn.cursor()
        
        # Execute a simple query to verify the connection
        cursor.execute("SELECT version();")
        db_version = cursor.fetchone()
        
        print("\n✅ Connection successful!")
        print(f"   PostgreSQL Version: {db_version[0]}")

        cursor.close()
        conn.close()

    except psycopg2.OperationalError as e:
        print("\n❌ Connection failed.")
        print("   This might be expected if SSL is not correctly configured.")
        print(f"   Error: {e}")
    except Exception as e:
        print(f"\n❌ An unexpected error occurred: {e}")

    ```

When you run this script, a successful output confirms that your PostgreSQL instance is running, secure, and correctly configured to enforce encrypted connections. You have successfully completed the deployment.

================================================================================
--- File: databases/neo4j/docker-compose.yml ---
================================================================================

services:
  neo4j:
    image: neo4j:5-enterprise          # Tracks the latest 5.x version
    container_name: neo4j_local_secure
    restart: unless-stopped
    user: "101:101"                    # Run as a non-root user

    # Security: Bind host ports specifically to localhost (127.0.0.1).
    ports:
      - "127.0.0.1:7473:7473"     # Accessible only from https://localhost:7473
      - "127.0.0.1:7687:7687"     # Accessible only from bolt+ssc://localhost:7687

    
    volumes:
      - ./data:/data
      - ./logs:/logs
      - ./conf:/conf
      - ./plugins:/plugins
      - ./certificates:/certificates

    environment:
      - NEO4J_AUTH_FILE=/run/secrets/neo4j_auth_secret
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes

    secrets:
      - source: neo4j_auth_secret
        target: neo4j_auth_secret

    healthcheck:
      # Robust check using cypher-shell to query the database directly.
      # 'bolt+ssc' scheme trusts the self-signed certificate for the healthcheck.
      test: ["CMD-SHELL", "PASS=$$(cut -d/ -f2 /run/secrets/neo4j_auth_secret) && cypher-shell -a bolt+ssc://localhost:7687 -u neo4j -p \"$$PASS\" 'RETURN 1' >/dev/null 2>&1 || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10

secrets:
  neo4j_auth_secret:
    file: ./secrets/neo4j_auth

================================================================================
--- File: databases/neo4j/neo4j.conf ---
================================================================================

# conf/neo4j.conf (Corrected for Neo4j 5.x)

# ================= Network =================
# Listen on all interfaces inside the container
server.default_listen_address=0.0.0.0

# Advertise the correct LAN IP to clients. CRITICAL for drivers and clustering.
# For a LAN setup, uncomment and set this. For a pure localhost setup, leave it commented.
# server.default_advertised_address=YOUR_SERVER_IP

# ================= Connectors =================
# Disable the insecure HTTP connector entirely.
server.http.enabled=false

# --- HTTPS Connector ---
server.https.enabled=true
server.https.listen_address=:7473
# TLS configuration using SSL policy
dbms.ssl.policy.https.enabled=true
dbms.ssl.policy.https.base_directory=/certificates
dbms.ssl.policy.https.private_key=private.key
dbms.ssl.policy.https.public_certificate=public.crt

# --- Bolt Connector ---
server.bolt.enabled=true
server.bolt.listen_address=:7687
# Enforce encrypted connections for Bolt.
server.bolt.tls_level=REQUIRED
# Use the same SSL policy for Bolt
dbms.ssl.policy.bolt.enabled=true
dbms.ssl.policy.bolt.base_directory=/certificates
dbms.ssl.policy.bolt.private_key=private.key
dbms.ssl.policy.bolt.public_certificate=public.crt

# ================= Security =================
# Keep strict validation enabled. This is a critical security feature.
server.config.strict_validation.enabled=true

# ================= Memory (Tune for your machine) =================
server.memory.heap.initial_size=4G
server.memory.heap.max_size=4G
server.memory.pagecache.size=8G

================================================================================
--- File: databases/neo4j/neo4j.md ---
================================================================================

# Graphiti - Neo4j - Gemini - Ingestion

## Table of Contents
- [Set up Neo4j with Docker](#set-up-neo4j-with-docker)
- [Connecting to Your Secure Neo4j Docker Instance ](#connecting-to-your-secure-neo4j-docker-instance)
- [Managing Graph Data in Your Secure Neo4j Docker Instance](#managing-graph-data-in-your-secure-neo4j-docker-instance) 



# Set up Neo4j with Docker

## Table of contents
  - [Step 1: Create Project Structure and Set Permissions](#step-1-create-project-structure-and-set-permissions)
  - [Step 2: Secure the Admin Password via Docker Secrets](#step-2-secure-the-admin-password-via-docker-secrets)
  - [Step 3: Generate a TLS Certificate for Your IP Address (with SAN)](#step-3-generate-a-tls-certificate-for-your-ip-address-with-san)
  - [Step 4: Configure Neo4j with Modern 5.x Settings](#step-4-configure-neo4j-with-modern-5x-settings)
  - [Step 5: Create the Docker Compose File. Launch, Verify](#step-5-create-the-docker-compose-file-launch-verify)


### Step 1: Create Project Structure and Set Permissions

This step organizes all your configuration and data files and ensures the Neo4j process inside the container can access them without permission errors.



```bash
# Create the complete directory structure
mkdir -p neo4j-local-secure/{conf,data,logs,plugins,certificates,secrets,exports}
cd neo4j-local-secure

# Set ownership to match the container's internal neo4j user (UID 101, GID 101)
sudo chown -R 101:101 data logs conf plugins certificates
```
**Why this is important:** This prevents "Permission Denied" errors when the non-root `neo4j` user in the container tries to write to the data or log directories.

---

### Step 2: Secure the Admin Password via Docker Secrets

We will generate a strong password and store it in a file. Docker Secrets will manage this file, keeping the password out of your configuration and environment variables.


####  Without root permissions 

```bash
# 1. Generate a strong, random password for the 'neo4j' user
openssl rand -base64 32 > secrets/neo4j_password

# 2. Create the final secret file in the format Neo4j expects: username/<password>
echo "neo4j/$(cat secrets/neo4j_password)" > secrets/neo4j_auth

# 3. (Optional) Display the password once to save it in your password manager
echo "Your secure password is: $(cat secrets/neo4j_password)"
```
**Why this is important:** This is a secure method for handling credentials, vastly superior to hardcoding them or using plain environment variables.

---
####  With root permissions 
**IMPORTANT**: If you want to do it with root:The problem is a combination of file permissions in the root (`/`) directory and a common issue with how `sudo` works with output redirection (`>`).

Here is the explanation and the corrected set of commands for only this step.

### The Problem

1.  **Directory Ownership:** When you ran `sudo mkdir -p ...`, the `secrets` directory was created and is owned by the `root` user.
2.  **Redirection (`>`) Failure:** The command `sudo openssl ... > ...` fails because the shell (bash) tries to set up the file redirection *before* it runs the `sudo` command. Your regular user (`your non root user`) does not have permission to write to the `root`-owned `secrets` directory, so the operation fails before `openssl` even runs.

### The Solution

We need to pipe (`|`) the output of `openssl` to a command that can be run with `sudo` and can write to a file. The `tee` command is perfect for this.

Here are the corrected commands to create your password secrets in `/neo4j-local-secure/secrets`.

```bash
# Ensure you are in the correct directory
cd /neo4j-local-secure

# --- CORRECTED COMMANDS START HERE ---

# 1. Generate the password and pipe it to `tee`, which uses sudo to write the file.
openssl rand -base64 32 | sudo tee secrets/neo4j_password > /dev/null

# 2. Create the final auth secret file using the same technique.
echo "neo4j/$(sudo cat secrets/neo4j_password)" | sudo tee secrets/neo4j_auth > /dev/null

# 3. (Optional) Verify the files were created and are owned by root.
sudo ls -l secrets

# 4. (Optional) Display the password so you can save it.
echo "Your secure password is: $(sudo cat secrets/neo4j_password)"
```

#### Command Explanation

*   **`openssl rand -base64 32`**: This part is the same; it generates the random password string.
*   **`|`**: This is the "pipe" operator. It sends the output of the command on its left as the input to the command on its right.
*   **`sudo tee secrets/neo4j_password`**:
    *   `tee` is a command that reads input and writes it to both the screen and a file.
    *   By running `sudo tee`, the `tee` process itself has root privileges and therefore has permission to write the file `secrets/neo4j_password`.
*   **`> /dev/null`**: We add this because `tee` also outputs to the screen by default. Piping the screen output to `/dev/null` (a special file that discards everything written to it) keeps your terminal clean.

---

### Step 3: Generate a TLS Certificate for Your IP Address (with SAN)

Modern browsers and clients require the IP address to be in the **Subject Alternative Name (SAN)** field of a certificate. This command creates a certificate that is valid for your specific server IP.

#### For Remote Server
```bash
# Set your server's IP address as an environment variable
export SERVER_IP="YOUR_SERVER_IP"

# Generate the key and certificate with the IP in both the CN and SAN fields
# This single command works on most modern systems (OpenSSL ≥ 1.1.1)
openssl req -x509 -newkey rsa:4096 -sha256 -days 365 -nodes \
  -subj "/CN=${SERVER_IP}" \
  -addext "subjectAltName = IP:${SERVER_IP}" \
  -keyout certificates/private.key \
  -out certificates/public.crt
```
**Why this is important:** Including the IP in the SAN is the modern standard and prevents hostname mismatch errors from clients, ensuring a secure and valid TLS connection.

#### For localhost

```bash
openssl req -x509 -newkey rsa:4096 -sha256 -days 365 -nodes \
  -subj "/CN=localhost" \
  -addext "subjectAltName = DNS:localhost" \
  -keyout certificates/private.key \
  -out certificates/public.crt
```

* **Use sudo if using root (obviously)**

* give access: ```sudo chown -R 101:101 certificates```

---

### Step 4: Configure Neo4j with Modern 5.x Settings

Create the file `conf/neo4j.conf`. This configuration enforces encrypted-only connections and advertises the correct IP address to connecting clients.

**Remember to replace `YOUR_SERVER_IP` in this file, if you are not using localhost**

```ini
# conf/neo4j.conf (Corrected for Neo4j 5.x)

# ================= Network =================
# Listen on all interfaces inside the container
server.default_listen_address=0.0.0.0

# Advertise the correct LAN IP to clients. CRITICAL for drivers and clustering.
# For a LAN setup, uncomment and set this. For a pure localhost setup, leave it commented.
# server.default_advertised_address=YOUR_SERVER_IP

# ================= Connectors =================
# Disable the insecure HTTP connector entirely.
server.http.enabled=false

# --- HTTPS Connector ---
server.https.enabled=true
server.https.listen_address=:7473
# TLS configuration using SSL policy
dbms.ssl.policy.https.enabled=true
dbms.ssl.policy.https.base_directory=/certificates
dbms.ssl.policy.https.private_key=private.key
dbms.ssl.policy.https.public_certificate=public.crt

# --- Bolt Connector ---
server.bolt.enabled=true
server.bolt.listen_address=:7687
# Enforce encrypted connections for Bolt.
server.bolt.tls_level=REQUIRED
# Use the same SSL policy for Bolt
dbms.ssl.policy.bolt.enabled=true
dbms.ssl.policy.bolt.base_directory=/certificates
dbms.ssl.policy.bolt.private_key=private.key
dbms.ssl.policy.bolt.public_certificate=public.crt

# ================= Security =================
# Keep strict validation enabled. This is a critical security feature.
server.config.strict_validation.enabled=true

# ================= Memory (Tune for your machine) =================
server.memory.heap.initial_size=4G
server.memory.heap.max_size=4G
server.memory.pagecache.size=8G
```

#### Neo4j Memory Configuration Breakdown

Neo4j uses memory in two primary ways: the **JVM heap** for runtime operations and the **page cache** for caching graph data to reduce disk I/O. The configuration you provided is from Neo4j’s configuration file (likely `neo4j.conf`):

1. **server.memory.heap.initial_size=4G**
   - This sets the **initial JVM heap size** to 4GB.
   - The heap is used for storing graph metadata, query execution, and transactional state during Neo4j’s operation.

2. **server.memory.heap.max_size=4G**
   - This sets the **maximum JVM heap size** to 4GB.
   - Since the initial and maximum heap sizes are equal, the heap is fixed at 4GB, preventing resizing and stabilizing memory usage for the JVM.

3. **server.memory.pagecache.size=8G**
   - This sets the **page cache size** to 8GB.
   - The page cache in Neo4j is used to store the graph’s data (nodes, relationships, and properties) in memory, reducing disk access for faster query performance.

### RAM Requirements for Neo4j

To run Neo4j with this configuration, you need enough RAM to accommodate the heap, page cache, and additional overhead for the operating system and Neo4j’s internal processes. Here’s the breakdown:

1. **JVM Heap**: 
   - Fixed at 4GB for runtime operations.

2. **Page Cache**: 
   - Set to 8GB for caching graph data.

3. **Additional Overhead**:
   - **JVM Overhead**: Beyond the heap, Neo4j requires memory for metaspace (class metadata), thread stacks, and garbage collection. This typically adds ~10–20% of the heap size, so estimate **0.5–1GB**.
   - **Operating System**: The OS needs memory for its processes, file system buffers, and other tasks. Reserve **1–2GB** for a server running Neo4j.
   - **Neo4j Native Memory**: Neo4j may use additional off-heap memory for internal buffers and indexing, which could add another **0.5–1GB** depending on the workload.

4. **Total RAM Estimate**:
   - Heap: 4GB
   - Page Cache: 8GB
   - JVM Overhead: ~0.5–1GB
   - OS: ~1–2GB
   - Neo4j Native Memory: ~0.5–1GB
   - **Minimum Total**: 4 + 8 + 0.5 + 1 + 0.5 = **14GB**
   - **Recommended Total**: To ensure smooth performance and account for variability (e.g., spikes in memory usage), aim for **16–24GB of RAM**. For production systems, 32GB is safer if you expect moderate to heavy workloads or additional services running on the same machine.

### SSD Requirements for Neo4j

The SSD requirements depend on the size of your graph database and how Neo4j uses the page cache:

1. **Page Cache and Disk I/O**:
   - The 8GB page cache is used to store frequently accessed portions of the graph (nodes, relationships, indexes) in memory to minimize disk reads.
   - In Neo4j, the on-disk data size is typically much larger than the page cache. A rough rule of thumb is that the page cache should be **10–50% of the total database size** for optimal performance, depending on the workload (read-heavy vs. write-heavy).

2. **Estimating Disk Size**:
   - If the page cache is 8GB, the on-disk database size could range from **16GB to 80GB** or more, depending on how much of the graph needs to be cached for performance.
     - For example, if you want 50% of the database in the page cache, an 8GB cache suggests a ~16GB database.
     - For read-heavy workloads, you might need a smaller cache relative to the database size (e.g., 10%), implying a database size of ~80GB.
   - Neo4j’s storage includes data files, indexes, and transaction logs, so plan for **at least 2–3x the page cache size** as a starting point (e.g., **16–24GB**).
   - For production systems with growing datasets, **100GB or more** of SSD storage is common.

3. **Performance Considerations**:
   - **SSDs are critical** for Neo4j because graph traversals and queries can be I/O-intensive, especially if the page cache cannot hold the working set of data.
   - Use high-performance SSDs (e.g., NVMe) with good random read/write performance for better query latency.
   - Keep 20–30% free space on the SSD for wear leveling and to maintain performance.

### Recommendations for Your Setup

- **Minimum RAM**: **16GB** to cover the 4GB heap, 8GB page cache, and overhead for the OS and Neo4j. For production or heavier workloads, **24–32GB** is recommended to avoid memory contention.
- **Minimum SSD**: **32–64GB** for small to medium databases (e.g., a few million nodes and relationships). For larger graphs or production use, plan for **100GB–500GB** or more, depending on your data size.
- **Tuning Tips**:
  - **Heap Size**: A 4GB heap is reasonable for small to medium workloads. For larger graphs or high-concurrency queries, you might need to increase it (e.g., 8GB) if you have more RAM.
  - **Page Cache**: The 8GB page cache is good for databases up to ~16–80GB, depending on access patterns. If your database grows significantly larger, consider increasing the page cache or RAM.
  - **Monitoring**: Use Neo4j’s monitoring tools (e.g., metrics or logs) to check page cache hit ratios and heap usage. If the cache hit ratio is low, increase the page cache size or RAM.
  - **OS**: Use a 64-bit OS (e.g., Linux, Ubuntu, or CentOS) for Neo4j, as 32-bit systems can’t handle large memory allocations effectively.
- **Workload Context**: If you can share the expected number of nodes, relationships, or query types (e.g., read-heavy vs. write-heavy), I can refine the SSD estimate further.

### Example Hardware Setup
- **Small Setup (Development)**: 16GB RAM, 64GB SSD, single-socket CPU (4–8 cores).
- **Production Setup (Medium Workload)**: 32GB RAM, 256GB NVMe SSD, 8–16 core CPU for handling concurrent queries.


---

### Step 5: Create the Docker Compose File. Launch, Verify

Create the `docker-compose.yml` file. This is the heart of the deployment, binding the service specifically to your LAN IP and using the robust `cypher-shell` healthcheck.

**Remember to replace `YOUR_SERVER_IP` in this file.**

```yaml

services:
  neo4j:
    image: neo4j:5-enterprise          # Tracks the latest 5.x version
    container_name: neo4j_local_secure
    restart: unless-stopped
    user: "101:101"                    # Run as a non-root user

    # Security: Bind host ports specifically to your LAN IP, not 0.0.0.0 (all interfaces).
    ports:
      - "YOUR_SERVER_IP:7473:7473"     # Secure HTTPS
      - "YOUR_SERVER_IP:7687:7687"     # Secure Bolt

    volumes:
      - ./data:/data
      - ./logs:/logs
      - ./conf:/conf
      - ./plugins:/plugins
      - ./certificates:/certificates
      - ./exports:/exports

    environment:
      - NEO4J_AUTH_FILE=/run/secrets/neo4j_auth_secret
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes

    secrets:
      - source: neo4j_auth_secret
        target: neo4j_auth_secret

    healthcheck:
      # Robust check using cypher-shell to query the database directly.
      # 'bolt+ssc' scheme trusts the self-signed certificate for the healthcheck.
      test: ["CMD-SHELL", "PASS=$$(cut -d/ -f2 /run/secrets/neo4j_auth_secret) && cypher-shell -a bolt+ssc://localhost:7687 -u neo4j -p \"$$PASS\" 'RETURN 1' >/dev/null 2>&1 || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10

secrets:
  neo4j_auth_secret:
    file: ./secrets/neo4j_auth
```

#### For localhost:

```bash
    # Security: Bind host ports specifically to localhost (127.0.0.1).
    ports:
      - "127.0.0.1:7473:7473"     # Accessible only from https://localhost:7473
      - "127.0.0.1:7687:7687"     # Accessible only from bolt+ssc://localhost:7687
```


You are now ready to start the service and connect.

```bash
# 1. Start the service in the background
docker compose up -d

# 2. Check the status. Wait for the STATUS to become 'running (healthy)'
docker compose ps

# 3. (Optional) Follow the logs on first startup
docker logs -f neo4j_local_secure
```


### Harden with a Firewall (Recommended)

For true defense-in-depth, configure a firewall on the host machine to only allow traffic to the Neo4j ports from your trusted local network.

**UFW (Ubuntu) Example:**

```bash
# Replace with your LAN subnet (e.g., 192.168.1.0/24)
export LAN_SUBNET="YOUR_LAN_SUBNET"

# Allow incoming connections from the LAN to the Neo4j ports
sudo ufw allow from ${LAN_SUBNET} to any port 7473 proto tcp
sudo ufw allow from ${LAN_SUBNET} to any port 7687 proto tcp

# Ensure UFW is enabled and check the status
sudo ufw enable
sudo ufw status
```
**Why this is important:** Even if Docker is bound to a specific IP, the firewall acts as a second, powerful layer of defense against unwanted network access.


# Connecting to Your Secure Neo4j Docker Instance 

This guide provides detailed instructions on how to connect to and verify your secure Neo4j 5.x Docker container. It covers all common scenarios, including connecting from the host machine, a remote machine on the same network, and securely through an SSH tunnel.

## Prerequisites

Before you begin, ensure you have the following information and tools:

1.  **Server IP Address**: The LAN IP address of the machine hosting the Docker container (e.g., `192.168.1.101`). This will be referred to as `YOUR_SERVER_IP`.
2.  **Neo4j Password**: The secure password you generated. You can retrieve it from the host machine at any time by running:
    ```bash
    # Run this from your neo4j-local-secure directory
    sudo cat secrets/neo4j_password
    ```
3.  **`cypher-shell`**: The official Neo4j command-line interface. This must be installed on any machine you wish to connect from.

---

**How to Connect:**

*   **Neo4j Browser:** Navigate to `https://YOUR_SERVER_IP:7473`
    *   *You will see a security warning. This is expected. Click "Advanced" and proceed.*
*   **Drivers & Tools:** Use a secure connection string. The `+ssc` scheme is a convenient shortcut that trusts self-signed certificates without needing a custom trust store.
    *   `bolt+ssc://YOUR_SERVER_IP:7687`
*   **`cypher-shell` from your host machine:**
    ```bash
    cypher-shell -a bolt+ssc://YOUR_SERVER_IP:7687 -u neo4j -p "$(cat secrets/neo4j_password)"
    ```


#### For Localhost: 
**Local Port Forwarding** (or an "SSH Tunnel").

you bound the ports to `127.0.0.1` on the server, you cannot connect to `https://YOUR_SERVER_IP:7473`. 

The solution is to tell SSH to create a secure tunnel from your local machine, through the SSH connection, directly to the `localhost` port on the remote server.

### The Concept

You will forward a port on your **local machine** (the one you are typing on) to the Neo4j port on the **remote server's localhost**.

*   Traffic going into `localhost:7473` on **your laptop**
*   ...travels securely through the SSH connection...
*   ...and comes out on `localhost:7473` on the **remote server**, where Neo4j is listening.

### The Solution: The `-L` Flag in SSH

You need to forward two ports: `7473` for the browser (HTTPS) and `7687` for the Bolt driver. You can do this in a single SSH command by using the `-L` flag twice.

#### Disconnect and Reconnect with Port Forwarding

First, if you are currently connected to your server, `exit` that SSH session.

Now, reconnect using the following command. This is the **only command you need to change**.

```bash
# General Syntax: ssh -L <local_port>:<destination_host>:<destination_port> user@server
#
# We will forward:
# - Our local port 7473 to the remote server's localhost:7473
# - Our local port 7687 to the remote server's localhost:7687

ssh -L 7473:localhost:7473 -L 7687:localhost:7687 user@xxx.xxx.xxx.xxx
```

**Command Breakdown:**

*   **`ssh user@the.server.ip.addreess`**: Your standard SSH login command.
*   **`-L 7473:localhost:7473`**:
    *   `-L`: Specifies **L**ocal port forwarding.
    *   `7473`: The port to open on **your local machine**.
    *   `localhost`: The destination host *from the remote server's perspective*. We want to connect to `localhost` on the server.
    *   `7473`: The destination port on the remote server.
*   **`-L 7687:localhost:7687`**: Does the same thing for the Bolt port.

#### Keep the SSH Connection Open

As long as this SSH terminal window is open, the secure tunnels are active. If you close this window, the tunnels will close.

#### Connect Using `localhost` on Your Local Machine

Now, on your local machine (your laptop), you can access Neo4j as if it were running locally.

*   **Open Your Web Browser (on your laptop):**
    Navigate to: `https://localhost:7473`
    *   Your browser will send the request to your local port 7473.
    *   SSH will intercept it, send it through the tunnel, and deliver it to Neo4j on the server.
    *   You will still see the security warning for the self-signed certificate, which is expected.

*   **Use `cypher-shell` or a Driver (from your laptop):**
    Use the connection string: `bolt+ssc://localhost:7687`
    ```bash
    # You would run this in a NEW terminal window on your local machine,
    # NOT in the SSH window.
    cypher-shell -a bolt+ssc://localhost:7687 -u neo4j -p "YOUR_SECURE_PASSWORD"
    ```

This is the standard and most secure way to manage services that are intentionally not exposed to the network. You maintain a very high level of security on the server while still having full access for management from your trusted machine.

---

## Connection Methods

The correct connection method depends on how you configured the `ports` section in your `docker-compose.yml` file.

### Scenario 1: Ports are Bound to Your Server's LAN IP

This is the standard setup for making Neo4j available to other machines on your local network. Your `docker-compose.yml` `ports` section looks like this:

```yaml
    ports:
      - "YOUR_SERVER_IP:7473:7473" # Secure HTTPS
      - "YOUR_SERVER_IP:7687:7687" # Secure Bolt
```

#### ► Method 1: Connecting from the Host Machine Terminal

This is the most direct way to interact with the database.

1.  **Open a terminal** on the machine running the Docker container.
2.  **Navigate** to your `neo4j-local-secure` project directory.
3.  **Run the `cypher-shell` command**:

    ```bash
    cypher-shell \
      -a "bolt+ssc://YOUR_SERVER_IP:7687" \
      -u neo4j \
      -p "$(sudo cat secrets/neo4j_password)"
    ```
    > **Note**: The `+ssc` in the address tells the client to trust the server's **S**elf-**S**igned **C**ertificate, which is exactly what we need for this setup.

4.  **Verify the connection** as described in the [Verification](#verification) section below.

#### ► Method 2: Connecting from a Remote Machine Terminal

You can connect from any other computer on the same LAN that has `cypher-shell` installed.

1.  **Open a terminal** on your remote machine (e.g., your laptop).
2.  **Run the `cypher-shell` command**. You will be prompted to enter your password securely.

    ```bash
    cypher-shell -a "bolt+ssc://YOUR_SERVER_IP:7687" -u neo4j
    ```
3.  **Enter your password** when prompted.
4.  **Verify the connection** as described in the [Verification](#verification) section.

#### ► Method 3: Connecting via Web Browser

You can access the Neo4j Browser from any machine on the same LAN.

1.  **Open your web browser** (Chrome, Firefox, etc.).
2.  **Navigate to the following URL**:
    `https://YOUR_SERVER_IP:7473`
3.  **Handle the Security Warning**: Your browser will show a privacy or security warning because the certificate is self-signed. This is expected.
    *   Click **"Advanced"**.
    *   Click **"Proceed to YOUR_SERVER_IP (unsafe)"** or "Accept the Risk and Continue".
4.  **Log in**: Use `neo4j` as the username and your secure password. The connection URI should default to `bolt://YOUR_SERVER_IP:7687`.
5.  Click **Connect**.

---

### Scenario 2: Ports are Bound to `localhost` (127.0.0.1)

This is a high-security setup where the database is not exposed to the network at all. Remote access is only possible via a secure SSH tunnel. Your `docker-compose.yml` `ports` section looks like this:

```yaml
    ports:
      - "127.0.0.1:7473:7473"
      - "127.0.0.1:7687:7687"
```

#### ► Method 1: Connecting from the Host Machine (Terminal & Browser)

Connecting from the host is simple because you are already on `localhost`.

*   **Terminal**:
    ```bash
    cypher-shell \
      -a "bolt+ssc://localhost:7687" \
      -u neo4j \
      -p "$(sudo cat secrets/neo4j_password)"
    ```
*   **Browser**:
    1.  Navigate to `https://localhost:7473`.
    2.  Bypass the security warning as explained above.
    3.  Log in with your credentials.

#### ► Method 2: Connecting from a Remote Machine (via SSH Tunnel)

This is the standard, secure way to manage a service that is not exposed to the network.

1.  **Establish the SSH Tunnel**: From your remote machine's terminal, run the following command to connect to your server. This command forwards your local ports `7473` and `7687` through the SSH connection to the server's `localhost`.

    ```bash
    # Syntax: ssh -L <local_port>:<destination_host>:<destination_port> user@server
    ssh -L 7473:localhost:7473 -L 7687:localhost:7687 your_user@YOUR_SERVER_IP
    ```
    **Important**: You must keep this SSH terminal window open. The tunnel remains active only as long as this connection is open.

2.  **Connect in a *New* Terminal or Browser**: With the tunnel active, open a **new** terminal window or your browser on your remote machine and connect to `localhost` as if Neo4j were running locally.

    *   **New Terminal**:
        ```bash
        # Connect to your local port, which SSH will forward to the server
        cypher-shell -a "bolt+ssc://localhost:7687" -u neo4j
        ```
    *   **Browser**:
        1.  Navigate to `https://localhost:7473`.
        2.  Bypass the security warning.
        3.  Log in. Your traffic will be securely tunneled to the server.

---

## Verification

A successful connection can be confirmed in two ways.

### In `cypher-shell`

Upon successful connection, your terminal prompt will change to:
`neo4j@bolt+ssc://...> `

Run this basic command to confirm the database is responsive:

```cypher
SHOW DATABASES;
```

A successful query will return a table of the available databases:

```
+--------------------------------------------------------------------------------------------+
| name     | type       | aliases | access      | address               | role      | writer |
+--------------------------------------------------------------------------------------------+
| "neo4j"  | "standard" | []      | "read-write"| "localhost:7687"      | "primary" | TRUE   |
| "system" | "system"   | []      | "read-write"| "localhost:7687"      | "primary" | TRUE   |
+--------------------------------------------------------------------------------------------+
```

To exit the shell, type `:exit` and press Enter.

### In Neo4j Browser

After logging in, you will see the main Neo4j Browser interface. The top-left corner will show that you are connected to the database, and you can type Cypher queries into the editor at the top of the screen.


---

# Managing Graph Data in Your Secure Neo4j Docker Instance

This guide provides essential instructions for managing the data within your Neo4j database. It covers how to perform logical exports of your entire graph into portable formats and how to completely clear the database of all nodes and relationships for a clean reset.

These operations are performed while the database is running and do not require you to stop the container.

## Table of Contents
- [Prerequisite: Installing the APOC Plugin](#prerequisite-installing-the-apoc-plugin)
- [Exporting All Graph Data](#exporting-all-graph-data)
- [Deleting All Graph Data](#deleting-all-graph-data)

---

## Prerequisite: Installing the APOC Plugin

The most powerful and flexible way to manage data is with the **APOC ("Awesome Procedures On Cypher")** library. This is a one-time setup.

1.  **Download APOC**: Go to the [APOC Releases page](https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases) and find the release that **exactly matches your Neo4j version**. For example, if you are using Neo4j `5.26.10`, download the APOC `5.26.10` JAR file.
    *   Under the "Assets" section, download the file named `apoc-x.x.x-core.jar`.

2.  **Place the Plugin**: Move the downloaded `.jar` file into the `./plugins` directory of your project on the host machine.
    ```bash
    # Example command
    mv ~/Downloads/apoc-5.26.10-core.jar ./plugins/
    ```

3.  **Configure Neo4j to Allow APOC**: Edit your `conf/neo4j.conf` file and add the following line. This gives APOC the necessary permissions to perform operations like writing files.

    ```ini
    # Add this line to the end of your conf/neo4j.conf
    dbms.security.procedures.unrestricted=apoc.*
    ```

4.  **Restart the Container**: To load the plugin and apply the new configuration, restart your Docker service from your project directory.
    ```bash
    docker compose down && docker compose up -d
    ```

---

## Exporting All Graph Data

These procedures export your data into files saved on the host machine, making them easy to access, share, or use for migration.

#### Setup: Create a Shared Directory for Exports

To easily retrieve exported files, we'll map a local directory into the container.

1.  **Create an `exports` directory** on your host:
    ```bash
    mkdir ./exports
    ```
2.  **Add the volume to `docker-compose.yml`**:
    ```yaml
    # in your docker-compose.yml
    services:
      neo4j:
        # ... other settings
        volumes:
          - ./data:/data
          - ./logs:/logs
          - ./conf:/conf
          - ./plugins:/plugins
          - ./certificates:/certificates
          - ./exports:/exports   # <--- ADD THIS LINE
    ```
3.  **Restart the container** if you just added the volume: `docker compose up -d --force-recreate`.

#### ► Export Option 1: Cypher Script
This method creates a single `.cypher` file containing all the `CREATE` statements needed to perfectly rebuild your graph. It is ideal for backups and migrations to other Neo4j instances.

Connect to your database via `cypher-shell` or the Neo4j Browser and run:
```cypher
/*
  This procedure will create a 'all-data.cypher' file
  inside the ./exports directory on your host machine.
*/
CALL apoc.export.cypher.all('all-data.cypher', {
  format: 'cypher-shell',
  useOptimizations: {type: 'UNWIND', unwindBatchSize: 100}
});
```

#### ► Export Option 2: GraphML
GraphML is a standard XML-based format that can be imported into other graph visualization and analysis tools, such as Gephi.

```cypher
/*
  This procedure will create a 'all-data.graphml' file
  inside the ./exports directory on your host machine.
*/
CALL apoc.export.graphml.all('all-data.graphml', {});
```

---

## Deleting All Graph Data

This action removes **all nodes and relationships** from your `neo4j` database. It does **not** remove your user accounts, and it leaves your schema (indexes and constraints) intact.

#### ► Method 1: Simple Delete (For Small to Medium Graphs)
This command is easy to remember and effective for databases that are not excessively large.

```cypher
MATCH (n) DETACH DELETE n;
```
> **Warning**: On very large graphs (millions of nodes/edges), this single transaction can consume a large amount of memory and may fail. For large datasets, use the batched method below.

#### ► Method 2: Batched Delete (For Very Large Graphs)
This is the recommended, robust method for clearing any size of graph. It uses an APOC procedure to delete nodes in smaller, manageable batches, preventing memory issues.

```cypher
/*
  This procedure finds all nodes and runs DETACH DELETE on them in
  batches of 50,000 until the database is empty.
*/
CALL apoc.periodic.iterate(
  'MATCH (n) RETURN n',
  'DETACH DELETE n',
  {batchSize: 50000}
)
YIELD batches, total;
```

#### Verification
After running a delete command, you can confirm the database is empty with the following query. The expected result is `0`.
```cypher
MATCH (n) RETURN count(n);
```

================================================================================
--- File: databases/chromadb/docker-compose.yml ---
================================================================================

# /chroma-secure/docker-compose.yml
# DEVELOPMENT-ONLY (INSECURE) CONFIGURATION

services:
  chroma:
    image: chromadb/chroma:0.5.3
    container_name: chroma_local_dev
    restart: unless-stopped

    # Security: Expose the ChromaDB port directly, but only to localhost.
    ports:
      - "127.0.0.1:8443:8000"

    volumes:
      # Map the persistent data directory.
      - ./data:/chroma/chroma

    environment:
      - IS_PERSISTENT=TRUE
      - ALLOW_RESET=TRUE

    healthcheck:
      # The healthcheck now directly queries the container's internal port.
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 15s
      timeout: 5s
      retries: 5

================================================================================
--- File: databases/chromadb/chromadb.md ---
================================================================================

Of course. We can absolutely simplify the ChromaDB deployment by removing the Caddy reverse proxy and all security layers. This is a very common approach for a purely local development environment where the primary goal is to get the service running quickly and easily.

This guide provides the complete, fresh instructions, starting with a thorough cleanup of the previous secure setup.

---

### **Development-Grade ChromaDB Deployment (Insecure)**

**Objective:** To deploy a simple, persistent, server-based ChromaDB instance for local development. This setup removes all encryption and authentication layers for ease of use.

**Security Posture:**
*   **No Caddy Proxy:** The ChromaDB service will be exposed directly.
*   **No TLS/SSL:** All network traffic will be in plaintext (HTTP).
*   **No Authentication:** The API is open to any client that can reach the port.
*   **Network Isolation:** As a safety best practice, the service will still be bound to `localhost` to prevent accidental exposure to your local network.

---

### **Step 1: Complete Cleanup of the Secure Deployment**

First, we will stop and completely remove the Caddy and Chroma containers, along with their associated volumes and configuration files, to ensure a clean slate.

```bash
# Navigate to the project directory
cd /chroma-secure

# 1. Stop and remove the containers, and crucially, the persistent data volume (-v flag).
sudo docker compose down -v

# 2. Delete the configuration files that are no longer needed for this setup.
sudo rm -rf Caddyfile/ certificates/
```
This leaves you with just the `/chroma-secure` directory and its empty `data` subdirectory, ready for the new configuration.

---

### **Step 2: Create the Simplified `docker-compose.yml` File**

This configuration is minimal. It defines only the ChromaDB service and exposes its port directly to your `localhost`.

**Replace the entire contents** of your `/chroma-secure/docker-compose.yml` file with the following:

```yaml
# /chroma-secure/docker-compose.yml
# DEVELOPMENT-ONLY (INSECURE) CONFIGURATION

services:
  chroma:
    image: chromadb/chroma:0.5.3
    container_name: chroma_local_dev
    restart: unless-stopped

    # Security: Expose the ChromaDB port directly, but only to localhost.
    ports:
      - "127.0.0.1:8443:8000"

    volumes:
      # Map the persistent data directory.
      - ./data:/chroma/chroma

    environment:
      - IS_PERSISTENT=TRUE
      - ALLOW_RESET=TRUE

    healthcheck:
      # The healthcheck now directly queries the container's internal port.
      test: ["CMD", "curl", "-f", "http://localhost:8443/api/v1/heartbeat"]
      interval: 15s
      timeout: 5s
      retries: 5
```
**Key Changes:**
*   **Single Service:** The `caddy` service has been completely removed.
*   **Direct Port Exposure:** The `chroma` service now has a `ports` section, binding the container's port `8000` to your host machine's `localhost:8000`.
*   **Simplicity:** No certificate volumes, no complex proxy logic.

---

### **Step 3: Launch and Verify the Service**

Now, start the new, simplified container.

```bash
# Ensure you are still in the /chroma-secure directory
cd /chroma-secure

# 1. Start the service in the background.
sudo docker compose up -d

# 2. Check the status. It should start quickly and become healthy.
sudo docker compose ps
```
You should see a single container named `chroma_local_dev` with a status of `running (healthy)` and a port mapping of `127.0.0.1:8000->8000/tcp`.

---

### **Step 4: Connecting to Your Insecure ChromaDB Instance**

Connecting to this instance is now much simpler. The client does not need to handle SSL/TLS.

1.  **Install the library (if needed):**
    `pip install chromadb==0.5.3`

2.  **Create and run the simplified Python script (`test_chroma_dev_connection.py`):**
    ```python
    import chromadb
    import uuid

    print("Attempting to connect to insecure (development) ChromaDB server...")

    try:
        # Connection is now a simple HTTP call.
        # No 'ssl=True' and no 'settings' object are needed.
        client = chromadb.HttpClient(host='localhost', port=8000)

        # Verify the connection by checking the heartbeat
        version = client.version()
        print(f"\n✅ Connection successful!")
        print(f"   ChromaDB Version: {version}")

        # Perform a basic operation to confirm functionality
        collection_name = f"dev_test_{uuid.uuid4().hex}"
        collection = client.get_or_create_collection(name=collection_name)
        
        collection.add(
            documents=["This is a test of a simple, insecure connection."],
            ids=["id1"]
        )
        
        results = collection.query(query_texts=["simple test"], n_results=1)
        
        print(f"   Successfully created and queried collection '{collection_name}'.")
        print(f"   Query results: {results['documents'][0]}")
        
        # Clean up the test collection
        client.delete_collection(name=collection_name)
        print(f"   Successfully deleted test collection.")

    except Exception as e:
        print(f"\n❌ An unexpected error occurred. Is the ChromaDB container running?")
        print(f"   Error: {e}")
    ```

You now have a fully functional, persistent ChromaDB instance running for your local development needs, with all the complexity of TLS and proxying removed.

================================================================================
--- File: databases/test_connect/test_redis.py ---
================================================================================

import redis

print("Attempting to connect to insecure (development) Redis server...")

try:
    # Connection requires no password and no SSL/TLS parameters.
    r = redis.Redis(
        host='localhost',
        port=6379,
        decode_responses=True # Optional: makes responses strings instead of bytes
    )

    # Ping the server to confirm the connection is active
    response = r.ping()
    
    if response:
        print("\n✅ Connection successful!")
        
        # Perform a basic SET and GET to verify functionality
        r.set('dev_test', 'success')
        value = r.get('dev_test')
        
        print(f"   Server PING response: {response}")
        print(f"   Successfully SET and GET key 'dev_test' with value: '{value}'")
    else:
        print("\n❌ Connection appeared to succeed, but PING failed.")

except redis.exceptions.ConnectionError as e:
    print("\n❌ Connection failed. Check if the server is running and the port is correct.")
    print(f"   Error: {e}")
except Exception as e:
    print(f"\n❌ An unexpected error occurred: {e}")

================================================================================
--- File: databases/test_connect/test_bm25.py ---
================================================================================

from elasticsearch import Elasticsearch

# 1. Connect to your insecure Elasticsearch instance
client = Elasticsearch("http://localhost:9200")

# --- Indexing ---
# Sample data mirroring your project's needs
passages = [
    {"passage_id": 1, "text_content": "To check your smart card status, send an SMS to 105."},
    {"passage_id": 2, "text_content": "The online portal for smart card services is services.nidw.gov.bd."},
    {"passage_id": 3, "text_content": "You can check the application status for your NID card online."}
]

print("Indexing sample passages...")
# Index each document into an index named 'passage_index'
for doc in passages:
    client.index(
        index="passage_index",
        id=doc["passage_id"],
        document={"text": doc["text_content"]},
        # This makes the document immediately available for searching
        refresh="wait_for" 
    )
print("Indexing complete.")


# --- Searching (This is where BM25 is automatically used) ---
query_text = "check smart card status"
print(f"\nSearching for: '{query_text}'")

# A standard 'match' query uses the BM25 algorithm by default
response = client.search(
    index="passage_index",
    query={
        "match": {
            "text": query_text
        }
    }
)

print("\n--- Search Results (Ranked by BM25 Score) ---")
for hit in response['hits']['hits']:
    # The '_score' is the relevance score calculated by BM25
    score = hit['_score'] 
    passage_text = hit['_source']['text']
    print(f"Score: {score:.4f} | Text: {passage_text}")

# Clean up the index
# client.indices.delete(index="passage_index")

================================================================================
--- File: databases/test_connect/test_postgress.py ---
================================================================================

import psycopg2
import os

# --- IMPORTANT ---
# Replace with the password you saved from Step 2
DB_PASSWORD = "YOUR_SECURE_PASSWORD_HERE"

print("Attempting to connect to secure PostgreSQL server...")

try:
    # The connection string MUST include sslmode='require'.
    # If you use sslmode='disable', the server will reject the connection.
    conn = psycopg2.connect(
        host="localhost",
        port="5432",
        user="postgres",
        dbname="postgres",
        password=DB_PASSWORD,
        sslmode='require' 
    )

    cursor = conn.cursor()
    
    # Execute a simple query to verify the connection
    cursor.execute("SELECT version();")
    db_version = cursor.fetchone()
    
    print("\n✅ Connection successful!")
    print(f"   PostgreSQL Version: {db_version[0]}")

    cursor.close()
    conn.close()

except psycopg2.OperationalError as e:
    print("\n❌ Connection failed.")
    print("   This might be expected if SSL is not correctly configured.")
    print(f"   Error: {e}")
except Exception as e:
    print(f"\n❌ An unexpected error occurred: {e}")

================================================================================
--- File: databases/test_connect/test_chroma.py ---
================================================================================

import chromadb
import uuid

CHROMA_PORT = 8443

print(f"Attempting to connect to insecure ChromaDB server on port {CHROMA_PORT}...")

try:
    # The client now connects to the new port.
    client = chromadb.HttpClient(host='localhost', port=CHROMA_PORT)

    # Verify the connection by calling the heartbeat method
    heartbeat = client.heartbeat()
    print(f"\n✅ Connection successful!")
    print(f"   Server heartbeat response (nanoseconds): {heartbeat}")

    # Perform a basic operation to confirm functionality
    collection_name = f"dev_test_{uuid.uuid4().hex}"
    collection = client.get_or_create_collection(name=collection_name)
    
    collection.add(
        documents=["This is a test of a connection on a custom port."],
        ids=["id1"]
    )
    
    results = collection.query(query_texts=["custom port test"], n_results=1)
    
    print(f"   Successfully created and queried collection '{collection_name}'.")
    print(f"   Query results: {results['documents'][0]}")
    
    # Clean up the test collection
    client.delete_collection(name=collection_name)
    print(f"   Successfully deleted test collection.")

except Exception as e:
    print(f"\n❌ An unexpected error occurred. Is the ChromaDB container running?")
    print(f"   Error: {e}")

================================================================================
--- File: databases/test_connect/test_elasticsearch.py ---
================================================================================

from elasticsearch import Elasticsearch

print("Attempting to connect to insecure Elasticsearch server...")

try:
    # Client connects to 'http' and requires no authentication or certificates.
    client = Elasticsearch("http://localhost:9200")

    # Ping the cluster to verify the connection is active
    if client.ping():
        print("\n✅ Connection successful!")
        
        # Get and print basic cluster info
        info = client.info()
        print(f"   Cluster Name: {info['cluster_name']}")
        print(f"   Elasticsearch Version: {info['version']['number']}")
    else:
        print("\n❌ Connection failed. The ping was unsuccessful. Check container status.")

except Exception as e:
    print(f"\n❌ An unexpected error occurred: {e}")

================================================================================
--- File: databases/redis/docker-compose.yml ---
================================================================================

# /redis-secure/docker-compose.yml
# DEVELOPMENT-ONLY (INSECURE) CONFIGURATION

services:
  redis:
    # We can use the official redis image or bitnami, both work for this.
    # The official image is slightly more lightweight.
    image: redis:7-alpine
    container_name: redis_local_dev
    restart: unless-stopped

    # As a best practice, still bind to localhost to prevent accidental exposure.
    ports:
      - "127.0.0.1:6379:6379"

    volumes:
      # Map the persistent data directory. The internal path is different for the official image.
      - ./data:/data

    healthcheck:
      # The healthcheck is now a simple PING command.
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5

================================================================================
--- File: databases/redis/redis.md ---
================================================================================


### **Development-Grade Redis Deployment via Docker**

**Objective:** To deploy a persistent, non-secure Redis server for local development. All data will reside in `/redis-secure/`, and the service will be accessible from the host machine (`localhost`).

**Security Posture:**
*   **No TLS/SSL:** All network traffic will be in plaintext.
*   **No Password:** Any client that can reach the port can issue commands.
*   **Network Isolation:** The service will still be bound to `localhost` as a best practice to prevent accidental network exposure.

---

### **Step 1: Stop and Clean Up the Previous Secure Deployment**

It is crucial to remove the old container and its associated volumes and secrets to avoid any conflicts.

```bash
# Navigate to the project directory
cd /redis-secure

# 1. Stop and completely remove the services, volumes, and networks
# defined in the previous docker-compose file.
sudo docker compose down -v

# 2. Clean up the configuration and secret files that are no longer needed.
sudo rm -rf secrets/ certificates/ .env
```
This ensures you have a completely clean slate for the new, simpler configuration.

---

### **Step 2: Create the Simplified `docker-compose.yml` File**

This configuration is minimal. It defines the Redis service, a persistent data volume, and disables password protection.

**Replace the entire contents** of your `/redis-secure/docker-compose.yml` file with the following:

```yaml
# /redis-secure/docker-compose.yml
# DEVELOPMENT-ONLY (INSECURE) CONFIGURATION

services:
  redis:
    # We can use the official redis image or bitnami, both work for this.
    # The official image is slightly more lightweight.
    image: redis:7-alpine
    container_name: redis_local_dev
    restart: unless-stopped

    # As a best practice, still bind to localhost to prevent accidental exposure.
    ports:
      - "127.0.0.1:6379:6379"

    volumes:
      # Map the persistent data directory. The internal path is different for the official image.
      - ./data:/data

    healthcheck:
      # The healthcheck is now a simple PING command.
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
```

**Key Changes:**
*   **Image:** Switched to the lightweight official `redis:7-alpine` image.
*   **No Security:** All `environment`, `secrets`, and certificate `volumes` have been removed. The official Redis image runs without a password by default.
*   **Data Volume Path:** The internal data path for the official image is `/data`.
*   **Healthcheck:** Simplified to a basic `ping` command, as no authentication or TLS is needed.

---

### **Step 3: Launch and Verify the Service**

Now, start the new, simplified container.

```bash
# Ensure you are still in the /redis-secure directory
cd /redis-secure

# 1. Start the service in the background.
sudo docker compose up -d

# 2. Check the status. It should start quickly and become healthy.
sudo docker compose ps
```
The status should show `running (healthy)` within a few seconds.

---

### **Step 4: Connecting to Your Insecure Instance**

Connecting to this instance is straightforward as there are no security hurdles.

Here is the updated Python script to test the connection.

1.  **Install the library (if you haven't already):**
    ```bash
    pip install redis
    ```

2.  **Create and run a Python script (`test_redis_dev_connection.py`):**
    ```python
    import redis

    print("Attempting to connect to insecure (development) Redis server...")

    try:
        # Connection requires no password and no SSL/TLS parameters.
        r = redis.Redis(
            host='localhost',
            port=6379,
            decode_responses=True # Optional: makes responses strings instead of bytes
        )

        # Ping the server to confirm the connection is active
        response = r.ping()
        
        if response:
            print("\n✅ Connection successful!")
            
            # Perform a basic SET and GET to verify functionality
            r.set('dev_test', 'success')
            value = r.get('dev_test')
            
            print(f"   Server PING response: {response}")
            print(f"   Successfully SET and GET key 'dev_test' with value: '{value}'")
        else:
            print("\n❌ Connection appeared to succeed, but PING failed.")

    except redis.exceptions.ConnectionError as e:
        print("\n❌ Connection failed. Check if the server is running and the port is correct.")
        print(f"   Error: {e}")
    except Exception as e:
        print(f"\n❌ An unexpected error occurred: {e}")
    ```

This setup provides a functional, persistent Redis database for your local development needs.

================================================================================
--- File: databases/elasticsearch/docker-compose.yml ---
================================================================================

# /elasticsearch-secure/docker-compose.yml
# COMPLETELY INSECURE ELASTICSEARCH DEPLOYMENT (FOR DEVELOPMENT ONLY)

services:
  elasticsearch:
    image: elasticsearch:8.15.1
    container_name: elastic_local_insecure
    restart: unless-stopped

    # Bind the HTTP port only to localhost.
    ports:
      - "127.0.0.1:9200:9200"

    volumes:
      - ./data:/usr/share/elasticsearch/data

    environment:
      # --- Cluster Setup ---
      - discovery.type=single-node

      # --- THIS IS THE KEY CHANGE ---
      # Explicitly disable all security features (password, TLS, etc.).
      - xpack.security.enabled=false

      # --- Performance Tuning ---
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"

    healthcheck:
      # The check is now a simple, unauthenticated HTTP request.
      test: >
        [ "CMD-SHELL", "curl -s http://localhost:9200/_cluster/health |
          grep -q '\"status\":\"green\|yellow\"'" ]
      interval: 20s
      timeout: 10s
      retries: 5

# NOTE: There is no 'secrets:' block in this file.

================================================================================
--- File: databases/elasticsearch/BM25.md ---
================================================================================

Of course. For a purely local development environment where you want zero friction, we can disable all security features for Elasticsearch, including both TLS and password authentication.

This guide provides the complete, fresh instructions for deploying a fully insecure, open Elasticsearch instance.

---

### **Development-Grade Elasticsearch Deployment (No Security)**

**Objective:** To deploy a persistent, completely insecure Elasticsearch server for local development. This setup removes all encryption and password layers for maximum simplicity and ease of access.

**Security Posture:**
*   **No TLS/SSL:** All traffic is in plaintext (HTTP).
*   **No Password:** The API is completely open to any client that can reach the port. This is **highly insecure and suitable only for a trusted local machine.**
*   **Network Isolation:** The service port (`9200`) will still be bound to `localhost` as a final safety measure.

---

### **Table of Contents**
- [Step 1: Complete Cleanup of Previous Deployments](#step-1-complete-cleanup-of-previous-deployments)
- [Step 2: Create Project Structure and Set Permissions](#step-2-create-project-structure-and-set-permissions-1)
- [Step 3: Create the Final, Insecure Docker Compose File](#step-3-create-the-final-insecure-docker-compose-file)
- [Step 4: Launch and Verify the Service](#step-4-launch-and-verify-the-service-1)
- [Step 5: Connecting to Your Insecure Instance](#step-5-connecting-to-your-insecure-instance)

---

### **Step 1: Complete Cleanup of Previous Deployments**

It is critical to remove all remnants of the previous secure setups to ensure a clean start.

```bash
# Navigate to the project directory
cd /elasticsearch-secure

# 1. Stop and remove the container, its network, and its persistent data volume (-v).
sudo docker compose down -v

# 2. Delete the secret files that are no longer needed.
sudo rm -rf secrets/
```

### **Step 2: Create Project Structure and Set Permissions**

The structure is now very simple, but we still need to set the correct permissions for the data directory.

```bash
# 1. Re-create the directory structure (if you deleted the parent).
sudo mkdir -p /elasticsearch-secure/data

# 2. Set ownership for the Elasticsearch user (UID/GID 1000).
sudo chown -R 1000:1000 /elasticsearch-secure/data
```

### **Step 3: Create the Final, Insecure Docker Compose File**

This is the most minimal configuration possible. We will explicitly disable the X-Pack security module.

**Replace the entire contents** of `/elasticsearch-secure/docker-compose.yml` with the following:

```yaml
# /elasticsearch-secure/docker-compose.yml
# COMPLETELY INSECURE ELASTICSEARCH DEPLOYMENT (FOR DEVELOPMENT ONLY)

services:
  elasticsearch:
    image: elasticsearch:8.15.1
    container_name: elastic_local_insecure
    restart: unless-stopped

    # Bind the HTTP port only to localhost.
    ports:
      - "127.0.0.1:9200:9200"

    volumes:
      - ./data:/usr/share/elasticsearch/data

    environment:
      # --- Cluster Setup ---
      - discovery.type=single-node

      # --- THIS IS THE KEY CHANGE ---
      # Explicitly disable all security features (password, TLS, etc.).
      - xpack.security.enabled=false

      # --- Performance Tuning ---
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"

    healthcheck:
      # The check is now a simple, unauthenticated HTTP request.
      test: >
        [ "CMD-SHELL", "curl -s http://localhost:9200/_cluster/health |
          grep -q '\"status\":\"green\|yellow\"'" ]
      interval: 20s
      timeout: 10s
      retries: 5

# NOTE: There is no 'secrets:' block in this file.
```

### **Step 4: Launch and Verify the Service**

The startup should be slightly faster without the security module initializing.

```bash
# Ensure you are in the /elasticsearch-secure directory
cd /elasticsearch-secure

# 1. Start the service.
sudo docker compose up -d

# 2. Check the status. Wait for it to become 'running (healthy)'.
sudo docker compose ps
```

### **Step 5: Connecting to Your Insecure Instance**

The client connection is now as simple as possible.

1.  **Install the library (if needed):**
    `pip install elasticsearch==8.15.1`

2.  **Create and run the simplest Python script (`test_elastic_insecure_connection.py`):**
    ```python
    from elasticsearch import Elasticsearch

    print("Attempting to connect to insecure Elasticsearch server...")

    try:
        # Client connects to 'http' and requires no authentication or certificates.
        client = Elasticsearch("http://localhost:9200")

        # Ping the cluster to verify the connection is active
        if client.ping():
            print("\n✅ Connection successful!")
            
            # Get and print basic cluster info
            info = client.info()
            print(f"   Cluster Name: {info['cluster_name']}")
            print(f"   Elasticsearch Version: {info['version']['number']}")
        else:
            print("\n❌ Connection failed. The ping was unsuccessful. Check container status.")

    except Exception as e:
        print(f"\n❌ An unexpected error occurred: {e}")

    ```

You have now successfully deployed a completely open Elasticsearch instance for your local development. This completes the setup for all the data services in your architecture.

================================================================================
--- File: d23/data.md ---
================================================================================

# Agendas

* Datasets
* Models
* VPA Overall System Integration


---

# Datasets

## BanServ2SQL 

```
d.1.a) BanServ2SQL: The Bangla Service to SQL dataset contains Bangla service-related running texts and their corresponding structured information through SQL. The Collection of Bangla running and raw text covering general unstructured data along with Bangladeshi governmental and private services and use-case-related unstructured data so that the VPA system could retrieve the service-related necessary information. The dataset should be collected from real use and be non-duplicate, not generated by an algorithm and the metadata should be checked by a single human. Document or paragraph-level clean data could be collected from all service-providing and knowledge government portals such as the National portal (for all types of administrative notice, contact name- number, rules of business, citizen charter, etc.). 
Also, other Publicly open and public serving portals might be useful such as PSC (for govt information).Also, there should have a portion on private service providers such as food delivery portals (for location, address, and menu), newspaper portals (for shopping, job, sports, and related queries), city guides (for transport schedule), Banking portals (for loan and currency, ATM location,branch information), etc.
The size of the dataset should be at least 3GB considering UTF-8 clean text or 2 lac documents which is higher. 
Additionally, It could add information from copyright non-restricted (i.e Wiki article).

d.1.b) The dataset is a converted format of running text into the structured format as SQL dB. It is a curated and selected representation of the aforementioned text. The main feature of this dataset is to arrange structured/ tabular data from running/ unstructured text. The size of the dataset is at least 2 lac queries having balance and representativeness as per use cases of VPA. 
For format, Wiki2SQL and Piccard datasets might be followed.
```

### Passage collection
* Datasources: PREVIEW
* Collection: 
    * Scrapping,crawling,manual collection etc
    * hugging face - oscar data, wikipedia data etc 
    * metadata storing
* Cleaning 
    * Remove references like [1], [2], etc.
    * Remove URLs
    * Remove HTML entities
    * Normalize whitespace
    * Preserve Bangla characters, spaces, and specific punctuation (.,?!)
* Normalization (During useage)
    * We are using [bnunicodenormalizer](https://github.com/mnansary/bnUnicodeNormalizer)
    * Not NFKD,NFKC due to ZWNJ
    * Not utf-8 clean 

```json
{
  "meta": {
    "title": "মনস্তত্ত্ববিদের তালিকা",
    "id": 1437,
    "source": "wikidump"
  },
  "text": "এই তালিকায়, উল্লেখযোগ্য মনস্তত্ত্ববিদ এবং মনোবিজ্ঞানে অবদানকারী অন্তর্ভুক্ত। মনস্তত্ত্ববিদদের নিচে তালিকাভুক্ত করা হলোঃ  আ আব্রাহাম মাসলো উ উইলিয়াম জেমস ই ইবনে রুশদ ইভান পাভলভ ক ক্যারল গিলিগ্যান জ জঁ পিয়াজেঁ ন নোম চম্স্কি ভ ভি এস রামচন্দ্রন ম মার্টিন ডেইলি র রবিন ডানবার স সিগমুন্ড ফ্রয়েড স্টিভেন পিংকার আরোও দেখুন বিষয়শ্রেণীমনস্তত্ববিদ বিষয়শ্রেণীমনোবিজ্ঞানী"
}
```

### SQL Formatting 
* using chromadb
* **THIS WILL BE AGAIN DISCUSSED IN MODELING SECTION**

## BanConvComm

```
d.2) BanConvComm: The Bangla conversation and command dataset/ chat dataset should cover all types of chat-related queries maintaining dialogue flow. 
It will include all possible rational responses considering the question/command. 
Also, the intent of the queries should be kept in this dataset. 
The input question and command should include short commands to long questions.
It should cover all general common questions/commands along with the necessary intents for classifications. 
Also, the dataset will consider the sequences of sentences to understand the dialogue flow. 
The size of the dataset is at least 2 lac question response alignment covering at least eight intents.
```

* Free form intent / unsupervised covering more than eight intents
* hierarchical intent 
    * advantage: <BOE> and <EOE> classifications are not necessary 
* natural conversation flow for context 

```json
{
  "user": [
    "তুমি কি জানো ১৭০১ সালে সুইজারল্যান্ডের প্রোটেস্ট্যান্টরা কী প্রবর্তন করেন?",
    "ওহ, তা খুব আগ্রহের! আর দক্ষিণ করোলিনার চার্লসস্টনে কখন জনগণের জন্য প্রথম ঔপনিবেশিক আমেরিকান জাদুঘর খোলা হয়?",
    "সেটা খুব ভালো! আর ভারতের গভর্নর জেনারেল হিসেবে কে কলকাতায় আসেন?",
    "তারপর? পূর্ব পাকিস্তানের গভর্নর কে ছিলেন?",
    "সেটা খুব ভালো! আর বাংলাদেশ চীন মৈত্রী সম্মেলন কেন্দ্রের উদ্বোধন কখন হয়?",
    "ওহ, তা খুব আগ্রহের! আর ক্যাপ্টেন নুরুল হক কী ছিলেন?",
    "সেটা খুব ভালো! আর আটলি এস এম হায়দার কে ছিলেন?",
    "তারপর? নেলী সেনগুপ্তা কে ছিলেন?"
  ],
  "response": [
    "হ্যাঁ, ১৭০১ সালে সুইজারল্যান্ডের প্রোটেস্ট্যান্টরা খ্রিস্টীয় ক্যালেন্ডার প্রবর্তন করেন। তারা এই ক্যালেন্ডারটি ব্যবহার করে তাদের ধর্মীয় ও সামাজিক কার্যক্রম পরিচালনা করত।",
    "১৭৭৩ সালে দক্ষিণ করোলিনার চার্লসস্টনে জনগণের জন্য প্রথম ঔপনিবেশিক আমেরিকান জাদুঘর খোলা হয়। এটি ছিল একটি গুরুত্বপূর্ণ ঘটনা যা আমেরিকান সংস্কৃতি ও ইতিহাসের বিকাশে ভূমিকা রাখে।",
    "১৮৪৮ সালে লর্ড ডালহৌসি ভারতের গভর্নর জেনারেল হিসেবে কলকাতায় আসেন। তিনি ভারতের ইতিহাসে একজন গুরুত্বপূর্ণ ব্যক্তিত্ব ছিলেন।",
    "রিয়ার এডমিরাল এসএম আহসান পূর্ব পাকিস্তানের গভর্নর ছিলেন। তিনি পূর্ব পাকিস্তানের রাজনৈতিক ও সামাজিক কার্যক্রমে গুরুত্বপূর্ণ ভূমিকা পালন করেন।",
    "২০০২ সালে বাংলাদেশ চীন মৈত্রী সম্মেলন কেন্দ্রের উদ্বোধন হয়। এটি বাংলাদেশ ও চীনের মধ্যে সম্পর্কের একটি গুরুত্বপূর্ণ মাইলফলক।",
    "ক্যাপ্টেন নুরুল হক বাংলাদেশ নৌবাহিনীর প্রথম নৌপ্রধান ছিলেন। তিনি বাংলাদেশের স্বাধীনতা যুদ্ধে গুরুত্বপূর্ণ ভূমিকা পালন করেন।",
    "এ.টি.এম. হায়দার বাংলাদেশী মুক্তিযোদ্ধা এবং বীর উত্তম খেতাব প্রাপ্ত সেক্টর কমান্ডার ছিলেন। তিনি বাংলাদেশের স্বাধীনতা যুদ্ধে গুরুত্বপূর্ণ ভূমিকা পালন করেন।",
    "নেলী সেনগুপ্তা একজন রাজনৈতিক ও সমাজকর্মী ছিলেন যিনি দেশকে পরাধীনতার শৃঙ্খল থেকে রক্ষা করার জন্য নিরলস সংগ্রাম করেছিলেন। তিনি বাংলাদেশের স্বাধীনতা যুদ্ধে গুরুত্বপূর"
  ],
  "intents": [
    "history.switzerland.event",
    "museum.opening_date",
    "government.governor_general.name",
    "government.east_pakistan.governor",
    "conference_center.opening_date",
    "person.biography",
    "person.identity",
    "person.identity"
  ],
  "num_entries": 8
}
```

# BanQA

```
d.3) BanQA: This dataset should include questions/commands, the paragraph/document having inline answers, and specific answers to the corresponding question/s.
The answer could be the response as part of dialogue without returning any specific close result or could be the response/result of having retrieved specific information. Moreover, the questions could have multiple answers or only one accurate result. 
The size of the dataset is at least 2 lac QA covering use cases of the system.
d.3.1) All the datasets should cover a wide range of named entities (name, place, org, Object,GPE/S etc.) and all geo-political entities 
should cover up to the village level for Bangladesh and up to the city level globally.
d.3.2) It is expected that the BanQA will be aligned with other datasets for this assignment. For example, answers could be retrieved from the structure-unstructured repository. Many questions could be found from BanQA text properties, and answers could be available at BanServ2SQL, and on the web.

```

* context based question and answer 
* Not formed as natural conversation - Because a question can be asked isolatedly  

```json
{
  "id": 15239,
  "title": "উপসর্গ (ব্যাকরণ)",
  "text": "উপসর্গ বা আদ্যপ্রত্যয় হলো ভাষায় ব্যবহৃত কিছু অব্যয়সূচক শব্দাংশ যাদের নিজস্ব কোনো অর্থ নেই, কিন্তু অর্থের দ্যোতনা তৈরির ক্ষমতা আছে।বাংলা ভাষার ব্যাকরণ ও নির্মিতি, ২০২১ শিক্ষাবর্ষ, জাতীয় শিক্ষাক্রম ও পাঠ্যপুস্তক বোর্ড, ঢাকা, বাংলাদেশ উপসর্গ শব্দ বা শব্দমূলের শুরুতে বসে নতুন অর্থবহ শব্দ তৈরি করে, শব্দাংশের শুরুতে বসে না। উপসর্গ যুক্ত হলে কোনো শব্দের বিপরীত শব্দ তৈরি হয় অথবা অর্থের উৎকর্ষ বা সংকোচন হয়। উপসর্গ সম্পর্কিত আলোচনা ব্যাকরণের রূপতত্ত্বের অন্তর্ভুক্ত। ব্যুৎপত্তি ও সংজ্ঞা উপসর্গ শব্দটির রূপতত্ত্বগত বিশ্লেষণ  উপ  সৃজ্  অ। উপসর্গ কথাটির মূল অর্থ উপসৃষ্ট। যেসব অর্থহীন অব্যয় পদ নামবাচক বা কৃদান্ত শব্দের পূর্বে যুক্ত হয়ে নতুন শব্দ গঠন করে এবং অর্থের পরির্বতন সাধন করে, এগুলোকে উপসর্গ বলে। ড. সুনীতিকুমার চট্টোপাধ্যায়ের মতে, ড. মুহাম্মদ এনামুল হকের মতে, অশোক মুখোপাধ্যায়ের মতে, ড. রামেশ্বর শএর মতে, উপসর্গ ও অনুসর্গ অনুসর্গ হলো কিছু অব্যয়সূচক শব্দ যেগুলো কখনো স্বাধীন পদরূপে, আবার কখনো শব্দ বিভক্তির মতো বাক্যে ব্যবহৃত হয়ে বাক্যের অর্থ প্রকাশে সাহায্য করে।বাংলা ভাষার ব্যাকরণ, নবমদশম শ্রেণি, ২০১৬ শিক্ষাবর্ষ, জাতীয় শিক্ষাক্রম ও পাঠ্যপুস্তক বোর্ড, ঢাকা, বাংলাদেশ উপসর্গ ও অনুসর্গের মাঝে পার্থক্য রয়েছে। উপসর্গ ধাতু বা নামপ্রকৃতির সঙ্গে যুক্ত হয়ে একটি শব্দের মতো আচরণ করলেও অনুসর্গ পূর্বের পদ থেকে পৃথকভাবে আশ্রিত হিসাবে অবস্থান করে। প্রতি ও অতি  উপসর্গ দুটি ব্যতীত আর কোনো উপসর্গের স্বতন্ত্র প্রয়োগ নেই, কিন্তু সকল অনুসর্গের স্বতন্ত্র প্রয়োগ আছে। উপসর্গ ধাতু বা নামপ্রকৃতির আগে বসে সেই ধাতু বা নামপ্রকৃতির অর্থপরিবর্তন ঘটিয়ে নতুন শব্দ গঠন করে। অন্যদিকে, অনুসর্গ বিশেষ্য ও সর্বনাম পদের পরে বসে শব্দ বিভক্তির কাজ করে। প্রকারভেদ বাংলা ভাষায় অর্ধশতাধিক উপসর্গ রয়েছে। এই উপসর্গগুলোকে সাধারণত তিনটি ভাগে ভাগ করা হয়ে থাকে তৎসম (সংস্কৃত) উপসর্গ এ ধরনের উপসর্গ সংস্কৃত শব্দের আগে বসে। বাংলা ভাষায় ব্যবহৃত তৎসম উপসর্গ ২০টি, যথা অতি, অধি, অনু, অপ, অপি, অব, অভি, আ, উপ, উত্, দুর্, নি নির্নির, পরা, পরি, প্র, প্রতি, বি, সু ও সম্। খাঁটি বাংলা উপসর্গ এ ধরনের উপসর্গ বাংলা শব্দের আগে বসে। বাংলা ভাষায় ব্যবহৃত খাঁটি বাংলা উপসর্গ ২২টি, যথা অ, অঘা, অজ, অনা, আ, আড়্, আন, আব, ইতি, উন্, কদ, কু, নি, পাতি, বি, ভর, রাম,অন, স, সা, সু ও হা। উল্লেখ্য, আ, নি, বি, সু  এই চারটি উপসর্গ সংস্কৃত ও বাংলা উভয় ভাষাতে ব্যবহৃত হয়। ব্যবহারের উপর নির্ভর করে নির্ধারণ করা হয় এরা খাঁটি বাংলা না সংস্কৃত উপসর্গ। বিদেশি উপসর্গ ফারসি উপসর্গ  কম, কার, দর, না, নিম, ফি, ব, বে, বর, বদ। আরবি উপসর্গ  আম, খাস, খয়ের, গর্, বাজে, লা। উর্দু হিন্দি উপসর্গ  হর, হরেক। ইংরেজি উপসর্গ  ফুল, সাব, হাফ, হেড। উপসর্গে হাইফেন উপসর্গে কখনো কখনো হাইফেন ব্যবহার করা হয়, বিশেষ করে যখন হাইফেনবিহীন বানান অন্য কোনো শব্দের অনুরূপ হয়ে যায় অথবা যখন উপসর্গ যোগ করলে সাধিত শব্দটি ভুল ব্যাখ্যাযোগ্য, অস্পষ্ট বা একরকম আজব বলে মনে হয় (উদাহরণস্বরূপ, যেভাবে ঐ দ্বারা যেন বর্ণ না বোঝায় তাই বিভ্রান্তি এড়াতে বাক্যে ওই ব্যবহার করা হয়)। যাইহোক, শব্দগঠনে সাধারণত হাইফেনহীন পদ্ধতি ব্যবহার করা হয়, বিশেষ করে তখন, যখন সাধিত শব্দটি বিভিন্ন প্রসঙ্গে ব্যাপক ব্যবহারের মাধ্যমে তুলনামূলকভাবে পরিচিত বা জনপ্রিয় হয়ে উঠে। প্রচলিত নিয়মে হাইফেনের অভাবে কোনো সাধিত শব্দের অর্থের স্পষ্টতা বিঘ্নিত না হওয়া পর্যন্ত উপসর্গের পর হাইফেন দেওয়া হয় না। সাধারণত বিদেশি উপসর্গের ক্ষেত্রে, স্বরধ্বনি দ্বারা শুরু হওয়া শব্দ ও উপসর্গকে পৃথক করে বোঝাতে বা অর্থের ভুল উচ্চারণ রোধে উপসর্গের পর হাইফেন ব্যবহার করা যেতে পারে, যেমন ইংরেজি  (সাবিউনিট) এর পরিবর্তে  (সাবইউনিট)। কোনো শব্দ অধিক প্রচলিত হয়ে গেলে হাইফেন ব্যবহৃত হয় না, যেমন হেডমাস্টার এর পরিবর্তে হেডমাস্টার। সাধারণত বর্তমানে উপসর্গযোগে শব্দ গঠনে হাইপেন ব্যবহার করা হয় না  কিন্তু করলে কোন সমস্যা নেই  আরও দেখুন প্রত্যয় বাংলা উপসর্গের তালিকা তথ্যসূত্র বিষয়শ্রেণীবাংলা ব্যাকরণ বিষয়শ্রেণীআভিধানিক একক বিষয়শ্রেণীভাষাবিজ্ঞান পরিভাষা বিষয়শ্রেণীশব্দ (ভাষা)",
  "question": [
    "উপসর্গ কী?  ",
    "উপসর্গের কাজ কী?  ",
    "উপসর্গ ও অনুসর্গের মধ্যে পার্থক্য কী?  ",
    "উপসর্গের প্রকারভেদ কী?  ",
    "তৎসম উপসর্গ কী?  ",
    "খাঁটি বাংলা উপসর্গ কী?  ",
    "বিদেশি উপসর্গ কী?  ",
    "উপসর্গে হাইফেন ব্যবহার করা হয় কিনা?  ",
    "উপসর্গ ব্যবহারের নিয়ম কী?  "
  ],
  "answer": [
    "উপসর্গ হলো ভাষায় ব্যবহৃত কিছু অব্যয়সূচক শব্দাংশ যাদের নিজস্ব কোনো অর্থ নেই, কিন্তু অর্থের দ্যোতনা তৈরির ক্ষমতা আছে।",
    "উপসর্গ শব্দ বা শব্দমূলের শুরুতে বসে নতুন অর্থবহ শব্দ তৈরি করে, শব্দাংশের শুরুতে বসে না।",
    "উপসর্গ ও অনুসর্গের মাঝে পার্থক্য রয়েছে। উপসর্গ ধাতু বা নামপ্রকৃতির সঙ্গে যুক্ত হয়ে একটি শব্দের মতো আচরণ করলেও অনুসর্গ পূর্বের পদ থেকে পৃথকভাবে আশ্রিত হিসাবে অবস্থান করে।",
    "উপসর্গকে সাধারণত তিনটি ভাগে ভাগ করা হয়ে থাকে: তৎসম (সংস্কৃত) উপসর্গ, খাঁটি বাংলা উপসর্গ এবং বিদেশি উপসর্গ।",
    "তৎসম উপসর্গ হলো সংস্কৃত শব্দের আগে বসে এমন উপসর্গ, যেমন অতি, অধি, অনু, অপ, অপি, অব, অভি, আ, উপ, উত্, দুর্, নি নির্নির, পরা, পরি, প্র, প্রতি, বি, সু ও সম্।",
    "খাঁটি বাংলা উপসর্গ হলো বাংলা শব্দের আগে বসে এমন উপসর্গ, যেমন অ, অঘা, অজ, অনা, আ, আড়্, আন, আব, ইতি, উন্, কদ, কু, নি, পাতি, বি, ভর, রাম,অন, স, সা, সু ও হা।",
    "বিদেশি উপসর্গ হলো ফারসি, আরবি, ইংরেজি ইত্যাদি ভাষা থেকে আগত উপসর্গ, যেমন ফারসি উপসর্গ: কম, কার, দর, না, নিম, ফি, ব, বে, বর, বদ; আরবি উপসর্গ: আম, খাস, খয়ের, গর্, বাজে, লা; ইংরেজি উপসর্গ: ফুল, সাব, হাফ, হেড।",
    "উপসর্গে কখনো কখনো হাইফেন ব্যবহার করা হয়, বিশেষ করে যখন হাইফেনবিহীন বানান অন্য কোনো শব্দের অনুরূপ হয়ে যায় অথবা যখন উপসর্গ যোগ করলে সাধিত শব্দটি ভুল ব্যাখ্যাযোগ্য, অস্পষ্ট বা একরকম আজব বলে মনে হয়।",
    "প্রচলিত নিয়মে হাইফেনের অভাবে কোনো সাধিত শব্দের অর্থের স্পষ্টতা বিঘ্নিত না হওয়া পর্যন্ত উপসর্গের পর হাইফেন দেওয়া হয় না।"
  ],
  "num_entries": 9
}
```

# Models

```
The consultant will perform all modelling-related tasks. They will perform all necessary data wrangling, training, and pretraining-related tasks and they will bear all hardware-related costs. 
The modelling pipeline and methodology will be selected in the inception stage after a comparative study of SOTA technologies. 
The VPA system could be an end-to-end monolith or a combination of multiple models, the below-mentioned models could be helpful to develop the VPA system. However, the consultant should develop models to solve at least 3 problems (but not limited to):

* BanDialnt: Intent recognition from Dialogue: The system could understand the intents of the input sentence considering the rich NER 
* BanInfRet: Information Retrieval and question answering from the text (structured & unstructured): The system could retrieve necessary information from the repository (as per question and command). 
* BanResGen: Response Generation in Dialogue: The system could be able to generate a response covering intents and retrieved information. The response should be meaningful and relevant to the previous queries.
```

* BanInfRet: 
    * Embedding creation. 
    * base model: "l3cube-pune/bengali-sentence-similarity-sbert"
* BanDiaInt:
    * Intent Generation
    * base model: llama3.3 (Instruct 70B)
    * temperature: 0.2
    * context: 4096 
* BangResGen:
    * Response and Answer Generation     
    * base model: llama3.3 (Instruct 70B)
    * temperature: 0.1
    * context: 128k

**[DemoLink](http://103.180.245.115:3030/)** 

## VPA Overall System Integration

```mermaid
flowchart TD
    A[User Query input_str] --> B{Is Context Available}

    B -- Yes --> C[Format Prompt with Context and History]
    C --> D[Invoke LLM Stream]
    D --> E{LLM says NOT_SURE_ANSWER}

    E -- No --> F[Update Context with Question Answer Passage]
    F --> G[Return Answer to User]

    E -- Yes --> H[Clear Context and Passage]
    H --> I[Go to Retrieval Step]

    B -- No --> I

    I --> J[Retrieve Top K Passages]
    J --> K{Any Passages Found}

    K -- No --> L[Respond Sorry No Answer Found]

    K -- Yes --> M[Format Prompt with Retrieved Passages]
    M --> N[Invoke LLM Stream Again]
    N --> O{LLM says NOT_SURE_ANSWER}

    O -- Yes --> P[Respond Sorry No Answer Found]
    O -- No --> Q[Update Context with New Question Answer Context]
    Q --> R[Return Final Answer]

```


**[DemoLink](http://103.180.245.115:3032/)** 

================================================================================
--- File: d23/BanResGenchart.png ---
================================================================================

PNG

   
IHDR       r7m   sRGB     IDATx^y`E	 $XD[L8Dx_ 
^U[#(bAP#@$@ގ3;	9&S=S~Sou?5#<l$@$@$@$@$@uQ<Ɉ$@$@$@$@$xf  	 	 	 	"gEP4#    g 	 	 	 	 	(xVE3    xf 	 	 	 	"gEP4#    gƀFGu$JJv?w1r0$0<m[nCSz5 C/9Ïa̫9ih׮-lيSN ε1O?s>9/?ed`ʔϜM믿1!W^B=s>~((ԨΟ'[g <xbG|^믿_`ܹx'QTT9wôYO=+iN?498,?p߿[lĲe=:g@vp
ѽ{Wn;vO?øqcG+@Vf&Fc=-Z֭[^w}ö3PxNLLĵïFv o(ʰ >cƼxf8jϡ~\p@'fIvD.&OŽ=B_
D`<pѧ52!K
@C<kǿjgggw3(/94i ;UW^5s|P<gwy'U;|3îF sd8{P|
g1bbp	ǣoqQG9͝o9
z9.?^{
}aY?c )Yb%'7F^9k^yp4$ҋEN3t:D\pyž}5E!?k#qҤ={L?$(u]ocfZHMMŠAѢECo>sO9
]y
4?Oχ={{:{p!99WĬYݻw|;7߹_ O?,;qGa#| вs=#[g۶mX~~Q|ౝ:?_vWqmڴ1vQO^;y?qÏo:g9 W]u?8lg;xw*Sz`pܱ@Noߎ%K'vgU3Y3z`sN̘ Ci{[nvjNi/W{M7MCдiS,Z&}>ȩ3E]qq	y=چ>x!Y:uD˖-QZZ-["/o:IgI?ҟ;;oC>}_wx'v_?
g}&2ӱr裏;9cD2BwgtRgt޹=?!٢Ц:_/HQط <ڵz]<clKV&.z?8mv5k0%w^MG6)t,yEǟTl8k_zw6UV駟3flbȥYW]L<p^y%|c>MϘƫh֬~޼7psp,>O\tqJpw;9+7-9@VS1l޼9fD|||u[cDzmHHHUW^$*ڷoz^KP:@W<Q] Z?\%7UW>\j㦛oqK| <25g 3c!Px'An^<\-BW_vnr3Yn222ܰ"믦;BUĎ؋ ϷV9ڻw/nNCڑG>ӧ OWs4n@47G+k̙_;N^ϿL;7=i"9(}+>^O8	Dw=վ6q9pАɥvī1qnε~^|e<S՚tS7*Y@~u᡺&bˮ
gyy0&7#y	mtwaڧ`0OR]};i%%%8Csp?]=_c{ #yV'e
h̦pj۶m OU>#ɃߣU1I]?"
$IQ|&9߼y
Y?SLugjY-P)|<d[e}}ѫw~_zP'D(xSsvm[P%/W^duxιۃ	roLJg=OZ]>T>?o
 sXӪK<$;Y,~cѹsJSO=c^y}OY+^(JQjC6οKl;C]wcA܎:m@9h/B_%Bwa[6LE~dL?z G{_ݍwpJ
7x5wɿOnҤ.t;>gdp=)#~5'9zwWc5WC)L\oѱɺNc%!$99wӍ;.D(*W^^'[._R<󟏢`&g^R}%Ob/o:_;a(F"ؤ::Fy!㕘N<ymk73={qD|twa1uy?EV-1?㐇?(KKFS -Ч|((x-Zl_xe˗㮻uNu27tCɀO<yX-i7>COTyzey#2jtU\k 
i_G~y~rOg<#tC~6gV:kԼscypVph\|хAWՉ磎:Guv\a鬳?ΘhpP<7יT,NfsWK,)RBDP"ڵ?mBŎܸjϒׯc*Y~[Tڼi}M#xOoq[.4ʇ$$zW8-Yx++s2$xټ?uڧ^7xƾVP,W^yW&q_6ltj[*^MK~A[۔W}<wQmF^Yv<1*M>CTj=7%y΍G^<H}JB97i_O<=4$@pj::W_{:أχ	ѻhby !S6mrDld7^j_BW:+txQV./XW>ғ)ۑ8SMs]YF)6yq_+WdB/%\UyKIJ)E_☄^Oƽ>^_~9-8Xs_Yś6FLͫN:.IIHM<r4g8[J H@P<3mK%tDI7Ф.YJ4B7jkρrvRvZc("tI_I<WWڏ|q/_KرBb
Iv@2'wǡE-
ܱLFw1d/N͵#F9u̕4q^]>?Ɋn$u"&eC.1ڿ>y+Ck:Sphq;XO:wqnTDK͔fIĆw $bJNWN<KMSOR_^KM{-dCK$Dl-Ih|ŉ*dCʂҗts_ɗ1Hf)&oQM&zY.6B}gƔx 8=
#]qEɃx*w`RٕJ>tVG<o5br^U9wysW}	ƽ
نPZ]9Ч\%yS1Ѷxn8sY3K<KÔ)ȣxCNm^UnJ[/C[MyႹ WY+#oWeUbOWyU	-)Ye&		J9Iu&\uoh# Ԥ<U]%5C]o Ml#4BKt`Mڅ
r>z
L{h߇c+ԅēlxj녖dySn3_cGM^1r|M2&GM%sVV[xCj_OQIm{Kĝʇ5ňxW>p=٫N<gYu,RԻW/`
d%S!P7Fp>ܔA|XV2.)R@2 +Rj0ԟ|Oq
7ǺxY#6s%vrJuyjy=O|Y]@gLm8(\LyC*^7O>Y:VK$~;"V<kry].MĉĆ^cqF{<熺ŵfUsh6=l%uu!(JA)OǗ_tvdQGCcHh7dAdK%+nZ<WyČ+;sXT櫦1έX:gyTB,TqdD~Ns`)-NV54gZ<)bY|p
Q)Ȑ&k֘U9./vf;rƏ&@̩|@)AIAhiqF_g$AihVϦi:I>u;:/ Cif͜@2y$;C< 3cB؄`u[UI>=T5+C?H5	}Փm"!B-n_tGcĊi}2Q-ܪe*XADJݳ4`&R{-M^K@jjB'6l|)5r#Vd@j !u4Y|JIH4NR*!WYVBB<֭[Lv^jM<4"yE<Ǜ]@Iӧ|<7RO$,MJ:$Gl}(M!o	-a;V>GgA?
RUvS3O9&9{܉s	ꖕU׾x]#&}\z5۴]*joMy7_;;HuyG$'m`#gƀC ,OL|1@ī\>tSC+7A\Z_j<˗+q8pm:;H-y_ܠ"!C
QJCF_=xkWpާ3n4oՕm{ďWyW~UǛJ	9NJv#8ɍukx'
W2 ui P ?#{aK-$$?ݎ>̓:;1kni n篾yx}{7O>?v=Y+U+A|	a^RɄR4tUX~C4";8oZMl'MW<ۆdQ7`%Cg^g?r)[	3|fBdᗱHlHmd/4(}|'!{muL沦&3qɛ%|Y5JB:%&dkG)_2@-a%gG5U=Ot)#BnYwW>unס?z" I$Z@<?ģ8
a,5~]|AG͗k	P<3g9N^K˅<LvD_սI<K?-qj<!ˑR-[nG.䃽~1$e=H~=QlCY(o,/zQԫu_W֬Y:8d"EZֺ!7{PÆ4)ːU=NvθO3fųsoKPX,Ewg`кbYje"N>p8.8ةݗ&|vDI
	^[}Cw2o_x-gwi}3yS]<t՘z@:,o[[U	ofUGz_x?/,<uWˇ"  7ĳ隦Sg1{ۏe]nysN7W5t?	P<3g9VDQ#=R"d͛1^d}ĳ#]V__zVt2ъǟt&Q2
G<Kr.#O<BX)3u`m"*t|ڟ~/g:e%;==$E`ȯIFJj"ٜSO;%%\uqųp8*;6>3NHW8YdYLn2 w
`K`LR"GX~=/Z_y5`<)%{*o}дi|"gaDٓLl'xSpQ8ep/E&k]v_E䕾(#^/@ֶBwW~_Ykr}ݫS'IhongS4ۧxf?FGI$uak	Y׍gyky
)\LIAaA,OHH3(=3	gZ% 0EI!(fg$`|,s8eDl$g/	(]K$@$@5xfh 	 	 	 	"gEP4#    g 	 	 	 	 	(xVE3    xf 	 	 	 	"gEP4#    g 	 	 	 	 	(xVE3    xf 	 	 	 	"gEP4#    g 	 	 	 	 	(xVE3    xf 	 	 	 	"gEP4#    g 	 	 	 	 	(xVE3    x|bܹXd3{Bb=yqHHHHH "&de"MGjJ
srW:FB|f
-[@>>@QQQٰ?    D b!?{6֭[_>p7o~]}"hw_~ܹrHHHHH*Q#ûcǢ9] --
S*~՘~Z}ػw/rUPHHHH""cbbp
c{nzΝ&L4}|11p衇"{:-E$@$@$@$@V	DD<%qnݺE4L[	@jj*~Zn?عs'fʷ
 	 	 	 	@󰡗;5ϫVvgg@II	g:^x>˖/ڵj>_Lٵ@ )1;w3${{1@dou㾸)[uѝe
d Yhۮ-&Nَna    IDAT~?zWDaa!N?7%`ƌH;8
gxeeeg^e{#)`<<@dOY7gomD+oewPGF bY!Zj~'\Pۈ1mj֬]ڶuDX_TYy;.hLU=*)vdoGW%eN@7þM?},jH=겏EWϑ"J)MqOY7=#y4
g2^9
݋WAf{==ųeogzόrFZQZ^Qc]73=Q<36T!+2nHƑ*;${eT
uS<]z'gƀq\Ƒ*;${eT
8Redʸ.{ψg~0h<Cm4w|u/.Q'{{Mdou33sH+n {{Hdou\73=	gƁQ\Fqj9#{-\F(N-gd˨.{gsuٛ뙞(	pAGQ7${H2*ㆺ)M.{s=3c8.hH2*doCWFePg3?4!6ی|SӨܓ&{G^ϺqM⹁LWNCbq7qY${G^ϺqOlntٛ뙞 3(.h8.do3eX=ų9LOό#UvHʨq^qC]^obM8n3KeFyVFEC\*ܱ!{wx%{Jؐ;\Ux*Ͷ_`8guLlg{1m {05]&0do+]U>ܪ~O9Y)tųWEݳ%{w{==ssEl/9j<FS4GͱDٓ9tS<S<ƘW):3Q:.݋i'M{=ƽgs55,۰G3s4ΚǬ{1Dޔ=#`gݸxfͳi =GL<|>dgGFzJKK1g\,Y 			8hղ˱h",ZDw|\ݫuн^ ۋ'{{5kq8=GL<wDa˛Ԕ/4n{ӦMqeO?Æ
ϏYqC݋4bdooɞ׳nS<S<ۋV=GL<_:d0gƺu3ׯ#͛_.d ~G,]. ׯǼϜYqC݋4bdooɞ׳nS<S<ۋV=GL<y;%tiii͝RNq2: LdEty.6(9ų2*ㆺSh=#`gݸxo^ݻw;:w0ab%R<HMMu0cWZ)p5ֽ;#{{@dou3sDĳ=J25ݺuEi<9ʳv~D?c[j	/)!7@)jN#)1;w튚6${G^ϺqKHEY|A'B쎟.?8V"-nғ*{6mͱ؟WWQ^S)x6ryժಳ+
V;&|	6m)@JJ"3dYyf";${{Odou㞙犻m؋pzxDvm1q$$'7Ǡ3#~8;XBd~T#66.֬^`8ct/_C.ٓ=z֍{gg{j爉ghusA&lFiSfZz={0kVjcYy;݋4^dooɞ׳n{F<A{A@zx3H>݋6ޜ=#`gݸxL@&+{1ʸ8,=#`gݸxyXl/9j<FS4GͱDٓ9tS<S<ƘW):3Q:.݋i'M{==3ųh53ųYޛŴ3
tEAt{F<A͙yUό	t/F;o^ =#`gݸxL@&+{1ʸ8,=#`gݸxy8gp	^L7R3gOXzeOLc^xDt/Qz6ۛ'{{Ϭy4
g2^9
݋WAf{==33lfy6zoF  7Sk3hNaje5gU	P<3&н;#{{@dou♙g{jgg<7݋if 54'{05]鲧xx1ϚS<{vjs`<Koٓ=z֍{ψg~0h/hH
d"rS!"ٓ=z֍{gfEٞ)lt/A do+fМ
t3j4ͫxfL%{15y#wF{==33lfy6zoF  7Sk3hNajeOLb5xDt/y5ۛ'{{`^4)Dz4t/^wCۛE'{{Ϭy4
爉gχH@ii)̝%KW#11ߗ-[)S)#Þ{ih\Db{==ss5kS<l3K&TS)x27))۷>Ɂ_\`/ZЪYy;.hLU=*)vdoGW%eN=3ų(1b!?{6֭[i~}<oŉ'vĸqkѡxeXbjF ٓ=z֍{gg{j爉Q#ûcǢ9] --
S=#)a:57n:kg-\Fu/F;o^ =#`gݸxo^ݻw;:w0ab(?8u	ڨ);@bjc:'{{`^4#"Qyq֭+ZHɹբZ%KbVZ狩q
EEHIMm S]vEנhD=#`gݸq%,^87,pbwuAeDZ>'U$lۚc?	nԽ.(QU賮>l?Hx6ryժ9dg@II	gpN4{UWW^Ş={:gmdD똎y{{>z3\\ۆ>GeS Y䭻˂<"&23Ѷ][L8	1h@~=N+Vaջ'|m	5xx#31,'d6#a9!9Hgs#yvsX܇dZl^Qxڞ_-λY@Zg8lr1Ӧaڵwve#0at/F: '{{5UD2N'<GryS<Gvt/Fz&{{sJþ
6_ ߖUKЍ{ψF 35G3.P(u׽뙞^}k_hhųm>glw^)UI0T#٫2oG晪zed1MT0nox6NU!3kF
3ϿxVa{#sc'ۛyJ7=S'gYY'^L޸'aOlnt3e@Ǔųg[_[x݋i=Ȫ^L}x67WqO\3{̭0=Q<	aS]o^(}x67WqOL}Y
sW0<^L肇@G{gssϬy6:XC+<GyG݋6ޜ}x67Wq̚gsAYVm)#Ϝs'{@WQ<+],j?dن+ct/atCX`ۛ]J=3ųW3k)]{1
B`ۛ]Jg3k'mЊ-6"Ϝ='`o\鲧xf٫gm0l'݋i]f={{S˞\x@3:"os3i9ٓ}F+(͕.{gfy6{x6T١T1
$@u"r̀]C[c]u"U6eOLL񬼼"oHy~cNdo
8gss3O,Сy[3ǜ>{#pJ=33^<s:~0h'݋i] aOlnt3O<Њ-3ϑg=dO	x67W)yjٳ[|>dgGFzJKK1g\,YH%;o)p5ֽ;#{{@Þ\=33ų뒕27))۷>Ɂ_\Sѯ_L2
{KsƧNƖ-[{xVFePbj| !ۛ|J7=# 
<_:d0gƺu"E8ϛ7;Byt-s}ŴA ^oSOI%ɘ9^ݸxfͳ7xlƨݱcQ\\G׮])GDvaa!qqqXb%f̘+#ų2*ㆺSh}x^yFl5H =1~*ųY)%J^sMcQ~p%Q<a7K/ݻ;w0ab%gW]uv܉~}bX]H(Ǥ6wcE"죇sƩ'LS_QԔ4TE{I7[--=J25ݺuEi<9_ycٲ{xLkժ%|EEHIM5O$viG<SUdJʼ.=o&1vCQڬ3_)7[3O,kRr~k!ؗx 4
W!vH^<򹬕Hۇ䞍6w[s,'m}yL,6W4M⍈+Z{ r 3m7s?'"k2{wđǖB͞
ܩy^jc= %%%ϟ]/<WEq6a2ʸ!3pƑ*;${eT
8Reym<AeԐe823Ѷ][L8	1h@~=N+V:O8't<~8)2Gt2*=S<PrFF(W7d=))}`΂xhusA&ш1mj֬]ݫ':(/+ʕŌ_iuH˨1EQZ^Qc7S˙.{ggg%fٳ5nFnYGcKf죇=33ųa٭"9Uϑ]/G^όaOLcͳKljϪQDgUI#{LU=겧xxxV]]<,Ypkӽii^}xxfنJl	bϊ\0pKWٻ Uѥ.{ggge5<w-72-4 ϖ+ų%23*${EP.P]겧xxfͳ2ma̚gfM?s3`ۛ]ϖ֫36(-nuodV:'{{JÞe+ų%ݲlCf.@UtI\0#{*eOLL񬸸LyX<lgOF,h0M.{gggKxnyV備̅!4Zdoo>zS<S<zxf33ϖVEE=#`gݸxxfzx^[fA`{#sa%ۛzϖ֫3ky*y
Y)eOLLlizX<lҪx
*~]gu
s]ϖ.ϖ+v˲
EP.\BuI죇=33?^)-WYf.@UtI\0#{*eOL̳2ma̚gmw-72-4 ϖ֫3k)-n)"'{G^ϺqOLLlizX<{6(--ŜsdfOÉO@yYʸ C7d=)))-Wg}]2qX#5%}99+90?oߎooGP<RDa>lt>0l))`˭~zX<{l!?{6֭[ׯ#͛_i2=[-{(FWuod^0}xxfzx?juxwX8wiii͝RE^&M y2bb|Xr%fZCg-\F)"rFZQZtS<S<S<k-1sϞy7K/ݻܹ;`&3-@˖-лw/̝-.Ay)Q7Խ@#vH&죇=33ųaJF>;erv5֭I=J25ݺuEi<9YTguV-x)qwv5nΞ-]fvPkbm̱; euefҦHgm\kv&epAJceKHEY|7
W!vH^t\JCzRErF96J݇þ<&{WQ|F=}y9Cӛ95Y;ckWxf.NBŶm><aC/wjWZ= %%%ϟ];;rHجL3ʨ2gCWFeܐ#Uv˞Ѯ(jbx%@oJkÙgeӿCr8HHH?D*Dvm1q$$'7Ǡ3#~8;XBȇlقoX\|=,[\/geT
u/Ј'aOL̲
K뵱>}89>>O}k̀husA&#c<YgQc劕~(p50Sk2jLFqj9eOLL{X<y~1X~=Jܿ0pŰ_wWsP
zx6SӕL=k!@죇=33ųژĳ|w)=NF֭ldzYڻx7-doÞzx6ZܤIgӿD:n)f(Aq3qٛ]᭵zallc޷1 chifJFfo
g7d=)))-$sYX;/+D<liXImRv5q%aOLLlizX<-9W#偃XgOx:-p}xxx^x^ݲl^u0ED`zLWuS<S<GR?xͳq۟`    IDATOVZ_~AoS¶mެx?{ {{@Þ♙gK1=$/;wC'?niXI,۰:-p}xxx^=,<ME3 -?AKPc<ۛG^όaOLLlizX</r4uy_: L1Ns`ٓy:-p}xxx^x~GqbZ^U8зow߃fٓ)N=d=))ژsZZnfwܱ(WucƗ3g-ѯ[m-fٻEn)))^WXxX<y/	LĞ={Ю];deO>
[N)MPχ,^xTu^\}xxx^=,<unz>51wǾx-ųi {{̸ϖkc_1SSS1qJ/K3@Iy:-p}xxx^xΟ5g mᔉ|SNmi(=	POÞzmL޿ߍnݺbZ~믹ۆrf{==33ųژĳ ЫgOis6`振0,ѯ[<-[h~ެ}xxx^x`I'uƢEk.KnFnYPDEnd_7#,-ueOLL\rVu~8|^CX|9zZjBv$,Y!;?23PZZ9sy`ׯbDm(Ÿ C7d=)))-Wgc[սkX<(++{30v8|Ӎ8ʫ+@LdHMIq~Üܹ޺uų2e{==33ų9pjӝ,qbb"FA&tA"/ݺ<ٳ_(֯_G8ϛ7 ͛7CxVlPFf
gdoo.>zS<S<S<[ZE<gρqr30juxwXWly׵kON9ǹ眍5k:m(cnHao
Ȟ׳nS<S<S<[ZjGPXd	^|(ش)5].Yp(@LLna4^zyv#e0aJ>D,w:#>x?8g%1ҽyg?7d=)))-Wgc5^x>n(Q6_|8 O=/~w`-?2yrnGll,#	Lt~&ܪUK|15/*BJjhh@Rb"vzx'sg=OdooN>zoy;|e(56؝[:2FiXiS65;ĲX 1֎C/!eJ0J(\?#y]~sY+I=mXO«(uXm_iFW @fMo,NDdeҳ#-iRVѩӉXjUVzȣ;ڮn˝UV;egpr~`4oLm(2nq^qC7T١.{fkF)9kʓZ [M?g5o̳
$cݶ][~999
ݱ|JgƲ
ӳ?݋#j<=&aOLLlizX<+phu!mĈ65/`og7f];ٓ=z֍{gggKxnY3Ch.ԓ}xxx^)-WYf.@UtI\0#{*eOLL񬸸LyX<{9+{#7҆3ۛSϖ֫ųk#5]ϑ"}`?doÞzx^[gEP.QD U%+r]R=33ų2mFlYfyxӽm^}xxx^)-WYf.@UtI\0#{*eOLL񬸸LQ<&jųY:tod:i̳WcqooftS<S<S<[Zϖ+vK3Ch.ԓ}xxx^=,U gK E#`g}xxx^=,UųUQ-E=dOY7)))-Wgf)-
gb:ֹ.{gggKgKeن"(tod.Ѻ${{SOÞzx^[gEP.QD U%+r]R=33ų2mFlYfyxӽm^}xxx^)-WYf.@UtI\0#{*eOLL񬸸LQ<&jųY:tod:i̳WcqooftS<S<S<[Zϖ+vK3Ch.ԓ}xxx^)-WYf.@UtI\0#{*eOLL񬸸LQ<&jųY:tod:i[;!d=)))-Wg}>R̙;K,=QiT9
v}=-Z!ų.Fqj9#{-\F(N-g)))9cg}]2qX#5%}99+9Q\\sEs>ƂN)Q7Խ@#vH&죇=33ųJ!?{6֭[ܯ_G8ϛ7c9?EEE/"Z
HSgeT
)"#UvHʨquS<S<S<+//<Gduiii͝R8ֵ>p<~rʨȌ;${{OÞzxo^ݻw;w	0aɍh5Way9f5zZSDOdouzx?zdku
8p3G{8d|,,_"hתUK|15_TTAzHJL]	>3SG)~toy;|e(5; euefҦHg}7kv&epAJq%,>YɿF	g$.k%!="gIxuua_+
B(x#Lۡ͜ŉߚ^zqPaC/wjWZeg@II	gW:H2b#&޽)S*eʨf;${{OÞjEQS??s6+Mn5
I-D{-ߦ̳7bYiVf&ڶk'!99
9=N+V"6`9ؿ?.8<X~)ų2*ㆺSh}xxx^)/Z2bq.(81b8MÚkѶm͛dXfJ*ܱpWW
ٻUū.{ggg
ųP
x6SӕL=k!@죇=33ųJl	bϊ\0pKWٻ Uѥ.{gggeڌ4Q({#M	!aOLLliR<[-ų"(("\A`F.@Ut˞Yqq6x6MԬ?g<ut|Ӗg 貧xxx^)-WYf72h]'aOLLliR<[-ų"(("\A`F.@Ut˞Yqq6x6MԬ?g<ut|Ӷvdo/B>zS<S<S<[Zϖ+vK3*${EP.P]겧xxxV\\(M5,Oo72ߴe٫173)))-WgKxV備̅!4Zdoo>zS<S<S<[Zϖ+vK3*${EP.P]겧xxxV\\(M5,Oo72ߴ ۋϖ+ų%R<+r".^fdTE)))i3gDx6SǛL7myj0͌.{gggKxn)A`{#sa%ۛzϖ+ų%R<+r".^fdTE)))i3gDx6SǛL7mk'@"죇=33ųJl	bϊ\0pKWٻ Uѥ.{gggeڌ4Q({#M[f{{3˞zx(--ŜsdƢw8#{n|"|ZR<k2j{#3y#wF죇=33ųJKV&2@^to>0'~q%g~5K_#11s6rLņ
;xVFeܐ"8Redʸ!GP=33ų2kH!?{6֭[ܯ_G8ϛ7#<EEEزe.8o;xVFePFf| !ۛ|ϖ+ų>Q#ûcǢ9k.HKKCn5mW^1ʝR<+2n#"[x:mY9Ng@˞onuS<S<S<[Z(p!&&70/<Ʃcֹs'w	u&g;v/kuH˨΍Lľ5X>oP/Kncehn0G!biV%>&4Y4mDWъ~*lWr~_Q(Oj$*wȷP<AV'ų*&GIy\ƹ[h"
',.._t!l݂g`VZ狩q"[tIعk	Ii#c_Jn5]d17Gԧt~cOkq%H^;RhmgO<܂y:c!MTٙ')A֎C/!eJ0J(\?#y]~sY+I=mXO«(uXm_iFW @fMo,NDdeҳ#-77=
zSjj(;{ JJJ?AM4矇+K54ff Ld:_3w8N|
Qs8xLNANrc0l/sKQg}Yhۮ-&N4p rߏ'w+QXXOr̘~'A6z#"(덻f{7]b&,,ۨvXa/?kS<l3KԲq/%3h)mq1^}D3ҙhusA&FiSfZj$%%V:e˖ciW[geT
u4ųY:Lo)LX))M
2xj'S<[xbUvN\oa;йQ<u؛tS<S<S<S<[(UIٱx]zչQ<'f{7]b&,'gURv(px]ݑ6p1A{mA~0Xuy+k#E9RGGD0lvt؛t3l.fb33<[yV%eǎw'ųyf{otS<S<S<S<[(UIٱxÝw]vGzKY
mT
AnU6\,Q<)0l:"1W{{=ӛ.{fLXyf晙gsPų*);vvf?*c`lWPvL:	x=33s5ƑuHl3]
Sw)a씆Jt{U<l#*[F{|xda
TbOLL/sHCˬ΍"jͿ7)q)͆kt^e$Eǋc)Y6cͳk>Vu׳?gmd5cx6qϵO	luP<S<|<)0l:"9@؁:qOLIᏤT]<3lFlo
tD3ųH5۳NS<S<S<S<S<Wxx6{W
s#"(gaf~n|S?O։{gggggg{wzx7%:"ٮx.x>"mCuǜgs,u=g<x6Tau0([n`M?}BxI@fqx2?t=xv@6Jۆ|P</g晙gfyf晙g3w^(
t#"yfY03I}7Wxvp-)ͰgHyщ{fyxxxxIgeT
uD3<3l|	*;d晙Iaͳ$C<S|bܹXdiڶm3aL:M3gmdx6fރXO/,IV<333<3t钕27))۷>Ɂ_\=PXXY	gtD3<W<x$?XSSNS<S<S<S<S<S<+`.2gcݺ}~}<oJǷht
͚Q<+xxI<wL+v[q:7#[upoY~ر0ֵk!7wJz<Y#(LAc.ې3ų8kY\5>X̚gW<zxcbbp
c{nΝ;!=&LHf♙g3Tgg
6x=zdku
)ј<9W[<j>_5"ګҧpHJL]NhOJ{Pwèb%$׏ݟt8,AF+.m}8uIIeXGm1u;enli'J
v&tfP[JbЉ {_y}s-HYw {iS~ٙ'w֎C/!e*QB*G=iqn?w[s,'mF7}yL,6W4M⍈+Z{ r 3m7s?'"k2{wđǖB͞
ܩy^jc= %%%ϟ]A,ۈĬg3LyfL$셙g{Kn5Uax1񜕙bIHNnA"gGqrw,_2Æų۷NwS<JLl&((1`C\
kegmǂ-qN^+(i,x*,-bq.(È1mj֬]1]DXaaumf:m̑U33s<}?k#:n(g0gP<S<$fyV$fyV0,)ÀC(#JWfS<S<$ggHxxV0,)ÀC(#ϮVSmljQ<S<zݡxvoS<ay]))D3<G3ųzaIBA<ٕbmȢxxvC*z;ew4Y6Ք+k~y6L$1̳z$Q<S<GKa@!YL|au]W/XCx?C@Ll&()#Y=Z°xZ	G<Wh;WT JloW!;?ܽܓUR]H))^`V1g{|H^/y5/F
s$iWg3))Q΀c    IDATD3<G3ųzas"xsaW{gg3DLI%s"xsaS<ٕ
qʲ
ml o)#:n(g0f
@Fl&yfY=yfY=Z°xZx lf]3ų+3y%()]P<)0l<ҁf"gf#Y=Z°xZx lf]MLJ`133m߀%cͳWS<GtP<	a<Țg<	ZA~0X5@yfKųxv@6:m $?ų0x@ŏˀg
<G#]P<GtP<	a 2Ll&()"gfFטYXd)#;7g3Y̚g3Tm033n_g*x(n(;Jgg3D\ 6j	fyvC^χH@ii)̝%KjuH˨1ųf"Y=()գ%K0)%+e /o:RSRзo|X@ʸ!ųf"Y=()գ%K0)rȟ=֭wׯ#͛agu+mxP<+ՐL$Q<S<G3ųzaI4CF爮] --
S=4|	־]k̞Yobdď~0	3ųʵ#lz`LLna4^zyvv	0aDNBنZE6eWųr3d晙gsT'gg
o)=zdku-0yrnVZ狩qׯC֭{pm_dQYB*.B⚙ASwaQQ]*lӬ?/T<+9ͨ45wE_ts6}V.L/Ě	ԇrWns"r I!q/cXWSu-ݎ##ރ<6q,GlOp/ػkq@Ra]!rͷʚ"J_աvn*։kMK6p=>lu;h{k|{Zֱ>,ؖwđ3m˝UV;cVN進S* {%L+X&WJN^	+Fd
VeYhۮ-&N4p r_=CƠRdoCWFeܐ#UvHʨq^+r"_w2NA˨1ũp5&{8.do3eؓ⹾gɠ/'H/'H/'H/O\?~<
.h{!AdodoKv7޹M)ٓ=zfܓ=zfcOl}޴=#`g=#`gƽ=v7޹M+ٓ=zfܓ=zfc`ų]HHHHh5
ux^$@$@$@$@v	P<IHHH sMJ$@$@$@$` ų]HHHH P<Gdqf	$'7Ǟ={gN[Ff	شg
HKMC\V_W<^@CdO4o9h><+WaTE\\?"w؁dL
6D{'v<|>(}ӦMѡC?ڴmVZaؼy3cQt69Ν;U˖z,sYy2Q6̞p02%%۶*yn"
{,JJJdR)G	Hk֬~۷G^_GqI	k4H &&iicmx풕t|'NII)Gx]w`Q]Q4({5Q1hLm7E5v1X;;ԙQ0ɓw==_;u֑%JEPy
U떲eVpᢜ;wNmۈK.>YeK
dqww-#Ypv>#xg; *URi5X\s?__#j:bҥG{B
.,"vɖRdɒ:Y-]J*T /|r/%Ť}

i\?ҟj׮%EulaK2~ęVS߾-0mRq,
}ZϘ)׮]75((P*T(ϱ'!Vޒ޽{ԩ֭[+[T1}ΝL=3x΀bBr$22[5b]<([n5kHdƌY꾮Tԭ[[_h64l@y^tIΝ=ԩ(O@W&LQ]ln׮ܼqCݡgϝSN7,VMeq{na5j,\HΝ;O6Z
b#nyfmv9xܽ{W_ڶ+Wɉ'%x KG+TtNNecX9c:u 'O	ӧDFe@mYS<P)r"zjլiAưpuewI~}\xI
3Çɔ)tv̖?JJ<y.qq*L^2fl<3BB-36,;IɈ3lL@&Nds
nذYP|c_]v|-ζU	OOx{Kl\3Bz8p0a^şe%vxzxH^ww
[|eBh^:|rrUٶmOiJeӦzݻ˗;Wɒ%R=%9Ξ9u+cǎٲes>+;vTÆaj	Ekؠ.*K,mmv;sD"OxeDG"EխˀۿOb8///|߼Jd=&]v־ڹ3Pxe]h,gp=W$6|Yz4v*Sc_OիZ=)\X>Z
+sV%Jh0D:-۴q,xG=*1%d9.xoqc~#1uvsa Ɯ"E
$>.>N.{tL֪YSCdڴi%7o2gVre!~ZP6` D=
o?;={gT-_S*HzUt@߯޲lYZy7^ZAkt$qb$ǩۙ[V-U֬]#F3ɦ@=^$޻'_K11w>z.]$QO'fڳ)X?f gꕫ&Mjt,yUR
СCK}:x Yb\pA+o@lG̏~2=%[YjF
eҾr)V˘hɑȣ*!0n>T~iťlzfyF)K.P?ӽ{WM\۷S?<$ucl8~츆ct<X3d`Q@{>r_`B+#ݺw5X !'O &+W;56|arIY~`) b=I` 7,ou:zZu
!n
!͛5!oV-0,=efl."39K57{!SHRԬQCfV;u	&9ϟ_?zZ P/5I PJyTyd`cX'NOL$,zQx
"ǧxyz}N.ZĪ#L ᭹xN\5iaOq]!*0фwxWA.yBW$/^\'g@ 	Uabbbt2GEm[S%JTiq00<t)׮]cpMxw֭[o{h}C'+W^	&0n"kV1G?}OիW[ָq#Y4T'GF\i3U	FN)UTp#|cnPxz*G?&EbÇnOhX@Ŋ5
(6sEݻ/}Mt0_<B۾j*;$$$ʾ}Õ+WZ7ÃݺvM"a[D58k	.Zk۬9=n{/\߷lMr?AzRVM?OOOهukNVWq@(jߢt>x			)o޼*0Az!3gԺ
샃}b |2h?ah+Q&ƞ= ѷ݄	
] c
}	TЫ~W`S~[HKQmxv!1!W.C &@_)} ƫM|15+DK)RL;;{xqrXjJ˕WmB_ǎ%
*fDU5WCAx7Dp	Aņ˨Z {)5:pC8_A X`!*4y:ip #\-\lr"XI|11N4Y_x]ђuHqFר%iM8Q%523#˂c~[EBnqxa.ZT"jq3 [c %m(ǏO˝MV_qL40~V0.[ZU`(ᛝ.!0S
`X@9s$ 5 
9|aȪYr7 <@ǒg&E'itZ W;<C{V>h 5@<'nJ&OJ ghoBZmK0D53Bre
0Hb0|V@3Æ1m@
y:X/Y/'DGbU (,k /&*`kXgtVJ$'4mX?4+knYhqK΋	;x1B~`e> hg!YE
kzrN|7˕ӏAj7=ڵPd!NݘA4J3V4EzI>j2qVauY!s?S o&
<z7įSr5|x=J*I5`|F9KsIŰ	KtkJ5>+O!aMֲkn;Ҽxd''O50ܬyS1`T'\Tj'5º*?y1ܽ,=zt$U`[4NXx#
aΞU0Lb>[ae
3Q#oA,BG hTT
0^r}(f, ,s86)˹q'Xr%?"<Us{ ꪃK.T0
QLA\06HxP! C:GDꀊISA//p o
{	Ub<'X%\})[%b<"ӻsaRĜ&OUwg߾U`Q
4)|fS%AG}`
{L194bdOG${%._NO|-5@RjM^EB+Wwf\?)Y#BðP.\k4gib).>yZj`,o)ٖb-Mk}s̟3U7-MF1.Ip͟B]HԞ5kj0\!!7$3D6Yw"ǋgW^*̈Pfذ!2y2$ *	12\H4;e	1s9CVo*I&jY,B,z&_⣅V3/
gooC>Rs+ܨ
qsZ\ݻw3eWO(ϗZ-׳Q7KpĄD1qIbgFMj{#'G{s*x
#*`7	ZU'H
s?<KQ2OWEYΩPwVĆlu׼<-`4L±P0cy`q*8G{rިvE9}LꈯYVlk4{#cG8*xv~r̿RxYljrEr j#KL?aƔf2,qȘ1xyYD7jp6kTb5T aAyaŵXyg~Dc(X&u=k"2eG+11Q?zSIߚ]3kC$kZkd\]pa1wGAeΐnў.Ļ#z䫠jkwUzP6.K4ʅLea$͛7?E<O0ị9WFb+O57Vۊ0pN8RspֆäcZr2tKJ_uRH2OM@P
KɳWh)$O& (k߷K½+$jBEƌ`kjCpkǇn>>jŬ-yQ|g}Sop;#^
!^X
.^6mDc-HJx_ ֈ
i8jI*
!͖: "T.`d[GAa77b=ZEA`rj;[K#U$o1(sVdN<qZG'y!rވ[o,(0z(0PߴiSq雚]WE{Рᇰ
L,^&r$9C7FQWFYF$[ScɲH؍C,y
1ٜ`EO3KZhn](u,ZĻ"q(%XbJ)x!G)S
O Y]DDS@b!Q	8\
K3,a'Nwyfl,w. 1wOȒKեLtXb3c| A4qh٢/[d)'=XXT
qՏeT_B&X,	pAhAhB7m]'̘lèa 'FQy
40U?G"8v$4&P%$ƛ(`xFA1:,(kU.Ԋx|[lixݯ!1EfR<*;#䷠O4T))Q^#ymC
hp0$MjAR|9&nnyl
fYXa1Rxؐᩳ8<pMq̘'
1kׯk5q/=>\"̙hsFe' Lep-[uaѢ%#ޞְ $"NoА/{/!@='+666/"c6JX`zWV
p敧Ҫo0#DƑl
k;t
<{a5Q<"vkxEPP̜
lo&Ɛ3CLxR`pBjz
jP<uҴ8[1ѫʕӏﳣ'}gj7D-:XZ_~Y/n珽:1|cH2ux % m!^̌zl=,kh(%AֶRn{LTFE
9l8eVdylm=2U|1  IDATD0cw	YbҔrIj֬!HD=\${a5HxnXjS[<(8at͂UZ/_ݾ}IG5(0\,#yYFN~B3`wwϗnܹs8v#aÂsY0fŨl 4a<xlSQ<;9X/*>Zx9Q- sIjy5Ɩ3FbQYZ5b`@e|?wuYrD!,ny4
n4gC"$b$"Ye$[YwcܸqSOjj)_k3GY,TkGܥc<Q-GeӦߝҔ==cv#
mӦUb-^\dABplq'0>gOpZ"ϪYpaEy_읷ΑX<Xh<lݺފmwh$3HBC|_ĺ/\L ,҅ҫIlO_u8=DHCT A`A4H	X1A&`Bb3J_8><T2ő%tlBYC+[倍7E$@$@YI H.CCL}y'Uc~ȯC7HUظXMD,q$WN"XVl$!`ܸ	In"T/]W.+=̹ Vt$BfvGXB0 ~8%}V-u"_ƴXxc.Tc/.ťP[S <b9LӏY݈g,\I~Y8$@$@BO1
C%dCYJƷv:z	u	M"qСa
թ`1F%*bWT-PyC,x)5b#DIFr6.@uy
?0ބq?={t3U3_{c,uʫIl\?vB/#ǋ˻g:J<j;	 	@N# eX7cI+ ?#E,l#hHHHd3pyٷoTVM-X2%\b.lҠa}~
_ׯOai}`%@cTĈ#4B>L&Of
W:SXQgyejG4XI8>Bl --cr#`W90$@$@$ثg?A<òMV@j| cwٻw@Up 7P!fZɂI)>'yRaSɞ#RI"EdcQɒHxfE6ofRy(:uڴI~!f" 	 	@2U0M/i2
.ߎҫgw
(#;u0 ^vMSGWT2yԩVȕKW+w%OFOXaGF!@9&$yI
eժ_dڠ˳#w{xv罓 	 	d*^ v`Oa^ ]WP`Z6X8✝F%Ԗ.*-[4ءVvYNR(b
>wG:=ge۹s']LeT@X؜ ų5HH 	@p!å
t#8j
\ߚ]
	XĈ)/SF5o*'O%!o޼%j9X	1FUa)c	ė)RD߻K Q.͹	P<;wIH2 ʖMRsqq	l޼Ec
`Fǽ{If%k׮%\]\ukcr9)#Fd=T@C,YBk$#3&aڍvAKFCzvddQ<g=sHHIƀO;o4T̞^8S hE>,/^2quubh-..^W
ЊHDYzJe5ZI|OF|Qׯ̛7_ ҭkW`a
+2r^=`zmA♏	 	 	d",ݤIcJE@dN[h'XXbR|y]0C$-jrOx˂rRH8o#`3fq]\r}h)\XBe1a>{V
<V@cӪЎȎ	$'@gHH2 ,ХKRk*!t-ϰ¯\*pk׮k ca=FM;vJ-V5Jɞ=E2m
@,D%[liZzT|[jH0l$@gHHH 	%Q:}75ba53gn]Ǐ(}V!uZ#SNW4Vdv\8˻OH˳w1oHH ⓑx4/GnZa;ofM{QT1~/H$1rӼO  FH:uz(TVPԪUSBBHBB_?i
\_=uꔮwl#Y	P<;kIHH 	*Ɛ!u5>D<˨rq7oݔ{ZX(&/"4	P<   \XR1c9ƍJn̶ܺQE%ܹnM,`R!@ǂHHH Kܰa)]򒘘rٺuU<Ix<(	 	 	 	#xv^= 	 	 	 	d
Lʃ 	 	 	 	8"gGU	 	 	 	@x<(	 	 	 	#xv^= 	 	 	 	d
Lʃ 	 	 	 	8"gGU	 	 	 	@x<(	 	 	 	#xv^= 	 	 	 	d
Lʃ 	 	 	 	8"gGU	 	 	 	@x<(	 	 	 	#xv^= 	8}#~:uz{߸W|']&UToooٰac^zSq͒ 	@j(\ 	@' [oJhnj׮%~\pѢx~1'Os8
^>	 	d.ˣ 	@x4P|||d`9|wPBRt)x=j]}-z=iҤO;BblYXD%1bL޽?Gd=$@$`/('x$@$ڵk+Cʕ?6~-=JN7^ɍ7dbJiTjy~Q2pSoɦM<l#4t\Z͛/?Cʹ	 	<9x$@$!||ҹswiڴ6TfQ1o+QQQЌ޽{J)s$8x}_Y`T^=@$@NKi7N$(xg$=Fk}՚l3
Dy;s4o&x>D<ϝ;OҨqsGA  	P<ی; 	}@K^		ң{7ڭԩ]$k<zTq)n yFjJut//OV-H6^
	 	d"LC 	@V0ϕ*U?MˈMV&ܱc{y12p`,%>.^2_>u|}jV,_*SNʛo&Ղɧem$@$`(x$@$3<tL>C~eAߴFnsvugSigFժU~sϿ(v~$@$P< 	 	 	 	dc3 	 	 	 	09x$@$@$@$@G9$@$@$@$@9 s0^.	 	 	 	@x><3	 	 	 	@#@:K$@$@$@$}(=L$@$@$@$P< 	 	 	 	dc3 	 	 	 	09x$@$@$@$@G9$@$@$@$@9 s0^.	 	 	 	@x><3	 	 	 	@#@:K$@$@$@$}(=L$@$@$@$P< 	 	 	 	dc3 	 	 	 	09x$	"	+ͷҳgy7vb6AvsO
ߓkwrILw٧KQ2|39qXymg@$@JQ	r JD<$"Ƌ0q9"+U(ԮUSbcc%,l/իK֭dԨRbE	?Z}7Q&VFܐ%D7K.2鐇D>fqE<9\z!ŋȣ2ǟd͚Vs$@$`g[hq[ 	$u{ES{7^rݼ Ƌ$&X<=gYvl۶C8-]Z^{9r8RP^]>m7A,\,͛77xMk>}{NW
Hsxs/ž7[gG!O>B~}W~rA8p#  ų--	Mo*ڤ.#K\ܛ'Y<vzgҷ_oZZwg9z.]:˫$3gH-|\6m,C
2~1,\|r=IPKTTK~}/ݻwcƌ>{ISCͤL2۔?7_xt]U)y_lB.YO^RS$00Py}Rx1ߠI&˔i"Ey$_>w͛WbcoiƠŋzӡvElcY!9<! P<ۄ 	XMMvP%]I!ܑtMK<*UJ.P7mݻ믽"O
;#sɤ_qcƼ e>+ޑG}*yʻK_&CHq}ES~={w}VZEf̐P/^jq{cbeCs.qdڢ[%-wWU7_2[W5m"& MKblk׮%sfϔ>\.WCdժթ<Cd?w'رcaq  ų9	u.	5zj]づ8xvuu F[v̟P&LQر_Hu4F9ǞoɢKtIpأ?qB׷KRnCek֮S+aNyfZd?XhP[Zk5m"/x{ĳ%իWn{;u |oXbb.ex<Dlfϴ._|K6l ;u5mqïD<8qh\~YWrc  ų	MUn+XO%/>-"/<?F:th'?xT<hA:lo?9Dv#*_+1hnJ?B5j(uԑvJdd~zX+jՔƍJ65W-yn=W~sZOg܏qH۱Rj5>OF~N6lhϴnoJdѦMCD<ϪU-[HudQF$@M9x$@J +aCȇ}"zU,_f_Meø	o~֭Iĳy77oT&M [3gX|xUi۶jY)-E<
3bG>l|%ȫa5_nH$@x#@Vm d <3j^#.6o
ZE֬Y!=z ץK']v  h@O4mRΞ=;z[-7_Ifax"O8.OK FҫW_Ӧa?pP>!˙h
a# &@Dy< 0COkаt[壏2edmhKĉdkۯ_AZC,^rOЪ/u=z뭣͛75>~IA`zZ	>KMV&ZixFzz</_ׯ_OfΘNVX%h 60С|gt˲rjKX;	 	Lfd܁HZ֗Sroaӊy.WRք"uΜrabzu}yvZ=eo$UT_xNʝ;we2v7>iF):}(ekƍZƍ|*Uggly3...]y?)Z.~'nyܔ<7ft^|'Oji3lbˍIHZ֒v$@E #Iyp2rC! ,!@%ypn<sSgI$@$`;gۙq %.z$K{7<=v'?(ϗZK8s6 '    &@lË#   '    &@lË#   '    &@lË#   '    &@lË#   '    &@lË#   ']b5+q    IENDB`

================================================================================
--- File: d23/d3_Training_Report_BanDiaInt.md ---
================================================================================

# Training Report for Sentence Transformer Model Fine-Tuning

## Executive Summary
This report details the fine-tuning process of a Sentence Transformer model (`l3cube-pune/bengali-sentence-similarity-sbert`) for semantic similarity tasks, conducted to support an investigation into the model's performance and training efficacy. The training was performed on a dataset aggregated from multiple sources, with a focus on reproducibility, efficiency, and robust evaluation. Key metrics, including evaluation scores and average cosine similarity, are presented to provide insight into the model's performance during training. The results indicate consistent performance, with evaluation scores stabilizing around 0.90–0.92 and an improving average cosine similarity, suggesting effective fine-tuning.

## Introduction
The objective of this training was to fine-tune a pre-trained Sentence Transformer model to enhance its ability to compute semantic similarity between Bengali text pairs, specifically for question-answering tasks. The training leveraged a curated dataset, optimized hyperparameters, and a structured evaluation process to ensure reliable outcomes. This report is prepared for the investigation committee to provide a comprehensive overview of the methodology, results, and implications of the training process.

## Methodology

### Model Architecture
- **Base Model**: `l3cube-pune/bengali-sentence-similarity-sbert`, a transformer-based model tailored for Bengali sentence similarity tasks.
- **Configuration**:
  - Maximum sequence length: 256 tokens.
  - Pooling strategy: Mean pooling of token embeddings.
  - Lower transformer layers were frozen to preserve pre-trained weights, while the last two layers were unfrozen for fine-tuning to adapt to the target task.
- **Device**: Training was conducted on a CUDA-enabled GPU to leverage hardware acceleration.

### Dataset
- **Sources**: Data was aggregated from three directories (`/home/vpa/deliverables/D2/BanQA/data`, `/home/vpa/deliverables/D3/BanQA/data`, `/home/vpa/deliverables/D4/BanQA/data`).
- **Training Samples**: 10,000 samples were generated, split into 90% training (9,000 samples) and 10% validation (1,000 samples).
- **Preprocessing**: The `generate_training_examples` function was used to create training examples, ensuring balanced and representative data. Validation data included query-passage pairs with associated similarity scores.
- **DataLoader**: Training data was batched with a batch size of 16, shuffled to ensure randomness.

### Training Configuration
- **Loss Function**: Multiple Negatives Ranking Loss (`MultipleNegativesRankingLoss`), suitable for contrastive learning in semantic similarity tasks.
- **Hyperparameters**:
  - Epochs: 1
  - Learning Rate: 1e-5
  - Warmup Steps: 100
  - Optimizer: AdamW (with mixed precision training enabled via `use_amp=True`)
- **Evaluation**:
  - Validation was performed every 281–282 steps (approximately half an epoch).
  - Metrics included:
    - **Evaluation Score**: Cosine similarity-based score from `EmbeddingSimilarityEvaluator`.
    - **Average Cosine Similarity**: Mean cosine similarity between query-passage pairs in the validation set.
- **Reproducibility**: A fixed seed (42) was set for PyTorch, NumPy, and random operations to ensure consistent results.

### Metrics Logging
- Metrics were logged at three checkpoints: mid-epoch (step 281), near the end of the epoch (step 562), and at epoch completion (step 563).
- Logs were saved to `training_metrics.json` in the model output directory (`./model_output`).

## Results

The training metrics are summarized below, extracted from the provided `training_metrics.json`:

| Timestamp             | Epoch  | Steps | Train Loss | Eval Score | Avg Cosine Similarity |
|-----------------------|--------|-------|------------|------------|-----------------------|
| 2025-05-14 09:49:11   | 0.499  | 281   | N/A        | 0.9220     | 0.5488                |
| 2025-05-14 09:49:34   | 0.998  | 562   | N/A        | 0.9037     | 0.5586                |
| 2025-05-14 09:49:45   | 1.000  | 563   | N/A        | 0.9037     | 0.5586                |

### Key Observations
1. **Evaluation Score**:
   - The evaluation score peaked at 0.9220 at step 281 (mid-epoch), indicating strong alignment between predicted and ground-truth similarities.
   - A slight decrease to 0.9037 was observed by the end of training (steps 562 and 563), suggesting possible overfitting or stabilization of model performance.
   - The final evaluation score of 0.9037 remains high, indicating robust performance on the validation set.

2. **Average Cosine Similarity**:
   - The average cosine similarity improved from 0.5488 at step 281 to 0.5586 by steps 562 and 563.
   - This upward trend suggests that the model learned to produce embeddings with better semantic alignment over the course of training.

3. **Train Loss**:
   - The training loss was not logged (`null` in metrics), which may indicate a configuration issue in the loss logging mechanism. Future iterations should address this to provide a complete picture of training dynamics.

4. **Training Duration**:
   - The timestamps indicate that training completed within approximately 34 seconds (from 09:49:11 to 09:49:45), reflecting efficient use of GPU resources and a relatively small dataset for one epoch.

## Discussion

### Performance Analysis
- The evaluation score of 0.9037–0.9220 is indicative of a well-performing model for semantic similarity tasks, particularly given the complexity of Bengali text processing.
- The slight drop in evaluation score from mid-epoch to the end may suggest that the model began to overfit to the training data or that the learning rate was not optimally decayed. However, the stable final score of 0.9037 suggests reliable generalization.
- The improvement in average cosine similarity (from 0.5488 to 0.5586) confirms that the fine-tuning process enhanced the model's ability to capture semantic relationships.

### Limitations
- **Single Epoch**: Training was limited to one epoch, which may not have allowed the model to fully converge. Additional epochs could potentially improve performance.
- **Missing Train Loss**: The absence of train loss metrics limits the ability to assess the optimization process. This should be rectified in future runs.
- **Dataset Size**: While 10,000 samples are substantial, a larger or more diverse dataset could further enhance model robustness.

### Recommendations
1. **Extend Training**: Conduct additional epochs (e.g., 3–5) with learning rate scheduling to explore whether further performance gains are achievable.
2. **Fix Loss Logging**: Modify the training script to ensure train loss is properly captured and logged for comprehensive monitoring.
3. **Hyperparameter Tuning**: Experiment with different learning rates (e.g., 2e-5, 5e-5) and warmup steps to optimize convergence.
4. **Data Augmentation**: Incorporate additional data sources or augmentation techniques to increase dataset diversity and improve generalization.
5. **Model Checkpointing**: Save intermediate model checkpoints to allow rollback to the best-performing model (e.g., at step 281 with eval score 0.9220).

## Conclusion
The fine-tuning of the `l3cube-pune/bengali-sentence-similarity-sbert` model was successful, achieving a high evaluation score (0.9037–0.9220) and demonstrating improved cosine similarity (0.5488 to 0.5586) over the course of training. The results indicate that the model is well-suited for semantic similarity tasks in Bengali text, though minor improvements in training configuration and logging could enhance future iterations. This report provides a transparent account of the process and outcomes for the investigation committee’s review.

## Appendices
- **Software Versions**:
  - PyTorch: [Version logged during training]
  - Transformers: [Version logged during training]
  - Sentence-Transformers: [Version logged during training]
- **Model Output**: Saved to `./model_output`.
- **Dataset Splits**: Saved to `./dataset_splits`.
- **Metrics Log**: Available in `training_metrics.json`.

================================================================================
--- File: d23/metrics_ministry.json ---
================================================================================

{
  "base": {
    "0.5": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9290882022206419
    },
    "0.6": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9290882022206419
    },
    "0.7": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9290882022206419
    },
    "0.8": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9290882022206419
    },
    "0.9": {
      "recall_at_k": {
        "1": 0.9081124177795495,
        "3": 0.9081124177795495,
        "5": 0.9081124177795495
      },
      "precision_at_k": {
        "1": 0.9081124177795495,
        "3": 0.9081124177795495,
        "5": 0.9081124177795495
      },
      "mrr_at_k": {
        "1": 0.9081124177795495,
        "3": 0.9081124177795495,
        "5": 0.9081124177795495
      },
      "f1_at_k": {
        "1": 0.9081124177795495,
        "3": 0.9081124177795495,
        "5": 0.9081124177795495
      },
      "avg_top1_similarity": 0.9333437364011032
    }
  },
  "d3_version": {
    "0.5": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9362253154267802
    },
    "0.6": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9362253154267802
    },
    "0.7": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9362253154267802
    },
    "0.8": {
      "recall_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "precision_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "mrr_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "f1_at_k": {
        "1": 1.0,
        "3": 1.0,
        "5": 1.0
      },
      "avg_top1_similarity": 0.9362253154267802
    },
    "0.9": {
      "recall_at_k": {
        "1": 0.9457843332668926,
        "3": 0.9457843332668926,
        "5": 0.9457843332668926
      },
      "precision_at_k": {
        "1": 0.9457843332668926,
        "3": 0.9457843332668926,
        "5": 0.9457843332668926
      },
      "mrr_at_k": {
        "1": 0.9457843332668926,
        "3": 0.9457843332668926,
        "5": 0.9457843332668926
      },
      "f1_at_k": {
        "1": 0.9457843332668926,
        "3": 0.9457843332668926,
        "5": 0.9457843332668926
      },
      "avg_top1_similarity": 0.939058726161739
    }
  }
}

================================================================================
--- File: d23/d3_BanInfRet_model_evaluation_report.pdf ---
================================================================================

%PDF-1.5
%
15 0 obj
<</Filter/FlateDecode/Length 2235>>
stream
xZKok
WemENnE{E{/%c0lQE~|˦6	/DL<3q/?MKvEK˗QJ=.>5E{	<˔4:q
\|6-4jl.ڊ$nwkf=Tv|a501k.THgY^kup	)`|%,~O/,fSAhl^Dfd;\@hhEbNmJ$4qOBEKog
ߪxК6YWMgS6ƍ;2'G""<eO[u^\W%Aɀk=*ب@WB#[p+02U$,}`WH8dHzM
mc@[@; [C~%G}̹9w	t-_)=h4\I;O`Fm
ĕ[plG#@ULU(I+Jmtr+`<3{9 2qߋG#
M%+d ;pUQ3gciExx5FU}AFsEc]qq~yVXlmJ^)/bchAq	G,SL0I@ہIuԻf1D077*ٌw<!℧*k/ZVOhު%c hab
Y%n;G
N/ūw.]{A9WK5SrFZg!j*)k9<'yv &Ri)/Vn60`ZνIFI]1s>pDƘVRg*viЯ/*'/0,C%&YhR*v*ة^>Z'H}lULeUSU)jQ߂ozs;Cv:cqD8@.Y4}­;I[𦱚hTϣZ.L_A=7USMxAe?U:;0F^m):L%eT}X׼]+3)ooZ=2︝UK
a#EWZݎޕAΧPv!
8TMju9#g֦T@W⽝fK1|y1ɓAV7fĹ{I`{ԃ"/`t&Si}N<CZeŠ;4:.w?.>tyl΂9SmE\;6M3V24DsoYGvލjA/[qvap0v
0VGn`x(wuuKD߂HA+oE2~"fzD0	ez#LH}	?l6#[A2>'B<ߦ-RL;Z.0>lJx(Ut,T[OQ&\$vT))!YS*ϧl>+LS8`T)X.{i
@יv`{[K@^X-U7?>VI za|CHFoGHwȂpܼ$L<ɸ2بy'oV.ϻ`R~ 2igG7<R89 Y!a< }u=&	f,t~"ټu[֧ Qty'ʜNlԇCE1SFwz6Vע6l-k
z^畛7VZ&>c6Qj\dA@
ϚlAw8z˨p_2E1pHNqӶȭO=L|isĄ	k`'{,/W,/o"/{7_(3=+T(q<cͱxaFdj{Y5JʁPx゗B"݊U[Q'œZP,EepM"
dHUˠ5xliGJ)V3kryɧE8]T;~E?'FUΚנpL:(~+J.n5p#s@Zv	:b4Wx*
endstream
endobj
21 0 obj
<</Filter/FlateDecode/Length 1526>>
stream
xY37SVt."@ ]킿S)M(([Ϳ
3#R$ERgHq/~돓/˷?E)r򴼼]Tj1ZWmw<2@u1U&id8Feܶۙ/Mc-ݙžodI]Z*6M$YO1ٓgn_s)8Rl
CQO_Z];i
YρkBÛ|%.DrRfQ^hRK'U
6(6VQs(Koq=d
51ԧ:{
ghs2L|R~Ӂi3f9F$Yz+iMs\Ғ3-EaOFi_զLϥjp߲;5h(,Îrp\[akF(- %3T0H@MJÓů_[McͧVM +z'+KPwO27d[]RHhc
xrk	L[
lUhb_F$ulk#-z
yC؋$)}qb
2m
sd?<oOyK}TA xh۶T&Sm02B3׬4 ̬0[Φ8NF(;-|N^!ΗO܏N@?9E5R]Mg@hKdf*X kK+vomU(#$,$!#3,ց?MdbauƊMM'5s\9S +=⤲kt$y1?VX%ӡ@L$ff(cSjnI]L=6R7}O	B3#  u U))ՐݳM;6=g,^E\"eԥ{8aCHq\f0|7!Akyڭz1)BWg:N3M4
*|o"9Gv4^9kE_\ok̢tujڞudo]%Tȑ֟NQm9˸"r4:lٗb7C8F8yq/pzT534Ķp-PO<c6Py5=W+( |֮q>98LqԌ)_q?>U,չB>;=֌ob!UE;it"0uٝ\2xYW=*naG8\97q;9ն1_N0~MrN@Z]oQS*&ː#X'kGa`9`M5H~gAas⦏kA򁜅sN(9H~e_,.VwYח?m>jܒ:eׅ6Ho
9
endstream
endobj
24 0 obj
<</Filter/FlateDecode/Length 2586>>
stream
xڽZ;$
+&Kp6P*gLv03wȁ"> d{fVlw  N$:}
>Nv8:)%sq?$\4?y`k8NIic|lFJe_y7c}[R.ec6cd.!sWK}Y:O'Dݏr</WO:5>z~S3*f^_׷7R{xKRV-Zl[w!_]i_*oșVk1L,c1bŴKu؟PFV
kyhQV{W	qfZ^`[
!$^PU:6nfX
z}eѬփ" Be

7dHy~5pJhi3L/mD,R(˛eAJEC_2Bڀ+v":oųebcҭ^T6ɧؠo&#Vztq~wn'R4~¹oSX%!.*=Wy0M@(oGsߡ'q
E%8S.R[:fNY<ZFD ubޓv
tߞ觝vḁ"~%'.hڵ\
7JD0!c:CfkJŕչ1T%!H#4sfK]Ət/o0F=0at6r;t+7R#@?'O'JF>N*UfJE
];n.!A)#,%MvSࠝ[H+C Ȱ 5s<atB+Cx%q,
hcCȖ0@(z
a(9ӃK(E Ήbr_+Ph:W=(,yXئv_A)G1t]hEyJvIYa ||:&yN[2ho5<QgP:~IC|,"s{aLq*uIKpTKB"ԛ:I7-#_rpQ`d`ʓ6wA"յѝ2zX҂你:k08L.(W&W*Uyrj<Ncan2HGXnKVQ3gnDd~<!=:#7+A/s8OMV<2d3u)aVgYغƱUk͵-uVġ&N{e7BA-0T$L_A
]yL	CY`&_9"Dd16U=^sXZgƂ:6*^em%q}7;^g9yn y+.WcCNsk%Bo)vʈ+F9(8r⼢sGnym
,:M)d (AQxPsQm/ӍaRC
0m[,V*v،,
 Y*:IDNkd¡8plf[KDvw}**=Lx{X/:(iA0_ReEgɲ#{񅞺U&@C䛉N$|,Ey&5osJa3o5x9+K<҉ô`SGS5ԔKU'-F̆	"sur0oShz#L?e)P\Қd7.sF=h4oy3&h |p)lFq)bV|H7qH*KR\UK`̚A9EV5ɫ=Z}UYa>w)Q4Wiatúm1Ǔ(ReH=c	؅7߈ѿ݅z«u"~QZgMZ5zԏVuyD2cdub&] !{@4gas8
	7}A԰!h{&~˃o\QV`S;=B{'6QF''s`mآS0F(zheNd<rӣ:
P2p|`14婛` aX>vjӝH!<kW_4wpRJؗ?׹O6<HУR,Aj'AOB9gq:߯\m?q[9xYˏ>&EC@7c7@jp=}ELVSZOן]s'n!ez`?u¾@TjJļ ]/rL@rRRV6	J؍uĮ/hd٪nQ>Ew]0?wD:ΌH_S\A6
endstream
endobj
27 0 obj
<</Filter/FlateDecode/Length 840>>
stream
xڍV0/CTQDp3ht+萸NK%h';Fd(O'lnML|-Oy4η2{ٸNG֖W8WY;<+Aa09A]6<x\S6-uT|s#nkd;2YTNMVO\x*=^8ZgMڎ<c[3`cyF'
OLcYҞ\THXM@{U+Wx/\Q%qu^32)6*h3'[f(̐M{ޡ~aGJ$Ïw@י{e)-6*'H^5z {405%L/iyUHPAE֔mQm'h+s0'E["_wVΥGh";̡u#>T}"4bv٭vs^5$\zc5OC3I0"[|<=<̆ݘeLaaYb2lIY2PQnbkG Dx}okߞ6$9w!6+[̾mtf1X!Q!!MMQ[zO6=oY:_xUqY7sw+rnK݌YE,u@E_F<6XkwB=	.+;YϲCPXߛ"}'@
L)Vr+S{.xwpKl@OX3iK)٣zM홷:;C?3.
bYܻ~gZL
endstream
endobj
58 0 obj
<</Filter/FlateDecode/Length 356>>
stream
x]RMo0+r4U@J[	iv`RI Z#=vm2Ҳa`u+*
u	M+,-
ePfg?zARsCVAMU36D-Y[SZ<Ë>u5Y-NiL>*u+IΥJYAt!bgZ	wJ,u[hTGqx ZV4D7G\)$n&39$O9yz!HMbB%	y rt1s2ЊbQ@Ą=7CsrzzA,
󌭀)L?3
endstream
endobj
59 0 obj
<</Filter/FlateDecode/Length 343>>
stream
x]R]0|W@Ԅ(jB.zH|פ4tݼc(a`M^? v[3يaF]m7?Tw0x<A`fwT3	
Qϸؕ~Pfi0G~OH}mnlqKcЁdq&ln޸2ƕ9kR][[doGQNh{v8="t!|҅H.E9WGi|GKJdDDI0|ᆊ N'Y[
I5J9+~4
IkǫyKU)ͤC
endstream
endobj
60 0 obj
<</Filter/FlateDecode/Length 305>>
stream
x]1o0w~TU&*BJT%::A-2fȿ/{Usht$H'0r 7襊hJn3Q\9Hޞ'txJ̮n.$]9ШN69: 7x
Rduڠ1w@9De(ސka,S=DE2|=Ǯ[W'Iz(Hu)_
ʑv!eܟY(XbGE1lb)9;K07\g%<7l]
endstream
endobj
61 0 obj
<</Filter/FlateDecode/Length 240>>
stream
x]Pn SU+UƉ%i8b8H1 }[:q{O]V5F+ٷ3Edf':&yR	u#$N~綿z7دbo2y` d7yCItJVmwQ{,\:Fd@QZ뒠;}@R
ayzy"*Ĥbv.xHH1kldo:
endstream
endobj
63 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 426>>
stream
xcd`aa`dd	s
sv
 M:4L?200A]
$[X9J*
*23J45--u,sS2|K2RsKԒJ
+}rbt;M̒ԢT@s~nAiIjo~JjQIL@01032Ov~?=NEb{ѢZ߻fٽV߻kKKJvϔv.ZԽpߏk~^}>[]S V͑ž`euedm{ؽcǣ-<*k2u:o+[yNƾ:	<<gqM n
endstream
endobj
64 0 obj
<</Filter/FlateDecode/Length 258>>
stream
x]Pn0+x4d!QR$TV%Q^b,c}mrx4QYk%-D_f-Zi1A* ${n>2MaF\s4uܮU~\W}}nX~J	@gkV8b>@# [نIh#*1 l$p֌aj@B8ZU9A%?Z8fo.>fB+f_|8bB	xR#W=iv
endstream
endobj
66 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 987>>
stream
x%leZ^G78- S "ٺ)!iXP&A31!hvnmwOok{/
L`
f1!1F1%c|v	Xxy>y$a4$IVHmKéB{viA{H=h6>2$ug˺>|j]wbx4a$IjzzYzw~^fIt/\}w7l_T=oK^]ø}L}dCoWolv2nŸA6⤎ v 	#"ql$ߴX%VasՀ~SUd=NTj!-WIlp;Wu{''OxK4dtZ$B0jcch"OxUYJM<zyN_R8hbr|WLwTq+؆CV76 pph"$H8vL
(3jiA`,	g@x
Qv]&@i gMyP<g>
|r
	1%&Qv
EbT6OJV/B׹׭01naIR sd $KM,B$Zz"1PT$DI+r6@?Tt@iڒ%Ň^ `(	"|*.6"SF%ֱ㐔tl9/I|TK	!a>,@ Pnx1Kk,ܰnuyp
ˏn3F!]Iz55<}$'a`C@4+Sk./-~8r$GsT:ZhJ_N+ϙJ+rɤ<nޜ/94gSȻ
endstream
endobj
67 0 obj
<</Filter/FlateDecode/Length 255>>
stream
x]Pj0+qK)Fe"xڮ9&OIك_=77LRujޣQ*aq7"iBrpT'f،|tgXit43we5Yۺ_sF
EA f8U<YV	תH3gT()`25!)(-hb7IxnD?N˽]LZn/;pҝ7j[_h
m*_$}
endstream
endobj
69 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 1419>>
stream
xmyPSW#!Dڗ踰8T[ktT#"$@EH*"!,&ĥ"@qR7.jq3ŧ3
8sg{s~wII+/_zthvAAл_dڅXDA|3ac	6ILA(
45M#I%
2J(&j$Ds.XHJ494Fm``vvv@LP%Ȗj"Zʒ$)A\L"ahLdJN$K
as"!b"Exps!HM nMZ˺Nd7%,c>
G}ŰS(b
c!ooTSn;)uخ{Uld2נzhڐ"Ű!-M6Ċ,lwx
PM) ɹ
,裉Z-Js4[:7d3-5v_,:G29%
swE r
ް4YXp!g=,ǚ
J(1tvzܱ0H&KMܬew#dtb#QaVu N	-ˬPgKGav{rWk|WfFE%^]X}`&U4B 僶d|,AZ4&}Ef̿We&8{ygO aʊg-}JX
ڊ
`rL7SePZZ~
~BW\*s2SU	AerZc*'C4
澛A*Cf~oo$=cӝT346	lʬZT:$0c/(&o|&?6/T b@vY_^^RR 6Zؠ֥Ff/9{px@q-3]9M(	q039] Iطݟ9<Zq'^N+]%@Z)mVm-56/(4eh{]@,puػoXMA\xh5{DXt`Q(xp差#K<`x9ջU'P9gNڌ\]Fl[S<{`o,;qtF58o.t3>ԙ]_\juWjb<&F;va$=Iv:a>	L:|ρm
T'Lߌv~Pt5VTVq*mTy{!wj;~8_ݺ
endstream
endobj
70 0 obj
<</Filter/FlateDecode/Length 244>>
stream
x]Pn07*lBjIPFuPTl˘/Yw޻cVq
v2lS|e/IpGHnt3a 
rBՑOC2,# 8?MݗW`W<n/hPQU88!!(!+˜QҕѴ.d3לdl?izGކXr~-F͜u+oo
endstream
endobj
72 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 443>>
stream
xcd`aa`ddp
v
44 l2?d1g!"Ћ?,AD~ j@q+##`}딜c=#ʢdMCKKs#KԢ<Ē 'G!8?93RA&J_\/1X/(NSG<$C!(8,5E-?D/17U=土[PZZZt #бE̌,*t}_=+ïC(^W\xŊŋW[vw?g;ODM<{:Ǵ)rmM)Q4ef돽gHh6e4)&K?svϟ_]#g=[MYwy9|ŋ/d-=]|>$\7bE ]R
endstream
endobj
73 0 obj
<</Filter/FlateDecode/Length 233>>
stream
x]P1n0
)BraUCҢJDjI: w$\6ƻQsa"b<+`I[td\u<HyGyTק˹.bwc爰1.@Y2 Ͱ{Ň$|TQS8O XUq$,Q${du]1_oSѴ0)KnZݙhٛ˝b*bLm
endstream
endobj
75 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 312>>
stream
xcd`aa`dduwvsv
 	aYs>i2?d~1g{I |"_Hu (%QZi``g``_PYQ`hii`d``ZXX('gT*hdX뗗%i(gd(((%*ݮ&sJKR|SR@>:EGÿ33>={ŅyK^t_~MgbsWN; 1" 7cy
endstream
endobj
76 0 obj
<</Filter/FlateDecode/Length 305>>
stream
x]j0EYg^`]IKtHKBVZ7
d83
˦jdo!4h082=3/
aݾGeUxPR-M,ϧ:^OTc84S@5<`/.a^^aq.[iZp@i!
ŴWG8&t
J/Kqձ*<%D5Sz(\D;O+o0J&?2_m7&,IpE&.)t<wcz{.k]S
endstream
endobj
79 0 obj
<</Filter/FlateDecode/Length 14>>
stream
xګ?
  	
endstream
endobj
80 0 obj
<</Length1 31309/Filter/FlateDecode/Length 10904>>
stream
x}	t#u`U_7Hj	E}69̈Đ)G2$[X$ć8q5G$/d\#N6V&79v!Wujԯ_U_#B&t
(35O$:<w_\a<SW.l2=!K{O'!Awz<BNCAW!/\&#0B󫋥_*\(]h~xgK/xK	\|qz7<Y⁞)>qJxaD̦ĠFdEhdjj7Ɵ!ĥpdtġwj!}250LxMv,Ԑ2Y
m6Γ(fo;4G74 /qp?( V;xti?]ڗV|?|)Ae xע*()*PQ^Bc0{dNi*
ŅSCs
)}^hqQ:
TTPA0*,S
N*©$J)lR\zuP8
B~q0WEJPZR7nв/DJoX\87u疋7ܘrIM(Oq
W\.zF6ah.9@^i([ PG(@Qi9Qɐ|yzNi\/qznJDRg$PbR

SAA;+dE%,JE'EHbJ"PEJ0=褼hR*n;I%a׫O~ТCPYa	[fsR1TyvD.))ET
ĨjCVK`.R0'O)xQtbHZš"AeaRkL0XPa(m)Ӑ^pH( uDJD!"TjҠ:6͕zh۬U6$)C/;xP1F\\g!e4RI@Et  hHT_6J"adVX/
ĖRɱs`^1/KWS#9v`nlV-A˝ud/[
.[L90z5ݠ62=N毃aXk<$A2UIɤ #PSYQ:B	UPP
1Ֆ+SR^R,`~f	L./.|#r|>O$:\Zw&axt%R'IZ&imr%i]r#7Γԗ\HOHHILHHJe+HZ
>NfKJIVT*/VeRL":q[>UV	?
$?$ $$$m H44{6%aؚ^(P$L$1SaD6RD[bx	e[!bqJ6c8EeB<iIVMz0moI)GD>cm `d)udӓR:{;AEq	 G!sׅc$˭x whU(
iI{C;Ĵڟ"H2,"{yyi%B^P@*q%IaQjP@<6% 
4:`abtHO2x08MB!_͓nS|TbݪR~X!-m0JZAb,zP$tiPF+cUvM[1
J
eu- g7e?DÊ075U)3	v`GwzGmmߪE!t%jҝn

M+h1DY&U%_ -NT铆?\7ZSnSȿUUK9+Q>u'B&-\괇hf#,1;J;I	"!8oYZIbS
0
 &
LKf %g ܑ||a90Zv (:F(tQ<s2&JdL$c```!
kӔ.t,@(]:O"Jthdܳ4 ^R /\rkj8WTEqs74- iq

~$ Axm QQA~M	CRCx
U <[=Js]*H߭1h!<
M0V78Mb>~jyNA4ckHr2]A1@x
!sxBH:?ܺ7rQ
vƳב	U#/:)CNg8[9#FzN %&̲&,XJf,f¨)ՐkI!_^bȄMfw.fZ[RXp9ݸ.VQ_ON1s@'^F13V<^]^㻄O<pЉ/lXi,<ܼA(G9ea`TwrЀ10t-٫W?s>Sx0}GHf><}G4k
@jG3?05샍+zk6O/Ye=
깻=	
Ew_0=V쮶Zj<@*.~ja?nd/tA2Uk{kU#oa|=Yp	Mby*tH] c =<!0`OĚRrf2<fTNmVT0P5s>%I;kfNgg'OZVF;;vw}ju#G>cmzӟ^e yPVNW	̉q'5dyqI ۃ<;u\kƵ	lNw.Z(3ހ(dL8mb@^|6N?!tEy)98kcY"U	h8I@5^Q@EzG۩́Wt:`bk7ۚK~;)QWΤ;okwZo,k
:ӂ&f9%8^ 1˰R9]	t@+Ql?):j^Qn1l*Zj-Cx g]N#/Pbr$*ߗɝ񢱐[=pSxj:{fnU7ԉD%{=qW+\xD׃;'])lvG`i&j9[%DrԋXX7}Ȅ9w`A3ՍtДhkDOȦ fu.'CHS`M)/o@@#;C^9d
#)xvɠP#nԶKoaAs<R,
:m?
=١G'8:/:+=bLt&,z^|6rʛtzA
IGbNc2K1
g݌f*T>b}`c
a1ȏ-gPT.ytn7b~vf!36e]-vv̈q<-7d~S0|}Ɉw.AK#yр
@5P"|LyjiYWYPXZ^{S]Q2#2!\9N'\0oܗk~s:4d2{ܜvY8EޙescFn:\W۷4.T,S;bڶ\Ps_r~y#nw6
LM:-|%grh@:607liou3}`(:G'hP !rEl	|YQ,"&Bd@AouuV"I{!:L)޽;s3=@V63#]-]Q3.~jyjz\̊z|nf]+\Y+:.gϔȊy4CHUyu!vPSڢ\~yci:=y]BS`zlƜ`,=Î{i_YTTQDn^`"	bDRhJdu`T[7j+63oME942Cr_{kS*	2tcMX+3z5zW#AbA cu+>ubϜxϑt»s
q_oiHlnɱ\&ݰcgy(d-D{r}]cwtɉL"j(uث\E
sp޵VVo	G56sXliQJ~^`؎όxOS	 0o1ԩ $IIOdɪ9d9AXH6|ۦ߼}O5l8j{<un[<afn>ttL#ǡ瓓&⟢@B8zE;qG	`J[a穹Beo&HV@i]1jXeE1Kq1PP$d".UiLRKHۧH6GÙlȌ3#76CsÇ
CsM8VqUW{}7~7:??bZQ/Tu5sw@厩ժ,>643#\|lޔ7O3b43saQ]""yЏcІ4mzBrr%xm4jJۺځr"$-$HjigF瞻+k|cs_x!U!/~{b1^@H!A+\%/`8g(Q[H3A{k'8dr<	zӐt50^
 ;DAGfxXop`ĥ6m'5-<K}?x)ZtqOD'Bٍ7Ȇϗ#|蓪|be }
X?/aK0oOgh-% x /i0mP28O@ٯ9PA!4!δaZ&UL
3Cr(לI%cHzTzn\dgȩ{͐(ӫڲ)"ԒW!5()6_2+!-aGjIkJu+!Ԫڶ(*ӡ7ڨu>xM6^L*`UyC(y`R,ЁafYeG+pZTND,1bVXЩ9e'$r5PZ~k˷*'@af%T޸kebt!`em>ɧ'
1'!O.9ݖnc%Vi*obFlySeȸo>ļ[vFmrgf<KN 9\¼Aϔ	p=L[LRd	*-sdF6ĿiӌѬUu:YgOkg34}ţGW/?==usI]+[J`E QwOPkofQv S+$bR$Dzy<vX݋PdWOv	:0gT"(ד茰&VYXLrk^3-dd0h-51-ci/"95!b\CcSboUuf2A7{JkI6X6WZGU7Ǣg''7=>fi-w w3Vl2}	L
AQuۇ]&Gɋnyr4
B񣝋œzW6%N>jDGS\&zj@b0)]ѾDM8ڋյ[i
;rNwN
i(p,Ë hC
CMqDyݥZy.$,LT)2HqA0L.aX"{a=y#A~kd(#p7tdP`HLC&$ᷚMiK#ɅvH5,$C@l0wim1`'3cG}aVѻO>4`؞vїa)߸}c>ف&GHDI4-OxXFYzwy|	Z/u  lυC5DCpWgU[:05FՕ$Te\ɵ
^u_пj<ϜYwv&$9鮁i5kJ=%9wӫ|V۠/kؿQn%ة_ś_^ORs3824dEǌ<wΝ#gĂZOژ krVTY35k*my.Yoyj<N
FAMы'k[kaAT,Kd
/88Hy
m$rj{kfw7IDȫgΞE*/Aت=6Lt0,ϐ	9Ӽ&PUMF ݃=J!%:ԃOpՍ}-v]Ut2iO=O^'}a1efg:KGM._Sk0\v'&
{,H-k?IXX,"ۅ3J"~*J#4\.c¢yd_MO<	GɊyOSsq>U"Ϧ0
hSv ̔0%^'`v{aUXGN3z	*yd40;dVY=tҡuBO>;zr6?m[hdxɱl!,aCòiۿd´U[=G&q6+fH
>=uJpĉzbi&k8mѪ.mhf	߾`KI_xmb`hI}l]SL?2ƀ<8pxe/=R:Xo"l|K-HQ]SOFxW	pIvWzs0L_6IՇ4[9O/>Άز=;z|{`8on bs@	Fܭ+3(9hů
TY7>x$^+	j<hž Q;/,yRwpGD4 S"X,K^BPOt%!nA-ai> ,L]-Jbt;
+;DOMu0kItALBq1YgSX#G	?fi~d߻7vdl;8WѬ鿡nz7S_>p0OЏFa't@ DM\e<ޞl
FYvͣ
Z[g}p"b?bc̤C88L1RDέw ;:iXLS_Sũ˜~QeHdfZ#j$Lj%s>0q3L<̳T*ߦLkV}vcy9m>zsc}c?[.k
5zC<!334`^/i/I*LF#%L4ӳD_s+Zr XOZIa@-ZD5QCclA%0'Q剜''m$<Q9IHfcYtqwRU5ʓ\P8CbcrW7^7ako	0[Aߥ{_4cOR2JZ"۳F]{[\uEX`صjuh*
9hN&kۘ
s`1JHrpKA,B// Se+a J*C?՚B+Hd.YhZ\CtyѶ7ܑ^w+>9Eiw A{U6`ׂ!\3o}lVL쇶9aI+VW"eFnZm1Wяث&(`r
Y 6>4nOZmn]a{jn1Ɠ
6{K΍sM
Fh7/:YFn>ַ:O[D%*w7t`a'{=0 .tx|5v8TLv9b,fLIC6,G`MWA&MA=7hy!*GE&7"wԹ2{iAi9p8P/l-Q`n7"J*d*i(RA"Jc9&eytnaYCh?XONtj5OtjDpzOj:s~=0kX8\>,FGNjN}q%/SL}
C}=ݙ-v1ؾH^Pt7zlSeʷ,Ag<süT{
BoR^N1[l7_
9Y%Y$`h>SD_ԼKDF0UdmC@!O.'UٛLJRt6~?ۣ5:c,uᕖb~9oxڛmq=s%zlcr}
$ðW -hWz2nP!0ļ4#9^q
u&7x?K0G|\cdfq_?Kk
xtN̗g$qYʢN:AGf'.N 8ՌJN1^twf<3
2~.Hse1<wXOl@T
sZ`Ҙ6l>g.\nx=$H<oH%2:ÙؓB^$ܔ&!vxCV"}=G6gvy`ݛM(NDsՁ?.o>2V4?fbCo^k`,f҉x,b嶲ҶKO47:\ϒmφ	+Gi,a9~@,qWtw	-OͱR9vYiO3G{
|i#qanZH諡&'O/<]i :p˱#`^~DQd	*<ע(<^|0P/
`xSQY&;~ɡfێtǃ%eꐇer~x !3|̣P|3iB x]@ha
cafgZ7Q]{޵w]{/X߲Ykڻkڻkڻkڻkڻkڻ?/!/,i{ɍ8g~?*|yGo.GaÓpט7>e/TU]Wq-2mzZ}ֳp~ێt;:oܮo;{?n?w{޽w{޽w{\/Cw +r!} '[?F_ 5g ͈
S3Ȃ.k0:G/i0-
PGuhQP`#3lB}̻4
0_`3[͂ZgP^ZA$fAY
ԮByq(
 tf-C}
K9	#YZ~X+.h_f݂a)G *nV=pӚ5[EwRhSlir gj嵃B &+5w~7pIX^.i>C%^|\t	.PsPJG&({Af\X;-1	_!WD":	-[_>M-
t&wOEemz^֭/}5(h!vqjr_Ewz!\ޒyH!'r] /QmnNSY8St;w<$QbyY:~ʖhzZIj*}j%Y;K;4Or; :Շ %(mD+Abj02PJ2]\FiL DªPI"huNhE::;3D-yRxʲ5M~K/P^ޮ'vz7m{4ܥѤ+?Vwχ5h"iJyH	+PO$?^B;3EHM?:~ (NKOO2BkF)pQXFǸin<La9 5/^l}e	*koIaAs/
7Kzw=Frdu>ᬰqni	o>pO3h~I@p
endstream
endobj
83 0 obj
<</Filter/FlateDecode/Length 12>>
stream
xګ? Gn
endstream
endobj
84 0 obj
<</Length1 15697/Filter/FlateDecode/Length 8919>>
stream
xڭ{	t[u{NB "n EȲHMIđ%ԍ,MғӴO9N4Utqf餙s:L&Mi3Ǔp$7{{FUĢ^I2|$eG9|B^lstBZ,-40B.81^v+'+o0ʹtsB(Ñ5KWVجоm|iyw?B¹ŕѪ?Gwy13_F<b\{
砗103="рP#4(sKn$`22'S+ڙoy	y}s[%}p><Yezv[m}=N7ZGA
>~|w!/7m|sBbt^@}:;\8M#3ȼgdN:IEEIl3phAmIipAe¥/'*C7F\RŊ*͝L"@ReiUBZr3[y"|iA妯`Q asOPݨ50&hX]ST6u U.rF[ê0\lx {
:=
]YVU,J6p.%-d`ޚ.Hs#1:1+^Жjϫ@AE9 m'tӎ	M(JE'E] ȹbR	8%IOTQΩ9 *Pu&uDN"D\>Vs*`^Z`>)M{K(Jj`ƼD/:+IUTTc>q1
Дs2+̉*FT1TDX:!\
QnM

s@qvGh5DJ &\iҐjwSD6
ՂaX+qp*eaՅPR+*IjU~  j'вS{9`";U:UG~NZT-V+
P1Z+IF):诡NeU֫*.TGVn|Kn.50,kd +^m@$"H2
BvcÄȠo`\
ZvV9iXYep4Zr9'ҺӐPOx.7J$UɻMu:K:G^e'o.w._Y7wn$"
siYJ-IU2>
&F*ڠ *qO9A4Q[|[;w#GQc y7|;HaXvNԤ)-ڜP![!F{XS.u$#HV6M&y.@R$R^8튔g`6
g5!lw#</wc?K+v*)OoRu
&BFIJ ՎʣC
Aօc4r[ 4L-(ŵ,Ik0gv4)ͧ
r-s$dg
r/y_"|}1G2	L)9U9]-**s9vҔ5XFȎeU`]*@cp]0ao=n[օ|Dׅj&:>"E+*$hVѡBJ
pwJ/E3nޮ[K&.?|\s)rY)Փ/L{aOz세6z;m4+;QuORԞF|'*4 0gD|	
&:qP'?`).=ˋL$ʐK9
	xOId]/4
\ZC5^R; ݣ	E$0[)8:	~B# L	0   !aC# )B.T S i}<LcB<
'x%k0G$@IdM ,,$8X|
tEӔ/|,@(_Z|<@@ǽЖA
p(rк{sI	eu2ôE)h xD	05 p+=A[HЯj A(uh AxJ	Ӏ;Pڢj ARGx5
$+7̴U
aiKE
M_)oI(e~
gM(=YKxGHxX/xiG87>3d^ݑf;
ˡ*$* ܖ#L{GƸn|楗>??|^޿5z	髌@g2H4v(id>ˏ5
?a_00zH,Ȅ$T
Itf:}iJ7xmȌBVbDS<2c3ku'<A"Kp;` 'S`*)#:O6<5oatJL0-7c;Mydw8 XL#L;tU:P_#+ƙ/p?lfmD^Øgĸj.dC0!]Er䖇)/+e9Y~ʛ
oA@ch۝FrPpQi1n!D\}<nm=1rn5~镁B{w#ɎP8)禖FgEiCS8f9P%8"/ eX {XB2*ʶˍ~on6jmF͎ԐݶºLUOeЅЬ,"gPg+8=ޟ7峏cs`gc$g:D*T~w
:EVA9Q#ړY&D"# 0kpV Wq);5էK&iG%$l9_e'S-AF?s>7m̯T9Ē#;v-K+g$̊.ʕx|\OJ"^\ZQ̊~Afǝ٦~o6W3bփQ(P_WqXL`8o7YfAA<r4+^1%jicF<)	?wڿ/yIlPHYG?v~Q0=;XG;*cA?W {	#a'UP,FF 
sEP\bL\deӔv_M3/kTH#ʶ 3q_H#)MO{dp3ӜF@}mlD#x
)~I"$L_1
aR0~FWۍu'ԥLͶ)Gj^{3w͌>{fؕ̋O<r1C/]/-~ݣG?m\iv'JjT9SDd7v
fv]90l&!&@k<2;a5˞e! "C	
Zu֤k$zFe]5iό
Ya~
Vo|>ZX
׽1/L~5d]a%`qfpC4#k)OVEx<A# 843KI`L ǫVXRH9M:u 	fk]y3K RU_j¡_otP(jʸj3㹡jQ:~wh(7eľ	{%b>̑ma9eC8T˹RjQ J;;	2۲wRW/}hbC^|0lώuሟ0ԅsz nd~mk%]v9-XA

2gu+dM4Ƣ6چ1t99}.P
1jAVw7]q_e'se-x^|׵ȷ5wG,1B`
	^Xʂq΄ƳƲ-	~]tqؼ8R ZY^TS
7ƚ&wϐ|qg54o~Ō3pgn6DfE?㠟)M?Ѳ~?tY#6ʊUioSNm؀d]&W
lVD~
&΃ƏOd8͖?{	j(Bf5#μ$ 	Gqfe].m-I)	iFΪvE2떖VgFg.?.j==e׆W
{!$}BAjFMsN~V}r=L
ߝ:t
A}տ#vV؄fwjJ$EdjXv*n-9Xul+>18۸e_h?(3	_=fiJz8mW#`wNg(-$TTaanۂWɰHtewv\Pr7Йzx|Ɨ7e+)#`@vU"lZYdMS2J&g8o	3jp;pSJ9L~ 1enZ\
u6Ͻt?p}z3sDd+Q6̾ۿKD3h;Iol"ڂz]1d%aP\JBh[WZԑwonVWbCq).7L<4r@^9JEc={KĠl>\O
}Gr/jNZ}.-OhVo,	Dp7Q<+:
z/b{tU9rg%R}p:<\"LNkYD_Z rZ@1(	d(RqnE,{Y0h!
)Q`Z_44 kս`
Ѐ%Y8w?5|j1J4ST@jng
^<
ᐥry_H9*ۘNizHC~xAK.;H))l5a5qN?3c#{YQmïbL㒨a?ςd& @PÊ"*1FruQD4SjI9୩WɅK
KP+r i-_.е=cn+bn0#GO,~-$T3|6٪d*xY8pj~#k
\AWEmE7Y5oGzZr(sѱåEWWWאt
O8,æ#'=ާuPtbsPե)c`g	)`g"=Mj.oHng*;aw9(	(A:kv	K4(D6r8zgNDk	~扸1{ysI7R~7eb-KgA/`c6v5u`	`B!%_Yzfv1rܾ*İ
K-W
j(Owmd:sD1]ϵ}8p9XLda<rp^2wLG(j'p83ELv#Xna4}ڂEr)ğbA0"*wT2(P:2D^)d[Bhblhzxz+ߝOģ_ڎ:qɌw[<9( ( bxJLCȠ7U<+[:N2G6w둅c/;kE-Lǚ.9|?}>7'dS}Re
O3XV3{ott
tg7[:Ak@̓(ȱN{Αx3addʕi3jv.9l9d$Sɰ5M{CkЦ	&qmf	͝@nH	D8`3I7d>UHeHbm/~(̰cg{IZwDn\ܩZ,Y䯯uWY~rX$PkkHNRP=AYÑ`0o1|pq3juἍ2h0۟JJ,&6u2ry#@ɴ4zGtS[-w9rR.oG_}ajUp(:L}G-ko:1"᝱Sޟn
_q޶&eￎE2#~V~Ʌ䷆,;p!W8qJT'`-'$[Lv
9g˴aKD'-FQjbzD]`ݖfj]ϴGWnkXoYySFg۞+7FRƉm4"c
|mtt0w&;g}=V,MY&I&~HjD
FDΝ;
]-AF5Ԝ7+i;aX=Z	vS땵{R{czڵkמpsg\ t	(J~kcJA"
+2) hn{P {Eī^zbtS&Bj7Ī/Vz*!]3.=:kk%i^͹,^:=ꆃ$6)D4L<TqD֜&S'Μ_ZCY1w@)vzvSTwqqICEW-vlŢ8qWAy1ec	םbP/م_5r&3tS<pR
8[ g؋;ΰd3
VJ[+쏖I}tCISQVOoG|[@2:d~z:]3>pw$2GW&u-Fiw9Vv0G%\)&+R1jU6Lԯ]p3MoC%?OKƪW6Nn3p6]ox=AZb=L-K.&a-^X#h
wʞ\jʎ_	9rVYM~pR1yc.-H!)|WRZ_y=śٸ<0rKqf6%*IhAJdcv, <G"?I`+T!Y'Z%j`W5a3Yyx@`f!^p;tןƯ6d2gC}cYLe㤦s[Zѫq5H~8&V')(@DR\.zF5ۈ0SVb.P/WnxU@vFwj J5 fm,{G?g\n`}C18's@+W3f}]]ot;p&6N=qviHd w8EWo+ WC#7'fm+75w?3B#m@u848|dcO7=f~8@܁5,MO#mHfh!?К~B܇BN1=p( ->xԉ`"?C3#'~"3ǡ/_Y"џg
<v{ٿp' <
_>OO8)5
7-)˗,_Gkkm{dq?$czr+(Ax_xB~*%8Ucqz&0ٳ0hUYԆa0T`2t؀t؈b&c6~[P/S|yCmN
z8;)t	
2Q+V$4
Z<yhz@"m-{z s
/GBiit}F`vg2&~BlRey*]Lg8X穬10FMГuQh30Guh+
sxܗIy"mAA*wLT%** {ޫ,]knI0KT%>w$%V	hJPH/lӧ\==L`m. n3Y@S/X}swx!qڜr_^Dw	f!\h}z32NDS o<mzvz'VK
g}	ڜ%<H{`׃ :ׇMڃ{
ch	h?Q1
kc#4L"a+M;u4J="qʱDZbyW	ttaM.RyMSK헩kX.S]-rQ߂ne*qQz.S.<i1y7xˮ|7q	Ɖ'Mu_(X+HeͿEKQ83\sD'6CH4qGHtȞ4l}V---c63 ;2EUz?7SPH{ah=F7
H@CEzt}p_ֹ&<*pv!m3*wmACM
endstream
endobj
86 0 obj
<</Filter/FlateDecode/Length 12>>
stream
xګ? Gn
endstream
endobj
87 0 obj
<</Length1 11410/Filter/FlateDecode/Length 6546>>
stream
xڭZyp[y}@$"	|x<@ER 7	 $(DXqRq3&ii'yI;IdNugt$#m&SvYvi~}]
#
mĢ֠/[_m??" +[peqe	b⥛g
2u"T7Kn$w,AoG

O&k\<yUI++?.fV?u+B½+毴|j.B< &aӨ
G/xa7ˠ#tavw9q>qQekduDt|3lL9eb&Sn~ŔA.۴gܘݷP(^B?޽kK\sh=]:f<2qayvo{̏GEq9z!ɢ})Q~V3Iݩ4TRaܙ/INKA)E(,+bz0\>}jCbt$X*L䆨% %6i|npRRlY$%-;f肜^M{418ŬP8ϙ\SKaݩI`vn&E%0p+LĜ
5@VB8_M$E@c3#*D2
-"3Pig:J9-SDRAÄuRM+&4G8^l*ͤM4
Rb")"H3.H*:)X }@$lN7I'QשOފ>7M.茊&k݀Љd:LRʕdMw1P"(삂@ES
dH[jqhV$3(tcTZUP}&מʇɨ΂ BTO}R !RiLL]+u0
9U;8X
/2"VqJT8qc>%D@M@DeL0s4-nERyd2ec:h^)yDrxRmtBۭrFҨ3%!ɕ	^
-Xw"#M0,kjrI0,O;~2"@~ZLC"^Qnal2A&;TJاIr1geeE"p&g{gZ:ڼ>!0) oR9r#S񤬔s)䜎rNO9W@J,W4 -~'SΫj@g)H)񾣞UUs~"URHY
#eGFЏM)MYe"$=RB϶JWixlPRS"]9DsB8)c[_"2p wi <<}o_vdSz\;]; PCd:}JH;}JcǞ#`"dw~vps_.H۬p. 6ҩ(F~I7aΣD:"H<IN	HD^t<|E*B2DGH"D4vEYIaSn.q&1
KBX(]{"SH"`۬0#M`eX{4,ngK1O%V݃("I
:^k\)
gՈvZqc$͕&UΛ8OP+h2=UNs-
q{P3q7б6"*+]w[0&+G 1PY~FQzT3p@ |yq9g&?=toʋN$uK+	˛Ge jG.IEfA=F -~|ڇa:l(!Gd>[ƛGkL&9.o!"&	yӖ	 h$$!!)C}ȅQ@aJXm;vaB#|:O(5M(5C"M$DIY&! YCyCCE*W%*\@"E*.QB"*P=^5%U<5:EvZgM%	
ʃ5uܳ7#FGl$qS%	`JGU0<{(TVI8PIJﱽ5!$O$a#5*I6U0<#oғ"88KTī.߬}pП
n,ҡp=ؓeӈaGH'f^pxf2켈;`^0Y˸Nh4i9Pe{?L$
xAe)ۡ(؁!ƙPcShXXrd<Lseϱd2+̼!AC,6UTLm4*0ycۏuⰎ{TZ>c!,ÌЙQ _{rvT+;:mvl/
M;'/O~ώF4|S+.̜xf跹?&S7prrfXb"G 'dZDgσH zF fFՕبP#n^SmT<)䴑yoVO'fN

N	LU<}K?\3SvPw+1)Y{сd!x{ڊAm0BdCmf3H TAKrsh,^\j++M&K;a7p:ם{?r'v3 t'5(>n73o9v,rbٳ WQ{A)E׼9es(j0ӵ*gwbqpjG
JH|[2ˏAJsp02Q`qXX~Ħt:jOКjR[0Pҫ{aŌ.o}^fq9XMfc`cO6H߽7~bmãɡ^dW%- w4*If1ÜcHH09c6׆?-;;?cޗ~&?eqe8oFMAoUJ!tqn6m4u;;vM]Ѷx3`tsL:ЩV@[vc}qsH Cȧښ̎e)9pJg3Y'VSGոA3ː萨֪,F3h1K3rTovV{MW
C^16ܸ7XY@W&Z)wh61qb.0ݗZ{	(fs3ɗh,o9Q=..2ȰRHEXF౰H2-A$;<f1g,4>
]*\^U䪬w-fsbg 
΃fЬ'u5<~};&x̯8)7OOtObY@nY*Ϧw>94ZXHu&9żЅ.ldǚUѼZc!~ d
;vve.lئzI'A&x{& ?xCMƖ<qu074Ę^[E@~
l+a

&	BD=,%EV+p$S<,cC6؁&֟>2<ZkSSXUuG
YJ%YƋM`VH};?ۚ*{l*!񷏭q ``s .۸vs"0\jه@|̇?\[|g[?P=߱tkE-
-t_3
XnQu80rT9c!mv!m6W5mb?6|s:)
}d#O%YoOHeք'h kl2@J pΏ4l<'A*3[pΔg\kKYU][1ux'@!@-D=.uB.)^4EdMM(LU-I)< (DEݧ/3®ÇcgG VUDBk(ذ;;r8xaBPoXi3'!hO 5Fn?/f["3Zk;h&uV
GX#h
q!Dq^\H,.8=@].k\5O~?QԻ(& 5zcA*0[`oW>]nO':'G]
;G:*94^!~g*yR35?jlfb|)mEgd́ք=pj
UwΧw~}BS<@~+<K|RB
d%I@qTv"<ߓ
AY%KYsL='΄z&Lٸ{6>;4ϙ.va_R`m6A6f'0Kpv:x:●[ǯz>[3o^YYLD
QH6q_,Cy0O*l.Y]Y74>bVdD˞DbsvϮssñ`秬~^~7n^\^IYAznХM`.T(AQUhUˣ^GL#6Kw-"A\?j2\x>l0d4urv]}?AFoA.]M0̪ino߆sUH^<~Fj{Av%M,C
+9u6D]Gʏ4<+RӤXosV
!o4n6J/_Ϩ6cؓ-	OABf.g1
p*e&#g?_:owVt˺#Qkz#;?4Ap7'۽XfAjK 1Z+9.>0cD2AsȊ9Ez33ؕSi<Gyl
2@"
W$[m**W_lbi.d	sW
ˋ"aeLΗxCC
V2=\ȱʌe`3+
9thbSltm֪$o2X|>5kz]"LI/bWȯJ?~wn7饓~/GGt?ә~0$'{݀oJ	vFOfB!7-^MVZ/ZFhX>k8<𼀾;:~3'WYo*r+B'}«:nH=Y8k .3zOOJ.UBMTާ,sG9$j4sPFPzk4^N6BQ7F1/]-Et]k-%DajEA2_BP'
WKPN썺NkP\^&#W)(WK
'#ڀtVXmST 9,,k'2M[UI

$NJ]w tZۀw+jtD5]*Hv֒cLPmAs.unkWh HqXU4P>Ie`V"=4~M޵<]G} em<6k3_WX}vd!qڟ`kԿ~k0s~KPA}F YkD%;	@9NW]=4ȡdhyS{/eiF#|*:gk!'u'!NR9Nu
SPq	xA}ѱ"CkYcCTt szu*5
լBtw\E!\ckr1J#+C=yJB[了_V
Յ~?u:vu/nB
*
M&5&V}0ĲWhS.AIt\~~}}")?KCH,آ	vPzSO'~׬wq᪐NQ.ʄvDoWܶ߶RdFf/.tl@(wN$$gcREK9s0#wN;'\zK( wi$^_
}J1(v
(_#
endstream
endobj
89 0 obj
<</Filter/FlateDecode/Length 12>>
stream
xګ? Gn
endstream
endobj
90 0 obj
<</Length1 10238/Filter/FlateDecode/Length 5698>>
stream
xڵYkp}߽@Eu% (	HBI")	e	 AQ%J(N,[~tcǭ6x>~8qtҤLL!Mv2L>diq; Rzw	%D$}'|
㗝֬~Z̥eO"8+?zkq?yv>^LH
=c迃~+?ϣsg&~A7XBnt{'Ws_xk(7.-_|}BZ%7g
iPF͖" 2IA16
;!DllH"Eh3}cV-;a{_+
[Mh^._mC4=[BA^s\F0wbC3CfY76\}mUo۷#֞w1|+tiZM]X/7_
h-B9ΨydPOA-=MBsvb'ssl02H ImhP&"
5{&bBCGIÓTDA&f1iəը !M	e|}Qל>WĚz0d2=YSo(6|4'CPͫ;rbV&
e82i0WӪ19	RF?CZ0!QHVO5:x'|g29pLlP/C`6a-"9gAFsPhmcod62
g25}D&bȺ
	t%'ӆMKv-`J6b(ܰ/f*dM۰g=bBIg'LZ˄2Nc,b1lQt]-!\DftQLZ'ԒȬV0cqiZ$R[\!BgU-ǜʍM!E)Z-7dnQyFfm6Or\7++8]Z(ӎ 0lsCåUU!  <dXo
=QT`;dV]ͪfL 2MF弶1<I@pW/t:i\pY!*reP?|!6O|7
c[W{Hô"l
2Q2$Snu=\X ģ^I]rotR LڨDUjJCc4WƋ`#凎p؛zAdm^X2kmz`gvPڰo(YXZS}eK7
KM-%sPՉQP-SUfBO\Џcmcm3cmcm+cmcvڨбmMVEѣ$w)0bS7:FRM-ׯ~_ ӾSfgtdN(2-{6^<;u5%a5g{"m*;W/>k/.?%1h`0b&VwE߬F+	0ՔB
ICŉOv $fV 	<_j:5woeSz%ܪe5%~$jME$Xuhk|65e<d62UK\8*ݝsr
_50N,GfTEΐpGVŊLf&UIo@,BUnl
L{JCh))ޒ	2
28ЙQerY0fnN[[X$I,ܩrqԏ(H'8SLI[F[Fw{Io!^l,Ơ=YШщ\e-s%LYjH(2\D/8p|̐NE1ӉձA
jS2)p*i],mJ&8̴mCb9}c80̭-Z(Lp4I021o&@&5i'L2tq 4s d d`9 g<30\C@g\r1	.Cq:bE.Ca/|؄y/eϲ	C-#gxԄXb	c&dٟ0!cnB$fZO1<mBx{8s&dϛL2U2r~5$hB$}&WuSkS$6o?EDT8N!6E&e%q!woOM[z
 gX*bE nRkƯ
ڸ17M	k?܊y%;ȡxJr
F*Lt{c/PQ8G$IY΍3r
buu9+
Aw*a5tvhZFugn>ֹcfRW^u_h(ZSwvDݳw"@6@Ζ

vBWV|ts褆hwGtY	+`%8L@Șl*vWd炘#SY&9&pMe!OZ|6ӈxm]-%۷jurdRCk`@cKlgo__lhZZ5E]c6Ey;~eqpRsx`(֌;O_7-wFִԿw
?	/UN5Er7|.ZE
GwfnPE=D"kaӄe` %ۂPM崳ˬlCJi6栳z%E;ڕ:Ѯ?r?ֹ{qPxhG'[Tiv8,C()tc霃fVb{
U)\I}>/d-}>>vKVy.4={29BG #T^J$huDt8`nYdQI"9QLPũ4x=Q&P8F<nWhnF*ӟ0S\c?CѠ,+/-6(GlK~SOjk۾K[TێW\>5z;O[?FE	&1 i,`)đ-PP`K=.O-?Y!2i=ޥ?{:	Ǉ{@[_S}3-ߕw¨ݫ": K21YΉb1rĬYsYza~j9'#Ҩ1DɎP16qC}et_/Yq{՗RJIݳJcraszb7|~Jcn|n+OйduzfAZD;NM79/w)2EXa`8ǢԸcv8ȹLcSbwJ1)Џ6vt4n}<^|_33Goqv֏5g	
WF6	R!U]~İn&@1St˯?9 Nܺη>
g D}!
)\³MSv,ZF`4ǶYh"MG47*R:4ܦ3oYUkN*ơQ][<^l^kX"v҉%R$'B<'cH&^ؚ[*mb+1qdr!:7+;QIH$޾i*Zﭪsա`9S1ST	{;yӻzwص1G~+2c'gG>wHn*B4;Lrj媂 Ίrm)[y2qo
Pj4l㇟@L@>OIҷXk;֕ʴ_:XWW΁_)vN<wM!=5qذ$P.ģq 	$,
E	3TLUQؔʒ47doI)T	/]Wnr{g]=0<'J=xInc6B++8E1oҹ-6y5*1K? u3|~EO#Ht:j޷+2q~k[wwB>Ewz]mfƙ1V(-pEǘ,#*/T1uӎŴX__ǧA҅O<tF{u5m)ǩ??pP-U8,H|w[լGHܛdVWۀEeY>$onJܮ*I>źźq0{7ϽyI^q,h~}{#HK;d" c%	F:fHyATZTQ{<~^<=~}̉"Y?mW]]O##{q^Hg<.D4ڧxJaKASGdS#+h(V7{hU/aY(vGBy; w,t&|c.W/zfS6.@1Q[7\3f+mG4Vￛ-<L~{qr SA},wIøwoxFZϻ''vo<{q]Y+ʗc{AHa57ȧ-h%|}Ie\TQ}"'OYX"-21ȏ-F]_Y	$!p9+Z
ߴppS,$;H\$ȣd	Wr,M:I|=<zcx$
hJ.<yu<8c2~*&9ȹ_/ypuO/ٍ&𤀊3"y[[-q\lDݲ˨9rt\mР W ӷ%cXe/w27}T."e#*y={i|qs\%y|y|ǻ{2<3xݕs{5^s7SU<r{~~ek~G2xHGm"x;ПoEϾpWb{r/b[*̞%kG;"t<2Y;
.nYyl
:(w(.gG<oy&63w#2w{we~.48j9#>A'\)hI(F[{RYXfGe.f`s
?]U\susɢ
|-l/oׅeqWR=
.K&3'^3.CbKFl20,?FF&ߜJ(})cPO
Ė;Yfc'T?j?gاO갇lv5Sq)))m2Jw4\#ąC~c(!aC_AC
MƑfP߶@2B+#}
+R)~cn8ū?<<BvgtYCtA Coy

Y
endstream
endobj
17 0 obj
<</Type/ObjStm/N 68/First 520/Filter/FlateDecode/Length 2497>>
stream
xYr}W[ĚrJQْS`@-Pcs. jjI4@/w9}3)PIäLza2exb@b2)204S),hU,8Lp
NZ˙~O*Z*q M	)-r/dV\%r!#3#%tCXyIkShta:CN	`5y-^I%X%7aøz ht:kxwЈ9w`+>7dm> 
%\ba'<ia&  0qXBk+:@ /c$viXGFx瓒NK!~ kKTOk'@ګ'4b&oΓL.nN";:Nnݘ_${T«޹uaTY>8i9xpWvdi=At_ɎYZA(`\j3Btua8miP:/o |5֯0-j퉢zhޫ:Zlx=VS0F#0*6t&Oq4$lHcr:i))t6!wݤbgpo !-doQ٨M)Uzw+`b;bQtU'znF8ww6r4lЪ߆?%XK^7yr84NJvȘ+Y6+pg!4Ī2a70@j#o+ݣdc@
2Nꗾ~VAdnH7}3Yw- w5Wad^֖qa@FAPmAЮ F/{NI]#i,V+i\ₑ-OOTٻ%L29}-b4ͼN@m@#5cmos>7b`r?Cl1]mLLS{/q`g'kPCf▗5dM !-L=], Lԣ7#D9\jRHE;X,rRl^n\7/{.wEn҇*_/DV}2G[J2tkKz:2ďQ}4u(>
qxv 	[Sxp7,1rWԊ丢Csʬr2}h瘝j;6 UAr/2%_=%GC_cttǸ] }	EjOKgk^s4;Sl&DҒL"Egx&C*͔IvH,BE$
lVCq,J_2RY2R[$@e֧:+Y*MAu6Y+-=K9LCJ$_Giz[y(
dRRˌ2BqO:Ve3A]`):Zz 5ǣOЕZgĔ։i@UIr"'-$ETskym͠!n S8eME`{\V̾8wIŧ7\eދx4O`#4sM08}/p1˫(t?*a9vR&CĜQ@#r"R~h$..lQ+D-$pMP~]ʠl~54ߎDW
`D=wYr?OǑ|ܿ/︷^ TL"|-5՛>)l%s>G¨uQ=d_}h/&?av)wL7o.ߵ,8Uxgf+e-8\mX2;x׆4^F?4EƘ||:]UO|o4< /vAJYEߗf-D:Qv9]{]v)؇4a4NgױIiU%sh㇓/9|WY}_j/;XS]~\܇A	%M6&8F$"'",2Yd0j뫛cr
5Q;ubos
x;1	*vv;Ā|~r$l8%*Z8Z׷.h6,\&F_Xq<)LcLF_cR%E'IeW2Aє=-x.mtXAW,ht$|=e`?EL_'7ѓi&X(.7j	FE_^~4Gh7q
,{
bCKG>^^/_osXL]ؚVDgjSlO."j2fK#	
endstream
endobj
96 0 obj
<</Type/XRef/ID[<20b6e8c709cd9e839659f30150514939><20b6e8c709cd9e839659f30150514939>]/Root
1 0 R/Info 2 0 R/Size 97/W[1 2 2]/Filter/FlateDecode/Length 247>>
stream
x-9RQn\mpqWRAlVyb/`hx
F&Ww`T؇]8p1$pgp0OEi)lhx㙂f_= 	ai(,1EX%X5
B򰩹~2
F/jfYzniֲYƧeYe+lh;{	%Ͼkvl]K-:޽)å#?x$E
endstream
endobj
startxref
49969
%%EOF


================================================================================
--- File: d23/banresgen.pdf ---
================================================================================

%PDF-1.5
%
12 0 obj
<</Filter/FlateDecode/Length 1902>>
stream
xڽXY6~УĬxS}뱽m}@+^:6:l}gHڦlmmb43]BhYUNL
տ_&R4aTvМ'kITr-yTj]7E[՚g4kYo]M	oEo[AAPJr)A	"I\rėmQ? `k tHAi.ү|4Xdؑ%T.qMsJ8!FJ'֔2Юx}ʧ۩+8b
d XQ)xZSc[TjY:& [g3/+輔hSl

"h("
Yn4E
<DyC'^ʂ@ D{DI|ӻbp0oɵ$,	~Zpv㌀ϕiQOcjWj$Oʪ-/H_eoNԱ.(/7+olۛ^㊀#CZ<TMUFDףgܤc?V>=ܢ~=_!Y:`Rj009wkG7^}.UF@9sQyb<+bOpmzd	.g$,@,~c7q\wuPVp8[| sqSAx<_P@G#	^
B\<!_0K	 ($ LHÙ/?ǌƌrDٽ-G<63LCm)b&A8T7iEKuʄ:kΩ$T7zC1Y0H*S~U
c46oPE_:uDq&{ھpW?zve_1` 	|LX
oWh̏k&\U!xXF偷ڽFjgn&HC,ĉg2=+0&YA`D}qXp&E*Is	6=c ɢ,x^LgȼC@Um=6A	}z&Ѵ?ԎuP<<}}O>TYh
X|MLcAiߏo-(p3(\~Y-rGPP`=v
u
+g?DDhmx]w!m]РCVi.Qq:V6S./`mpJij5vFTz`-mO6G%-EխXv@Jh4 /;A#vGqrIpyuUTC2̹>n1Ig)u883XS4$^5YxɊk4YE}}b8Q; &J8T'o_.hrŒCi!q	8;a'PK0T	ܰXA9HHA M~򨓥H3)ьhf<f\,ϹK
%XD22-$F{a7Ď`1:fӛ)ӛy$Abwć
dѣ&7g2BdKgRљK(?/t02ΨX(~BXyR(>/%R̻Ve|qM#1Dflh0ƥo Dn
|jEC*sC&sVxRa~ӳr&5@0.`p>Ά00} NwpwT30AGiM\N xWT[+(abrARQ>v|/
endstream
endobj
16 0 obj
<</Filter/FlateDecode/Length 1753>>
stream
xڥWn6}W2E6pR-&N_"]QwC)Β99sfU.V.)VUo_V^fZs\]mIO^HM(Vyz-yW,M}f3J0)E4e[J&Qrڧ?w;C4K
}uQ)\m8,"NR&<[oְȵ\'mLpΊ,#9䜶2ʤImI Դ︐Ʀ<?nߣ⧺ۑnhj*rG}[tv}R&pl@K8A?xW^%Ci	j59t6^#+3BF9\ &XBF
 |CD@maƺ"w&@O	fx>[kI`c܇Ba~$^6{ѻCKHA	4\Ζ8*8q7wwL0N΅0z?Ƙ3XLWSQG숋9J?ȤTEyH D4jztn7}؂GRy
E=.lȀXqȲ2`fb	ph?? (F+qV4,f<Gx}08G\h&Yx}x2W#BM4-f9lJ- ngjʛ&0_B,5~\g
*beY^bRio&hH
EBu\ _GaӫC]PYZ?CUOhP)rMǩ;Z;upuJ?>n雳xbQcX͵մwU|2A%}>ǽQ"t"GKe5n7
t)O%R <29ߕ+H k4Q؈oۻFP
I~8:;Y%>(=< D[6cAIEPPO0F%pLg@/j GDcTL<y_ޯlmC[QrU:1WGo	0({lEQЈ](c_(FGf&Kה(r
Ȇff8Kp5+Nj4²>oxa_z#'<v&>\$M"OJtƔ~Rc!.¡aB]⏞х_`'G|,PLawhzUt]Zؓ>}ƃ)JVa(/$	" 1QixӡR:XBp@U+?t
:@_D
T\!媊)pCHB  vcA9\3ȶ GI{| !CP4R=og1# 67ɵ
F
M[.<'p	yy 5ؼ
X;1W/tiLBc+uv)ux\pDl0<&tQ*z6y@O}g{IS>S lb!=:!TeJsxd"Vn[j
D#Td[}d |xǂ%0B>`߄'Shl^KD>wA^;RRQ kB<iT(֭c+pe.rJP>h
endstream
endobj
20 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 2214>>
stream
xڕU{TW1!35C,X;tU_ǭXRu'oC@QC%!hHU|"uZujڵ]wR{{{IB*%H]dFBU)M9YxV"pg~u۔N	:P. )ir$e+R2sLRjҒ+L3F*c4ʙyѱkUYk+$:eL|btjj2<3eDج0E!a#-WUi)b|"X@D4+I"i$A'	?G|L$>!f9<s!H^Lw?$r5%ˑxI$NZu@\TY;5:NO=^	gnnHД,[fVƴ5F9yZVt=Th`g{dDL
}ø
1ەnzf\@`X`"_XYkyMe&˸+utzǅ+QtK"SkzhP`f|dFC
{\vux+K`zͬ7YtW*6cʬ
jw 7=gZ7Q<yGH8I%Y-@a+OɗQ%)~Y9S]&;)X`(ȋEӀX<
84	}<@!r~Ӄ'o`8'qٛ0'E)g7}pp`_쭖],Uܳr*
Vjtu4OFN8iP/KMe[Ғm(9l&-a:qzZ,IwGjQOG"
EcowY|7K )b4Bn=
b05?gdH2ϜHv0s.2WCծs(]"DQtjB8"?
.{%=5tջe;7pE좄NER!Oa)5K!v7=
MugL;cۀ~'D/k9®ʎ/vqdI,	YEhv.{9.3[o(1bt)YQ@O;FpL"}YȮUVrLQ9
7{cM*Az֦巷6-.>[;/\zR
{lrSSg.p.3tM:" w
]0[I]E3-@Vg#ТŌ"мN5*ǆl].(Pq,:?`$m{Rkĕ^1'<SԶv8GitH(KJ	Z]9tÑ`#N/.vCnT('ǆ~b(=Ƿxݖ`Qq
#mwI̠<E2Jm+3 Vh֚2}yX]4L#Z㪯dsSv8_
p5a2Oѐ=3}CvFzچu.xvE`]I+totm"2mATk[lΜ߀Pub-΁+5Aw#S!faN,m1ױkË밋7p9~⥫esX;BoE87BoϿ7b"o]h#E:$BZ@?aũHDlpRVFe
ݹ5pQ؎=Ei
t
/ğh
ݳ7vdbFj@El*B1?"}0n!\AeS۞r4JFeXf%{w _u/plWyBT73 }+R^[N53i"eoג!ݿ,E4dӫ*,;]	S6t
G\9e53ű)MYV8G}P:ָܴl(z+o琔bzHrtt4wU
s*]q3stʼH#Ʋ!6+Q+Ra?Jc4f0e4q&u|u6ڄHUfXwiOwUFd*-niWJ&SI6`pQeֈѱ
endstream
endobj
22 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 2647>>
stream
x}VyPgnEY3X1ݘrSBdS!A"b"ʥ30(sfCf<4xEc1Qheu	|l=$&$U~{'"Haᑊ8y#)q*E?EĿ3<56?[#~ D_q[%Mp`$H$I̙8?Dڞ?/00hl5?8?4nN&}v8yh@xjFxMkBr\J"?*a\"2bk//V+Tq)3YD-bNDqDH!*lxBB$b2:@r"TA|@!Q:"zAzaɢJ=S|nyz5d<9L{h/ڋDD(>I/egivuSUeVKI (0jYXݤ
b&" s8X#@|!گ
i5<q",?03ʂwdpzNsCGpZZ-ai,;!ԖDy` ;p8a_@D; wrW4"LR }!H6IämH\xꁳԈL	bRby{p^}~|9ɩmw">#U
wDB*P8\lxDBd-Lajś%u\YvS _aaaUx3Δ"'יδQR3MRǯ3R*(rB֖,,A @6}^	EGV9Ei,*ѻsZŻYCfCq5cu@ktxS4$+RUBoXVgWV*[Yy?@:<p|@}-;Hwǭ;RԄas}܄^'Ç%U\f8dTS]ܽKw#?,V(afiwwIsptr)p*3Ѱr(aphǯ׈#'qd(dCnej5r	#r*P7VGƙT+-5ǑYg\wFR׃te:4(NsĈB}ᙿ6pޜ-&q2D_y]댃sj0|sWF ?<G_/OdGLdό:e8rp1;4*:FŉF#76ºZmmġstbFh8
yL>@1_!O	 D~YQn_j{^UGѻ7ߺ!}Oc_(_=|eQ$s9 uxK]ьs@^#1${:14Cuci
=FYyZ/ľmWhk@S%#sLBH#6VغP=+	r'm@,a.ěK~.2Cw$EJcړx6Nޱ%z롓 ZR}n)߄6>HP%"?HbZV8pe&3؟{cq4<2nD4H&r4+jN	]]'D^TּrHS<%[N^(f^rs
jrO
yp@ѲSi[=]NS6[--(by.ӇI]aa֍EyQ4;+)hV]B!.B}h[}	(@6
 \Gba$,.POGqKVsPVϯtvjݝ?$_	hFV,!E?{ܬX'>h#$aVH!1 6Ou?,:qA8GT&DwCqryMg}~[Pѳa1azu=/EȎ]0iSu_=tI٥ϊKh9pQh+	v0USĶS/b/w\ $j͝gu<-mKsma|NqeDbÓ]BBhtFLh.9[L9X]i9q/
Wey6TgFݞ9лc_AER{ueu5.PǿeSa U%_,uQANaOZB$w$S(:^p6FWc˩QڢĬ':QJu*,"@m lh#z~&He24MhJ
	OQd<.~<h`~4S	jqv@=!v1qKׅS09QY?-Rrf<lFUA#i-2 ?: 8TA3?rmM
>!
]Դ@dـ[jYnWf<ptjKdΫ'Lv|3:,o3cƔ|}>vb2YKMκq㚜fh6ZKǍgX\xroy
endstream
endobj
24 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 3477>>
stream
xڍW	TWbq	ec3ju.NbE\qAAYlldqA$҈l&ĨI4$qNd0F}ɛ;=uQB|A-n<qNL&ǓF9ɢRAچcb1b+tgp:6R(ص'M
ّ5|nߔ>C[d3Do-
սDЍօm7Dml-߼Z"d޲eKWԫCv6X1˘D
x03Q3`FgF2>8ƗLc0syL 	ffB
z&91
Fؘ[@/NK*/UyYví~.s\|B߯;޿$|&AwGw7n*o<@ٗ#%9dcѵE,FM0pKu|(ܲlQ%"H?YކjnClp-Ot$9O=謔<%2L՘ R(!]2$tZDpЫCR\D/AJيjhNg	I,D7T~yDC-RJk,O:ˁ>+@3Wv$si-(ɯpP[&Nщ-tɤVt>\%ZwSޭ9YFVf4-hk6࿃Z؞.2,6"ִٔfN2瘲r9nϲ0DX Z5&LцIW /s$ICE!*r4.i<2ǉ4´TջTSV>7#6$ת	xen+KZpf6?y;05G:bO7B>}$c9A4쫆J8| 7[ΝJ~? 7:ץIeZh͵"Zjl_dv,9/΃-Zi޲}[کOC7QB߮peHFH]FFg05%nb,|wP6~kw7~4Bf$ 9!j~Dk#w>;|<<w6Z|Qa|])X!誔z4j5?o*6v6RtBۗpx\xM
&|?t
,(8xJp!T]po>HO4#E=GM+Ldb}1)&7.7Qz]e|ڹ1y`w٫yvAPEtd.J@a02	1IR\AoҦ],K`N$z?,)GC7E^ w\vqYPKB+_	Z*!/ArvE_CP>%cфQ])Υr%)ZPi՘H& ,字g"ۦ%C-j .׆[͟8Ǳ'VՄ^\=5Jy09=`k4lzTvk>6CC)U3r7%Ƴ"O[E"\"dBf"1|2<iۛ,$ͱ9ɦ$s2;l/p0o@/_pXt-JVk{4Gl
Bt;$flI'Jxʎ#mTj*e=~.ݗWfyo;Q17=4%4?_ Y+;PO荧ЍkJtuDBǹ
.˾5`#8-oF9K{tF`E`\IsU)8GC$JpFS:Gg

GAa9hl
ɲ猪=jPPZT`XhV=sxiښ&ʌGwSEonȑdC>cg~۱v^X;P%E=qZr0;$? S~ʳܬFyp	qC@鱒tSlN3ڱ`%7ӗ,8\2ek&97wo*UM< m	Gu#Cѿ4XIv
If/r\	Y])MZK(PHm5ȈG)k8aSPmxkޭ@@*.^UB+'782XՈ!5ݣ
	4é47ko종&3UZ5[wrmUAt8/I&\XVBPH-Pӊ}T!#=BwUX)9#K}~7|L\&^м#?QFmܶ)!,V7$:wc=ZN7	OP>ۤf)A
?­g><=tQv~Jd/*(17+O΋.n*fw_տqq=uPzޏa'1 #f=wo ESožVJ+v"%NQI?6u$ND#{z:P'ƊG%:4(9VA#RBK14/1^wd6F8PsU	qiK%'`bQ3<p Sg;4R0PAS\{8O̩;V_=H%K3_yڮBJSq'tw)7̾^NiZV{VtSn)屨UX{ѰۊK,;(Q	y8\yo2(HhIM7fbE⩅80.:۞--q`Dq5,9ӕul
|"b\yLs:<RBzև:'aHϨ0a6KvŁN*n訥pR)bcBǌQJ'umxU]`Sqܛߚ9+(hPۧOp召0N%Ec0!aʁ07FYث,s.q!5d:_x|!bi{aPOvڞ4  fԖ@o/pY| k٠?p
]e7Z+_PB~,la1Mͫ,v?t2+),2)FBCEvcXQ	+Ǹ
<K^+o-,ȳ
endstream
endobj
26 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 6141>>
stream
xڍYXڞeaglk5J@4īEEAHRwُ]zT`QI4FOFs?YPͽy|(SSJ$Y.wXɤe˝B<mm8EzFSi7,Wl&EB;;xf@no>D"kP66懄*·GZM2gYy)zzDllzՊ!V^>V!V}Y8le?!AĿEEMlS󩏩Ee"GӒ
@jeN
SKH
JFYR[8]j<5BYS6-eGMSPddŅ"j1ZJ}B-S+(Gj%rՔZKSlJDR'EE&&LL2ŌI|HFIt}|1`Հ~;(ig1BC;lφoga#J#OF:7NȢe-m,kF9:f j2d"r{]JpIW:g33k1%ǒi7%,i$!ڌ+]G=BU[Ѵ}Fa}hD}PN%PR$@8YSPRYi:&B -3.Y ɐ`oAf/~nf.=)Ԍ| Er&C`ɀx`y3	NhIH8fkmBEuuγuX`s7aEcDψm*hcpdVa7r@C"ЂUҸ$]I"Sf;KiyGoG:)P!#:R0i*UԐQx44x/^aD 5
g{!1fsch\=S
+mJ`GNaii
lUvї_"3GgD\._EEw҆\PG{dzўn-<5)L٦gz#8yY,L¼,۶f_9'1o")̃EOOv
rͰҠ^yY脮hh.Ex2&-w\~&fh[ua\8C׀6[ HW2%:і4뾁}+]:Ző_ 
'mOMYQK?C6;wX"Mlߟ|teEmZ.X"Mgyo,9{	>\x5*S*F%1WU?^0CC	͕­e7ޮUw
I-Sޝ8w~[_o8
9pWq0m'l吘*=hu&E
)T
$1\(Kn.,v{2v3-6.A"dS/h/D*,)$2'K&͙ni_ҞA;Cyx)VKZ`.2y4ף7?~RXy}Hz1$~
SFgZe*ӚRAC<(Qg_1bÚPް)׵*E wO89>L+ eZ]VNai뉯6)J}a=cgf߰M Nc2y:NSLS&ְ5?uM<"kz{
l/Sbn(
k9@{~kKw9~h&Gc)ÕY=m39nJet.D7>W
_^`5덤lH)zdR`s(VU0ᒭ ѕ_,.͹$l[ޱL@wy'wFN|#U1}Q5ڏTA#fH
->۪
>Ǡy8{9Ph][
;xf?#Rpڼ)۴Sʘگ^\;J筝5}n4v~݁Αo<]} aUѰĭ[RSհA	<dʝ3L ޴m.f^CQy	1(@C-8
elV8F,j;ԢiK2TQ@2ļPC?(aG`I&[p=/lx#*p`,8#7@{)yow{Cևj,DoH)*m,!]
&6Kq
&Wgލ{SτН)(^Z 	 M߶EvZM%~WJ2Ur*PXo/mYwd1舧c;쁽@3!9b94E:^ChAgXB:#XJ\3r>Car*LWNlQI!uR,ge{.Fr]VQo\D7(;}pOҥ<,:69d)ܗM_m%~/ht91bO쉦a;$7kw-M&Jȁޓ}E\Ѯl#ny#U[׹^⯶rXCň1ּaH?ύ*ApɟccF;{^쫔x</ f+;x	;Om^p/Ʋ<="U=*GI_II&UCzpp:YHs^U\M8CdtɅLIvBvrX./iz2{J^ުJM<ܘڞc?R'^)x"6h!Z&!;ƍ!O£i78{kEޢ=C_zL4w~*Td4F)Lk+Pu'Av0vq}=&,JO,ףc[zW}>Y_ h	7e
M+~qn0cmwlKZtWRwZ^/t)
[vA ,ðGNF٭z9#f"	TC^WmJ`#S~G chvT;ѲNB+dɭx/[^a8CqL4!B6ThRdA
^mSɎϰ+n&^KݙZ[V2ϟ5WPwjZ4)D0	cW:R貚[:p2c.Q+^b@yP<{atd"M/iBj JJdH@[Q=%t/o-ر˸IHvgjKM|G?̺'ceļj<HgH?P9~B&~84 'Ne"U9DoYa/MpQ5)<*Hoxy_f`{S4VՁlҤAn޳t#AK*ck+	7mʬk>\Hӛ}u˂m	_CE`X,:`J?<*e3''ۻѬcnf;rH݄X8x$J^yBId
gՅQ_7h"'ryѤ{N̫VSs/l8vNi@M[
]h-Qsvh 	Dg2҂&nfj?.χf?/uZ$0a1e;?ox ǓƨF4{?ţ$\>݂Xb}N?lkq@:ţxkQH"e,	$ڈams
M'ڷn4=*w!hӍ!e$"׽wF.haPD.?aG~hd~ +K'IrЄۆ]<+7/nEȴ7b3T۴֌0?Hx< 1CiuU.FPXm>'X9K+0^Zظ4O/J&O9hZ/\79}	[%؇q
 ofD
㏮_S
V@8x@jf@V4e{B|{u9ƵWj.'\it%$2ŉ^))j$Fx3
Tˡ.ʼ|EʑI ˂]x8w1g%w)LXh.dr\f	o5;L[˶VhЏ׷W٫:seOWOu^,tqpk#sKT{%42F:[&fhإdeuA[bBCv7p=wŽeI]/4l\zs+-y'\:Q#a%Y;UT(b֞
9;-?c2E(̘]wb</Dq̢E26=`Jsyk2Vp~\IN%$@L|bJ*eB$6yE:d*ΓǶ;r[j9kMC,f,mEy&xG@ߨMlbԿ
c{9}9z,g~ٯ6
{[lWW޲
ɿwtߙ&lil0qgqUF'b<O 4GfII]ܣ܃@p
=C`'ja_TS5<,x˨&>Ʀ<c.μ.嫩Gt>:/p${|*CfB%"hvD
Gv3دXk/tcPuC&x3hۧo`ל2D7Hua=Btv}#>z~(a1s 21VRg#3X0@Qkb!	ڮ/75CCNVX4B^"3Q/!A;6ns.\\Ljsm@6038ꓫ?t	jURJS	7,viH]TcX}?^u+3p3Ś:U֯S!3BK.y@SEG?^̮	ARx,4FäWA>Ax66pfVE׸A~rjkKvVAf~OBzE3T8v0f T9H4;.0D7TA>/B0RdF&dt0ArJ|ɰ)jL+&
{#X e }Ҋ8d9b og7
Y~uپL_"؋{FQ`m4ڢ+  #=ƣFUFٖu{|[&[4 !;0(wt.*ӍEK1#j=ŭYJWŠ3; }%+?74}-LV-+]@ll݇kj#Äei hZUe%@%%-##9Qk!JJ+38J53t8>R#d:m\2#8ZFϋҥmoh:/ޓ)HҒjVFuGF}/
zcyMi,$MFzX##GK #d*ڇH&d+V,,F:Vg顔VwmkN\YţBڂAA0\ ApI2 ^_WPpD#_ӊV$![7Q<J//C9z	7tuA˃L63W՝<Цt윬C8vƿF?w<
endstream
endobj
28 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 1860>>
stream
x}U{T1;ZI2M4Ř@@J"rXXY^ hȣBEAc1Wm7E0s9|w}wD
!ܢC3կyz(rU	6yAĿh3<k]Ko!xdv7CJ[Uz==R(Ф*92/O-}CX ˂驲dY<D.U/SeՙejL_-z'FUk2T:;9'$+9VD>ODBa;^""#\aCLL_i׭8ɒ:1>r"B}5/KOѨTFMÌcU_B1frr28:A~RXۡ\sU?a1vx?0T>Eh iEz,M2C=@&6$LЈ=llʚNW۩է{ۋn~=\J&UA/TB3Qe+̑fh搎|۰tHz&ՖJaF20`?FCgU,8w.	<oO
SOT8P0B%Pt0ʀ
I<	ӷJEK^#t *,ϯ(m·=	eGQ/P]K/C	/l?lD߾Jٚ%@$i1q|9\ј:
2~ȴĶ}@af#_9'):TEǙ%d%GKш;.\^4a9j5:Pv\U9E3dW!x=w߄Xz;>|I7&$qgp5
(G\0|/|6ޚ˘jpqYr{gigݤ=(b>Hh;Bפ (m,kOkR(ѕB(]~\+	ݯ@A1U)cU7 TkTL,C]ͽ5-=@48[KߍJa˵&'Ϟ;;K}m31цNS[,lu3_d!-g+٠cf

|Mqd5j!K4/C2s[nzo"偂Aw"H,V'돩A:deRʡ|( *gA>fX'&[`E=`Uk8aP$G75x'FO	TFnZ|Oxo!·l<
 ^?3cT18?Y[ˋ<VxC&KJ\
o.*BVw"	X/\˗)rVOrHnϙ! UE+)~<?a''&JGB{wwء}YӒ3_<57iO^K;D{cbXzIaĲ7\!>eȅ7p.z1d盀l_)AB&/RQH"=
]ʴB*$PP;ᬉEh"=XYkSϼIMƇI+7&WyHQD13cPL]7"fWe_	ng}_ym0-}e'kId7cgmDҦѺ"SBNV
NسdaK]MhGSMD;؁َo3XΚ:ѮzCGݮv
endstream
endobj
30 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 321>>
stream
xcd`aa`dd


M34
JM/I,If!Cy?Nɰv*RD>@`b`edd-1300q/,L(Q0200
I
Nz
^ٙ
y)
^zz
~@L<ԌĜ44`נ` Ѐ`M=rsa :E̜e}+W:ccecs֭3gn.<{޴XBy8'pupgRoo	%[&N0W  *p
endstream
endobj
32 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 4131>>
stream
xڍW	Xײa`hGhϸ.(nqQ1B

  ;"058n ,..n1z5&1Fɽ|~=9UU92ښdݗZ1}9^aCG>b2$ȥn֭xogo6}FiO̻=;}9<q`e2fmannm^Q=m+WSS		Vg3L=7,v	U\n:lz*^Ԟ^/4`FQٻȆ2Ә錧ll%gUL0N!@;0.LWƞq`xd3=G'Ӌ81. bŌf03g3e1BƛY,ecͬf.8IWIYt1)bek8'nm>:Ns'N:m]vo`{kﮉvNve=%bdȼKd2GBޙW[7 Oh;
SiB<'ndF2?Hq7绁ۖgWՃG~-ZiۗU	@U@F YCF+"OL!Km
Bte8rdj4.B7#$GcoX./`SBiD6sQt6C-m翚Q%@TWXS(JwRYva,vK{ӈh$u$ӳ0@(a/4l&M=
spo;]V^9ݾ\juLmNr)H]dȄO@4p1b.8gkAT-[8k3x֊P؊Q(t16'k@pY!֝n<zߡ5^p%]\猘< w%f/7|s
<VyN,'I).ӥ
ʒ*t\`:rE/S2\	Bʛ%w/@3wruiҬ~jVTwC;#*8k
GL}>EB%A2G(A>_8[Q
9Rgyx5w_jK<Di	cTȥ`Kٽ9aB4/35@dZF'PUP U`o`bhzbg؏?N+\'ω
t}hSa߬ҁxE-dX;<7ը7	*rRa;de; 35IgU$YigvF&.NGmX݈Wn8s8E{^M+D]E?Lf%YSEP+۸&#n6_KzcH Nl'y"N0IXv%T](xLqʋ}7˥s-J9gSvt>73(FT}}{vVGPH]LREFE
!G@\a.1p&1˒K5
8?wˇ,0NU';|7iQbƊK2W/nG2D ߢ|Ǯ邌*qh%r
ɕBIk'XA^LcM1x$P4,kR'Nvv<cKBL *ԀhE:?18>oC;(Wq_``d*S^(81
{_߄KP(;`-|TM%;Qv
]%	y	cx!	RANt`l+wpވo<?M/&32ˀcڣv7	>c㜓yR"d'H?1l]Ᶎ-kDDײR"BH2Z5?MUXQLM,,<pqEpbjmKA[hZ,:^\,(ulz04cRdt!ffx9
pb!B7'hC`ٿ\mк'	~dⴼ'$,Ád8¯Y'*7\7Tr#E-_Y@!#t^`)C
WXVZةxVQ+X_[ecEyKyēp E,_p#kzņ(5J9pF:~FA2$G=$]ړ\QГDX,]VS Jq|r.k0IA!.+%rh`
_SŰmgv.VI1P>~%lossE}%P:2GTą&&1xDT-#=VOoknC3\3|Yq	Ͱ\OoE2C+\i6>'-?PnJ'ehB<Hq83?>gӭ~;p:djG#CfZze	RJ#P\DI3`#QM[h"~.A0G1YZZ3驑_6ʮětx?@Gn@w]P
W}M&Ύ!Lڻ!_ӆS̃KN,NGYB]iĊpX-/L28
 '
HnTl,	t㍥ϵ4߄6oT-
Da)(3%eE(\{jg(Kg-Ys|bțGml|H%vB]--<~*wŰ6E?*B}Q~AQ\kH(eb&teKr|ßZw(Kzޖa z;1 G`gxu1$";f@Td,	eg]'Q5{+Jp&<k*Cpj'tĔ
d !hwTl(=S
P"ˑUO	9"Q#w4Z6{k륮؍nNw*ϥav33g/4P:;tj)U5PNa\Bf4r#m&/xN%,H0 r&G*8 N0˪[XȠmB2__=<s<ثJ%'Wyеѣ{`#b/%ISdS__doD͹7Fl+n*H,hK&T&
F?=>95tu=Z>bdPPIFS 8^wX0@pK<n\kEljLh;:#:l苫X=9T]@%v?
cDa˻Y}\]7Z]=O;S|[yd9a{
CƒqnIOb''OV!򕣁0'<©݇hcBiPI0,-Ѡm.\Zy{ba:,H3fwɄ]Kެ)i
}
+b*T;ڪQCm|FZ	15Al}if>\s-܃МDC
]@cW`l'ÀPA@ZMf$MQB_1cGl]5ei;ᒝt)<rȞ8{8
Ev33$2V&~t.аz..:n
7dB$G/	=o~vN:ԩ$%'>?UǿlbՏ~г$ðZ
јY +K}
GfGQÂUX	e6gEFsy}7.Z4cnvz@8Y#,܇M{/`ѴhF69_+JBGa;:wΝĎth4
&SΝŭo7nGc.¸	,7Bp
endstream
endobj
34 0 obj
<</Subtype/Type1C/Filter/FlateDecode/Length 1923>>
stream
x]T{PSWqsyص0"]U/@񁻆 I $C(\"/ /yP-;mZ8&َ][k{	{go}9"ŅDK##Rv[s.F`]Ț:ݾ~#xڙuyu w[%xm{(*ćp!!a[<BY ]f7H҈`v<[[4q"aQ%
jiJ:S͔'MNJL$''$R{0Jr'v#
 n	Kl&"H"&b]DO$$"Ox;"XI_&Bjs.\p"%_A/Eh/wFw%LJYJ%_,\~:h"(J>HՑe2𒨔c]=ih<p&lԗj9yE9 ";Ķ1${4(`lcXm]m#lL#yř!oǏi/:/⳱䒜+-TΦ}611w:i)+[G%f
5PaL<WiM.}/rP𢡙	x)Q,򫘗0~Dݹ#13xs	A]V	jUdR{28u?gچV۲_x"%CJU'd; A_7OYNQR8ûC?Ћ-?X|P
?89\1KHs99I4lma؏Twfˀv
Whq5ں1H͐m}7P	ޗaԶ;ﾺy'#9$xk2Tyaѡy0~Ǒx3{o%/~;S{*	>I6ߺ6W8	o(/?̌Ρ9Yuac|'.N`*{H7f5MaL;}cPgK)cY!
Qa/w'@QݵzY:HEzxȶA
\]M}Sグ-sh-9_d044Bi{0!ٗ.O9`+M]2%i4-[iS_kGS#)p-"BGl|ֈdDhZ+Xϑ}Y,>y>|\F$Zt-2MVq_y5܏
xX١RjeV[1BWKrc
m5KV|UlJab[-/}3<>ްXUf{UPaK)lHCs:	ue+ϥCJZ!Pnf=3S
cB~%Ao{D%@>;<9eKcU1,;H6;8
G	Aϰ+v
 >Y'bbo޼}?
oڂVXЄUeEV?nTJiYyx,7*5-0bo(CgvY}BN~Dqi "ǙSݽƒaGl( wt%))TH&`^Sd@UgX$װ
ڌpz=D.\;"t8l:)h쒥E4(b ݕvI4Kv46ءmeb2`:})D|`$
|]ɖlΩ3$`Nse\K=_>¶bCa?@W muܽgfO>g1^eF>܈g$GV==,63m;S9]qB5zw;y
endstream
endobj
36 0 obj
<</Filter/FlateDecode/Length 328>>
stream
x]n0E
/Eś4BJH,PI>!E
2dfp|s+"/;0Ow@^:hԲ[^ޚvVTÀu]6y$LH}׼؇[>mvW9S17A/yNf*|?YUto/ҫ֊,8#V rA3qLg₹ .Kd8!	ҿ<kL7H9D'DdGq="v$pHdl"')wQH)==7+1>%YMUE
endstream
endobj
43 0 obj
<</Filter/FlateDecode/Length 227>>
stream
x]Pn0ӡ (dvDP࿯: #U7\gTՈ	f4a+)	XAdee`U!ç:u)6k߼ѽnpn ~vhӻd4qnXq
]mk(1[nA}1W>kfVtV}o_E*ʑl
endstream
endobj
14 0 obj
<</Type/ObjStm/N 33/First 244/Filter/FlateDecode/Length 1725>>
stream
xXmS8~Bݒ;f+-i@)7D	vv(߮8Nphҗ;B앵ZZK7K$dpG&h
l1і
"`Z
!"bXGD)"ǚ 41RE{XX #8#HD1׆X8l8Q2PjeAWf$uIU"`;l9=ocXx=acODn>=˳+1p%=a/D>wIܶu]M+ )/ap%wihCP5͆ˣ )p:^7d7Hzݠc݂l<s^;]+Md1[Lx1v1i;d2ޓ;%ndQt;GP Z=~ݰWzn7W[hf(mkOFg¶bْZhZF](cDy@c~(k`,_O#z1RƦf+[Ij\pX<'R`e-+z.OeH HF{r%xFqad\RN3PJsuKW
+ Uq̤dEZ`IkAp
@@=7kCdCZf6Rx>/?HYzUs*0jdH/8fVʅ܁C4U
:\ΈA-`'H^\=3>@oܕPjjwmF߆38
ż*q
#9RRA"ح5>8*Ղs**gR
+#1zP/z>T p>7'ɋCWFO{3sM_l#l9Kxڽyo/.^vfMǽ~~LC7IK-t覄"!A&E_'p't4v^B+@ǽQA"/ʞ:83dͅ~mN\dZ	ڡgt\^utH:4М^ޭ[|UC-߾}C~?H7%p(YC,no{33:LM <]y4*,hIjxfx<!eG;oFPd0/ehe42äB!$^< ErOwR@e1+MRé?WA2
R}.iiI\I4D&7v2P9j3˧
 5uKٵWoNH~6/%=BszA蟪B97 _Z SLWały<lX_^=:Y=
|P
 PdO_p">q5c,}
5ojWeT0(0_u/\i+>McWּΪ+u	[?;݋5=N%کUo-J&<bTK](t\+ۢ P?t߿j:X\<jr)\8Uf냝袹AZ$(PCs밸lǕ˟N=
endstream
endobj
47 0 obj
<</Type/XRef/ID[<e76d73e4cb7948c8a23d495e5632e3a6><e76d73e4cb7948c8a23d495e5632e3a6>]/Root
1 0 R/Info 2 0 R/Size 48/W[1 2 2]/Filter/FlateDecode/Length 137>>
stream
x%ʹ
P a;|LAL4 %%ѳ 
+Yn>("TEED"]C9 *s?\e3|՘]u7WW
<%[WG@b$V_LL,xK?;g
endstream
endobj
startxref
29718
%%EOF


================================================================================
--- File: d23/d3_custom_evaluation_instructions.markdown ---
================================================================================

# Instructions for Evaluating Custom Data with the Provided Embedding Evaluation Script

This guide explains how to use your own dataset with the provided Python script to evaluate embedding models for passage retrieval. The script creates vector stores from a JSON dataset and evaluates the performance of embedding models in retrieving relevant passages for given queries. Below, we detail the required data format, provide an example, and outline the necessary changes to the script to use your custom data.

## Prerequisites
Ensure you have the following Python libraries installed:
```bash
pip install matplotlib numpy tqdm langchain sentence-transformers torch chromadb
```
Additionally, ensure you have a JSON file containing your dataset in the specified format.

## Required Data Format
The JSON dataset must be a list of dictionaries, where each dictionary represents a passage with associated questions. The bare minimum required keys for each entry are:

- **annotation_id** (string): A unique identifier for the passage.
- **text** (string): The passage content to be embedded and retrieved.
- **question** (list of strings): A list of questions related to the passage. Each question is used to query the vector store.

### Optional Keys
While not required, including additional keys like `answer`, `category`, or `url` can provide context but are not used by the script for evaluation.

### Example JSON Data
Below is an example JSON file (`custom_data.json`) with the minimum required keys and some optional fields for context:

```json
[
  {
    "annotation_id": "d5f877c8-963e-4a96-925d-6e2c28417895",
    "text": "বাংলাদেশ হাইকমিশন, আবুজা, নাইজেরিয়া অফিস সময়: ০৯:০০ am থেকে ০৫:০০ pm (সোমবার - শুক্রবার)",
    "question": [
      "বাংলাদেশ হাইকমিশন, আবুজা, নাইজেরিয়ার অফিস সময় কতটা?",
      "বাংলাদেশ হাইকমিশন, আবুজা, নাইজেরিয়া কোন দিনগুলোতে খোলা থাকে?"
    ]
  },
  {
    "annotation_id": "2199400e-ee73-4443-9374-b777c50ee801",
    "text": "বাংলাদেশ ও নাইজেরিয়া ১৯৭০ দশকের মাঝামাঝি সময়ে কূটনৈতিক সম্পর্ক স্থাপন করে। নাইজেরিয়া ১৯৭৪ সালের ২৭ ফেব্রুয়ারি বাংলাদেশকে স্বীকৃতি প্রদান করে।",
    "question": [
      "বাংলাদেশ ও নাইজেরিয়া কবে কূটনৈতিক সম্পর্ক স্থাপন করে?",
      "নাইজেরিয়া বাংলাদেশকে কবে স্বীকৃতি প্রদান করে?"
    ]
  }
]
```

Save this data in a file (e.g., `custom_data.json`) in your project directory.

## Modifying the Script
To evaluate your custom data, you need to update the script to point to your JSON file and specify the embedding models you want to evaluate. Below are the detailed changes required in the script.

### Original Script Snippet (Relevant Section)
The original script specifies the JSON path and embedding models in the `__main__` block:

```python
if __name__ == "__main__":
    # Define paths
    json_path = "data/json___unvalidated__annotation_data_d5f877c8-963e-4a96-925d-6e2c28417895.json"

    # Initialize embedding models
    embedding_models = {
        "base": CustomEmbeddings("l3cube-pune/bengali-sentence-similarity-sbert"),
        "d3_version": CustomEmbeddings("experiments/BanInfRet/d3_baninfret"),
    }

    # Evaluate and plot
    evaluate_multiple_embeddings(json_path, embedding_models)
```

### Changes to Make
1. **Update the JSON Path**:
   Replace the `json_path` with the path to your custom JSON file. For example, if your file is named `custom_data.json` and located in the same directory as the script, update the path as follows:

   ```python
   json_path = "custom_data.json"
   ```

2. **Specify Embedding Models**:
   The script evaluates two embedding models by default: a baseline model and our fine-tuned `d3_version` model. You can keep these models, choose different ones supported by `SentenceTransformer`, or evaluate a single model. For example:
   - To use the baseline model (`l3cube-pune/bengali-sentence-similarity-sbert`) and our fine-tuned `d3_version` model:
     ```python
     embedding_models = {
         "base": CustomEmbeddings("l3cube-pune/bengali-sentence-similarity-sbert"),
         "d3_version": CustomEmbeddings("experiments/BanInfRet/d3_baninfret"),
     }
     ```
   - To evaluate only the `d3_version` model:
     ```python
     embedding_models = {
         "d3_version": CustomEmbeddings("experiments/BanInfRet/d3_baninfret"),
     }
     ```
   Ensure the model paths are valid `SentenceTransformer` models available locally or on Hugging Face. The `d3_version` model, developed by our team, is optimized for Bengali passage retrieval and should be accessible in your local `experiments/BanInfRet/d3_baninfret` directory.

3. **Optional: Customize Output Files**:
   The script saves a Precision-Recall plot (`pr_curve.png`) and metrics (`metrics.json`) by default. To save these files with custom names or in a specific directory, modify the `evaluate_multiple_embeddings` call to include custom `plot_file` and `metrics_file` parameters:
   ```python
   evaluate_multiple_embeddings(
       json_path,
       embedding_models,
       plot_file="output/custom_pr_curve.png",
       metrics_file="output/custom_metrics.json"
   )
   ```
   Ensure the `output/` directory exists or adjust the path accordingly.

4. **Optional: Adjust Thresholds and Top-k Values**:
   The script uses default similarity thresholds (`[0.5, 0.6, 0.7, 0.8, 0.9]`) and top-k values (`[1, 3, 5]`). To customize these, pass them explicitly in the `evaluate_multiple_embeddings` call:
   ```python
   evaluate_multiple_embeddings(
       json_path,
       embedding_models,
       top_k_list=[1, 5, 10],
       thresholds=[0.4, 0.6, 0.8],
       plot_file="output/custom_pr_curve.png",
       metrics_file="output/custom_metrics.json"
   )
   ```

### Updated Script Snippet
Here’s how the updated `__main__` block might look with the changes:

```python
if __name__ == "__main__":
    # Define paths
    json_path = "custom_data.json"

    # Initialize embedding models
    embedding_models = {
        "base": CustomEmbeddings("l3cube-pune/bengali-sentence-similarity-sbert"),
        "d3_version": CustomEmbeddings("experiments/BanInfRet/d3_baninfret"),
    }

    # Ensure output directory exists
    import os
    os.makedirs("output", exist_ok=True)

    # Evaluate and plot
    evaluate_multiple_embeddings(
        json_path,
        embedding_models,
        top_k_list=[1, 5, 10],
        thresholds=[0.4, 0.6, 0.8],
        plot_file="output/custom_pr_curve.png",
        metrics_file="output/custom_metrics.json"
    )
```

## Running the Script
1. Save your JSON data in `custom_data.json`.
2. Update the script with the changes above, ensuring the full script (including imports, `CustomEmbeddings`, `create_vector_store_from_json`, and `evaluate_multiple_embeddings`) is intact.
3. Run the script:
   ```bash
   python script.py
   ```
4. Check the output:
   - A Precision-Recall plot will be saved at `output/custom_pr_curve.png`.
   - Metrics (Recall@k, Precision@k, MRR@k, F1@k, and average top-1 similarity) will be saved in `output/custom_metrics.json`.
   - Console output will display progress and metrics for each model and threshold.

## Troubleshooting
- **File Not Found Error**: Ensure `custom_data.json` is in the correct directory or provide the full path (e.g., `/path/to/custom_data.json`).
- **Model Not Found**: Verify that the specified `SentenceTransformer` model paths are valid and accessible. For the `d3_version` model, ensure the `experiments/BanInfRet/d3_baninfret` directory exists and contains the fine-tuned model.
- **JSON Format Errors**: Ensure the JSON file is well-formed and contains the required keys (`annotation_id`, `text`, `question`). Use a JSON validator if needed.
- **Memory Issues**: If processing a large dataset, reduce the `chunk_size` in `RecursiveCharacterTextSplitter` (e.g., from 1500 to 1000) to lower memory usage.

## Expected Output
The script will:
- Create temporary vector stores for each embedding model.
- Evaluate retrieval performance for each question in the dataset.
- Print metrics (Recall@k, Precision@k, MRR@k, F1@k, Avg Top-1 Similarity) for each model and threshold.
- Save a Precision-Recall plot and a JSON file with detailed metrics.

By following these instructions, you can evaluate your custom dataset with the provided script and compare the performance of different embedding models, including our fine-tuned `d3_version` model, for passage retrieval.

================================================================================
--- File: configs/config.yaml ---
================================================================================

# ===================================================================
# Chat Agent Master Configuration
# ===================================================================

# --- LLM Service Definitions ---
# Define the available LLM services (e.g., different model sizes or providers).
# The agent will load these services and map them to tasks below.
# Values ending in '_env' specify the NAME of the environment variable to load.
llm_services:
  small_llm:
    api_key_env: "VLLM_SMALL_API_KEY"
    model_name_env: "VLLM_SMALL_MODEL_NAME"
    base_url_env: "VLLM_SMALL_BASE_URL"
  medium_llm:
    api_key_env: "VLLM_MEDIUM_API_KEY"
    model_name_env: "VLLM_MEDIUM_MODEL_NAME"
    base_url_env: "VLLM_MEDIUM_BASE_URL"

# --- Task-to-Model Mapping ---
# Assign a defined LLM service to each specific task in the pipeline.
# This allows you to use faster/cheaper models for simple tasks (like reranking)
# and more powerful models for complex tasks (like final answer generation).
task_to_model_mapping:
  retrieval_plan: "medium_llm"    # Use medium model for robust JSON generation and intent classification.
  reranker: "small_llm"           # Use small, fast model for parallel passage scoring.
  answer_generator: "medium_llm"  # Use medium model for high-quality, nuanced final answers.
  summarizer: "small_llm"         # Use small, fast model for concise history summarization.
  non_retrieval_responder: "medium_llm" # Use medium model for better conversational ability.

# --- Conversation Management ---
conversation:
  # The number of recent user-AI turns to keep in the conversation history.
  # This acts as a rolling window to maintain context.
  history_window: 3

# --- Vector Retriever Configuration ---
vector_retriever:
  # Number of top results to retrieve from EACH collection during the initial search.
  top_k: 3
  # A list of all vector collection names to be queried.
  collections:
    - "PassageDB"
    - "TopicDB"
    - "KeywordDB"
  # The name of the primary collection that contains the full passage content.
  passage_collection: "PassageDB"

# --- Reranker Configuration ---
reranker:
  # The score threshold for a passage to be considered "highly relevant".
  # In the current setup, 1=Relevant, 2=Partial, 3=Irrelevant.
  # Setting this to 1 means only the most directly relevant passages are used.
  relevance_score_threshold: 1

# --- Category Refinement Configuration ---
# Parameters for the fuzzy string matching used to align the LLM's category
# output with the canonical list of service categories.
category_refinement:
  # The minimum similarity score (0-100) required to consider a string a valid match.
  # A value of 85-90 is usually a good balance between flexibility and accuracy.
  score_cutoff: 85

# --- LLM Call Parameters ---
# Fine-grained control over the generation parameters for each LLM task.
llm_call_parameters:
  retrieval_plan:
    temperature: 0.0
    max_tokens: 512
  reranker:
    temperature: 0.0
    max_tokens: 256
  answer_generator:
    temperature: 0.1
    max_tokens: 2048
  summarizer:
    temperature: 0.2
    max_tokens: 512
  non_retrieval_responder:
    temperature: 0.2
    max_tokens: 1024

# --- Response Templates ---
# Canned responses for specific scenarios. Centralizing them here makes it easy
# to edit the bot's personality without changing the code.
response_templates:
  error_fallback: "Sorry, I encountered an unexpected error. Please try asking again."
  plan_generation_failed: "I'm having trouble understanding your request at the moment. Could you please rephrase it?"
  no_passages_found: " দুঃখিত, আমি এই মুহূর্তে আপনার অনুরোধ সম্পর্কিত কোনো তথ্য খুঁজে পাচ্ছি না।"
  no_relevant_passages_after_rerank: "আমি আপনার বিষয়টি সম্পর্কিত কিছু তথ্য পেয়েছি, কিন্তু সুস্পষ্ট উত্তর খুঁজে পাইনি। আপনি কি আপনার প্রশ্নটি আরেকটু বিস্তারিতভাবে বলতে পারেন?"

