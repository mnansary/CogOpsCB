# ===================================================================
# Chat Agent Master Configuration
# ===================================================================

# --- LLM Service Definitions ---
# Define the available LLM services.
llm_services:
  small_llm:
    api_key_env: "VLLM_SMALL_API_KEY"
    model_name_env: "VLLM_SMALL_MODEL_NAME"
    base_url_env: "VLLM_SMALL_BASE_URL"
    max_context_tokens: 16384
  medium_llm:
    api_key_env: "VLLM_MEDIUM_API_KEY"
    model_name_env: "VLLM_MEDIUM_MODEL_NAME"
    base_url_env: "VLLM_MEDIUM_BASE_URL"
    max_context_tokens: 131072

# --- Task-to-Model Mapping ---
# Assign a defined LLM service to each specific task in the pipeline.
task_to_model_mapping:
  retrieval_plan: "medium_llm"
  reranker: "small_llm"
  answer_generator: "medium_llm"
  summarizer: "small_llm"
  non_retrieval_responder: "medium_llm"

# --- Conversation Management ---
conversation:
  history_window: 3

# --- Vector Retriever Configuration ---
vector_retriever:
  top_k: 10
  collections:
    - "PassageDB"
    - "TopicDB"
    - "KeywordDB"
  passage_collection: "PassageDB"
  max_passages_to_rerank: 4
  # --- Reciprocal Rank Fusion (RRF) constant.
  # A standard value is 60. Lower values give more weight to items that appear
  # in the top ranks, while higher values spread the weight more evenly.
  rrf_k: 60
  passage_id_meta_key: "passage_id"

# --- Reranker Configuration ---
reranker:
  relevance_score_threshold: 1

# --- Category Refinement Configuration ---
category_refinement:
  score_cutoff: 85

# --- NEW: Concurrency Control ---
concurrency_control:
  # The maximum number of reranker LLM calls that can run at the same time.
  # A value between 3-5 is a safe starting point for a single GPU.
  reranker_concurrency_limit: 2

# --- NEW: Token Management ---
# Centralized configuration for prompt building and truncation logic.
token_management:
  # The Hugging Face model name for the tokenizer used to count tokens.
  # This should be a model from the same family as the ones being used.
  tokenizer_model_name: "google/gemma-3-4b-it"
  
  # A safety margin (as a decimal percentage) to reserve for the prompt's
  # static text (instructions, boilerplate) when calculating available tokens.
  prompt_template_reservation_tokens: 500

  # When truncating components, this defines the budget allocation (as a decimal).
  # This value means up to 40% of the *remaining* token budget will be used for history.
  # The rest will be allocated to the passages or other variable content.
  history_truncation_budget: 0.4 # Allocate 40% of available space to history

# --- LLM Call Parameters ---
# Fine-grained control over the generation parameters for each LLM task.
llm_call_parameters:
  retrieval_plan:
    temperature: 0.0
    max_tokens: 512
  reranker:
    temperature: 0.0
    max_tokens: 256
  answer_generator:
    temperature: 0.1
    max_tokens: 2048
  summarizer:
    temperature: 0.2
    max_tokens: 512
  non_retrieval_responder:
    temperature: 0.2
    max_tokens: 1024

# --- Response Templates ---
# Canned responses for specific scenarios.
response_templates:
  error_fallback: "Sorry, I encountered an unexpected error within Network and server. Please try later"
  plan_generation_failed: "I'm having trouble understanding your request at the moment. Could you please rephrase it?"
  no_passages_found: " দুঃখিত, আমি এই মুহূর্তে আপনার অনুরোধ সম্পর্কিত কোনো তথ্য খুঁজে পাচ্ছি না।"
  no_relevant_passages_after_rerank: "আমি আপনার বিষয়টি সম্পর্কিত কিছু তথ্য পেয়েছি, কিন্তু সুস্পষ্ট উত্তর খুঁজে পাইনি। আপনি কি আপনার প্রশ্নটি আরেকটু বিস্তারিতভাবে বলতে পারেন?"