# ~/gemma3-server/docker-compose.yml
# A robust, secure, and conflict-free setup for a vLLM OpenAI-compatible server.

version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: qwen3-server

    # --- Secure API Key and Model Configuration ---
    # The command is built using the variable from our .env file.
    # This keeps the secret out of the main configuration.
    command: >
      --model Qwen/Qwen3-30B-A3B-Instruct-2507-FP8
      --host 0.0.0.0
      --port 8000
      --max-model-len 130000
      --gpu-memory-utilization 0.95
      --guided-decoding-backend xgrammar
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
      --trust-remote-code
      --api-key rYGBOPZ8meDvpJQRj4aF9hJJlatgApirBCCEBLICTL40sGemma327Bit

    # --- GPU Access ---
    # This is the equivalent of '--gpus all'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # --- Data Persistence ---
    # Mounts the Hugging Face cache to speed up model downloads on subsequent runs.
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface

    # --- Port Mapping ---
    # Binds port 5000 on your host machine to port 8000 in the container.
    # The '127.0.0.1:' part makes it accessible only from your local machine for security.
    # Remove '127.0.0.1:' to expose it to your local network.
    ports:
      - "127.0.0.1:5000:8000"

    # --- Stability and Health Monitoring ---
    # Automatically restarts the container unless you manually stop it.
    restart: unless-stopped

    # Checks if the vLLM server is responsive.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 15s
      retries: 5
      # Give the server 10 minutes to start and load the model before the first health check.
      start_period: 600s

    # --- Conflict-Free Networking ---
    # Connects this service to our custom network.
    networks:
      - vllm-net

# --- Custom Network Definition to Avoid Conflicts ---
networks:
  vllm-net:
    driver: bridge
    ipam:
      driver: default
      config:
        # We define a custom subnet that is very unlikely to conflict with a VPN.
        - subnet: "10.58.0.0/24"